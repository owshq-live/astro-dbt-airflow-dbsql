[0m16:24:09.387400 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11968fd10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118aec610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11968e190>]}


============================== 16:24:09.391966 | 7ce1a6b8-71d5-45bc-9092-ff3bc5cfad19 ==============================
[0m16:24:09.391966 [info ] [MainThread]: Running with dbt=1.7.8
[0m16:24:09.392287 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'version_check': 'True', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m16:24:09.397102 [info ] [MainThread]: Error importing adapter: No module named 'dbt.adapters.databricks'
[0m16:24:09.397379 [error] [MainThread]: Encountered an error:
Runtime Error
  Credentials in profile "default", target "dev" invalid: Runtime Error
    Could not find adapter type databricks!
[0m16:24:09.412050 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 0.089634664, "process_user_time": 0.788779, "process_kernel_time": 0.162367, "process_mem_max_rss": "108265472", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:24:09.412487 [debug] [MainThread]: Command `cli run` failed at 16:24:09.412407 after 0.09 seconds
[0m16:24:09.412727 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1196b7a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1196b4d50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1030d2b90>]}
[0m16:24:09.412942 [debug] [MainThread]: Flushing usage events
[0m16:26:30.727466 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f46110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f44150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105fc2310>]}


============================== 16:26:30.730099 | 4e8f04a6-fa48-4f70-9926-142d83997ec7 ==============================
[0m16:26:30.730099 [info ] [MainThread]: Running with dbt=1.7.8
[0m16:26:30.730401 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:26:32.484119 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4e8f04a6-fa48-4f70-9926-142d83997ec7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105fc09d0>]}
[0m16:26:32.512768 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4e8f04a6-fa48-4f70-9926-142d83997ec7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x17f868ad0>]}
[0m16:26:32.513023 [info ] [MainThread]: Registered adapter: databricks=1.7.9
[0m16:26:32.532490 [debug] [MainThread]: checksum: 67f0013ca5f0bd43af9a0873dd50792fde83ef69de63b71cacd0b4ac656c52e5, vars: {}, profile: , target: , version: 1.7.8
[0m16:26:32.535805 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:26:32.536104 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '4e8f04a6-fa48-4f70-9926-142d83997ec7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x17fa00c10>]}
[0m16:26:33.116768 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4e8f04a6-fa48-4f70-9926-142d83997ec7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16fae1dd0>]}
[0m16:26:33.128677 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4e8f04a6-fa48-4f70-9926-142d83997ec7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x178097590>]}
[0m16:26:33.128894 [info ] [MainThread]: Found 3 models, 3 sources, 0 exposures, 0 metrics, 538 macros, 0 groups, 0 semantic models
[0m16:26:33.129062 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4e8f04a6-fa48-4f70-9926-142d83997ec7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16fa4ea50>]}
[0m16:26:33.129670 [info ] [MainThread]: 
[0m16:26:33.129826 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m16:26:33.130033 [debug] [MainThread]: Command end result
[0m16:26:33.211147 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 2.5099454, "process_user_time": 2.509887, "process_kernel_time": 3.384827, "process_mem_max_rss": "215744512", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:26:33.211526 [debug] [MainThread]: Command `cli test` succeeded at 16:26:33.211453 after 2.51 seconds
[0m16:26:33.211778 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105fc2310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105fc2150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105f30690>]}
[0m16:26:33.212018 [debug] [MainThread]: Flushing usage events
[0m16:28:37.830954 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108043990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1080c1e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1080c24d0>]}


============================== 16:28:37.833669 | 55e37f9c-6af0-4582-b7c7-03181e9387ad ==============================
[0m16:28:37.833669 [info ] [MainThread]: Running with dbt=1.7.8
[0m16:28:37.833971 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:28:39.542105 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '55e37f9c-6af0-4582-b7c7-03181e9387ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1080c1250>]}
[0m16:28:39.570242 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '55e37f9c-6af0-4582-b7c7-03181e9387ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1080e1690>]}
[0m16:28:39.570506 [info ] [MainThread]: Registered adapter: databricks=1.7.9
[0m16:28:39.590941 [debug] [MainThread]: checksum: 67f0013ca5f0bd43af9a0873dd50792fde83ef69de63b71cacd0b4ac656c52e5, vars: {}, profile: , target: , version: 1.7.8
[0m16:28:39.671365 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:28:39.671595 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:28:39.674241 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '55e37f9c-6af0-4582-b7c7-03181e9387ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16ac34b50>]}
[0m16:28:39.680783 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '55e37f9c-6af0-4582-b7c7-03181e9387ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16adbb910>]}
[0m16:28:39.681019 [info ] [MainThread]: Found 3 models, 3 sources, 0 exposures, 0 metrics, 538 macros, 0 groups, 0 semantic models
[0m16:28:39.681188 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '55e37f9c-6af0-4582-b7c7-03181e9387ad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16acbc850>]}
[0m16:28:39.681830 [info ] [MainThread]: 
[0m16:28:39.681988 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m16:28:39.682212 [debug] [MainThread]: Command end result
[0m16:28:39.743117 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 1.9403082, "process_user_time": 2.063534, "process_kernel_time": 3.29985, "process_mem_max_rss": "210157568", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:28:39.743477 [debug] [MainThread]: Command `cli test` succeeded at 16:28:39.743409 after 1.94 seconds
[0m16:28:39.743715 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108092b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10809dad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108030a90>]}
[0m16:28:39.743903 [debug] [MainThread]: Flushing usage events
[0m16:30:48.409558 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121144410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1211c1b90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1211c21d0>]}


============================== 16:30:48.412375 | ede38d45-3dcb-4e70-9081-f2cad44e4078 ==============================
[0m16:30:48.412375 [info ] [MainThread]: Running with dbt=1.7.8
[0m16:30:48.412667 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:30:49.950107 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ede38d45-3dcb-4e70-9081-f2cad44e4078', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1211440d0>]}
[0m16:30:49.978372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ede38d45-3dcb-4e70-9081-f2cad44e4078', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1472e4bd0>]}
[0m16:30:49.978624 [info ] [MainThread]: Registered adapter: databricks=1.7.9
[0m16:30:49.996264 [debug] [MainThread]: checksum: 67f0013ca5f0bd43af9a0873dd50792fde83ef69de63b71cacd0b4ac656c52e5, vars: {}, profile: , target: , version: 1.7.8
[0m16:30:50.075593 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:30:50.075947 [debug] [MainThread]: Partial parsing: updated file: default://models/stage/stage_users.sql
[0m16:30:50.129664 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ede38d45-3dcb-4e70-9081-f2cad44e4078', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1474ba810>]}
[0m16:30:50.137204 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ede38d45-3dcb-4e70-9081-f2cad44e4078', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x14771b350>]}
[0m16:30:50.137480 [info ] [MainThread]: Found 3 models, 3 sources, 0 exposures, 0 metrics, 538 macros, 0 groups, 0 semantic models
[0m16:30:50.137671 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ede38d45-3dcb-4e70-9081-f2cad44e4078', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1476d6f90>]}
[0m16:30:50.138343 [info ] [MainThread]: 
[0m16:30:50.138790 [debug] [MainThread]: Databricks adapter: conn: 5493624528: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (70691, 7965269056), cmpt: ``, lut: None
[0m16:30:50.138942 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:30:50.139073 [debug] [MainThread]: Databricks adapter: Thread (70691, 7965269056) using default compute resource.
[0m16:30:50.139200 [debug] [MainThread]: Databricks adapter: conn: 5493624528: _acquire sess: None, name: master, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (70691, 7965269056), cmpt: ``, lut: 1710790250.139158
[0m16:30:50.139736 [debug] [ThreadPool]: Databricks adapter: conn: 5490674320: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore, idle: 0s, acqrelcnt: 0, lang: None, thrd: (70691, 11156877312), cmpt: ``, lut: None
[0m16:30:50.139920 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m16:30:50.140052 [debug] [ThreadPool]: Databricks adapter: Thread (70691, 11156877312) using default compute resource.
[0m16:30:50.140184 [debug] [ThreadPool]: Databricks adapter: conn: 5490674320: _acquire sess: None, name: list_hive_metastore, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (70691, 11156877312), cmpt: ``, lut: 1710790250.1401432
[0m16:30:50.140331 [debug] [ThreadPool]: Databricks adapter: conn: 5490674320: get_thread_connection: sess: None, name: list_hive_metastore, idle: 0.00014901161193847656s, acqrelcnt: 1, lang: None, thrd: (70691, 11156877312), cmpt: ``, lut: 1710790250.1401432
[0m16:30:50.140468 [debug] [ThreadPool]: Databricks adapter: conn: 5490674320: idle check connection: sess: None, name: list_hive_metastore, idle: 0.0002808570861816406s, acqrelcnt: 1, lang: None, thrd: (70691, 11156877312), cmpt: ``, lut: 1710790250.1401432
[0m16:30:50.140597 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m16:30:50.140723 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m16:30:50.140844 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:30:50.703242 [debug] [ThreadPool]: Databricks adapter: conn: 5490674320: session opened sess: 01eee55e-073c-1e12-aab1-5a18bca6d40e, name: list_hive_metastore, idle: 1.7881393432617188e-05s, acqrelcnt: 1, lang: None, thrd: (70691, 11156877312), cmpt: ``, lut: 1710790250.702774
[0m16:30:51.300286 [debug] [ThreadPool]: SQL status: OK in 1.159999966621399 seconds
[0m16:30:51.308229 [debug] [ThreadPool]: Databricks adapter: conn: 5490674320: _release sess: 01eee55e-073c-1e12-aab1-5a18bca6d40e, name: list_hive_metastore, idle: 3.0994415283203125e-06s, acqrelcnt: 0, lang: None, thrd: (70691, 11156877312), cmpt: ``, lut: 1710790251.308136
[0m16:30:51.309262 [debug] [ThreadPool]: Databricks adapter: conn: 5490674320: idle check connection: sess: 01eee55e-073c-1e12-aab1-5a18bca6d40e, name: list_hive_metastore, idle: 0.001055002212524414s, acqrelcnt: 0, lang: None, thrd: (70691, 11156877312), cmpt: ``, lut: 1710790251.308136
[0m16:30:51.309479 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now list_hive_metastore_default)
[0m16:30:51.309660 [debug] [ThreadPool]: Databricks adapter: conn: 5490674320: reusing connection list_hive_metastore sess: 01eee55e-073c-1e12-aab1-5a18bca6d40e, name: list_hive_metastore_default, idle: 0.0014681816101074219s, acqrelcnt: 0, lang: None, thrd: (70691, 11156877312), cmpt: ``, lut: 1710790251.308136
[0m16:30:51.309809 [debug] [ThreadPool]: Databricks adapter: Thread (70691, 11156877312) using default compute resource.
[0m16:30:51.309954 [debug] [ThreadPool]: Databricks adapter: conn: 5490674320: _acquire sess: 01eee55e-073c-1e12-aab1-5a18bca6d40e, name: list_hive_metastore_default, idle: 0.0017740726470947266s, acqrelcnt: 1, lang: None, thrd: (70691, 11156877312), cmpt: ``, lut: 1710790251.308136
[0m16:30:51.312325 [debug] [ThreadPool]: Databricks adapter: conn: 5490674320: get_thread_connection: sess: 01eee55e-073c-1e12-aab1-5a18bca6d40e, name: list_hive_metastore_default, idle: 0.00413203239440918s, acqrelcnt: 1, lang: None, thrd: (70691, 11156877312), cmpt: ``, lut: 1710790251.308136
[0m16:30:51.312497 [debug] [ThreadPool]: Databricks adapter: conn: 5490674320: idle check connection: sess: 01eee55e-073c-1e12-aab1-5a18bca6d40e, name: list_hive_metastore_default, idle: 0.00431513786315918s, acqrelcnt: 1, lang: None, thrd: (70691, 11156877312), cmpt: ``, lut: 1710790251.308136
[0m16:30:51.312632 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m16:30:51.312762 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m16:30:51.720978 [debug] [ThreadPool]: SQL status: OK in 0.4099999964237213 seconds
[0m16:30:51.734531 [debug] [ThreadPool]: Databricks adapter: conn: 5490674320: get_thread_connection: sess: 01eee55e-073c-1e12-aab1-5a18bca6d40e, name: list_hive_metastore_default, idle: 0.4262068271636963s, acqrelcnt: 1, lang: None, thrd: (70691, 11156877312), cmpt: ``, lut: 1710790251.308136
[0m16:30:51.735010 [debug] [ThreadPool]: Databricks adapter: conn: 5490674320: idle check connection: sess: 01eee55e-073c-1e12-aab1-5a18bca6d40e, name: list_hive_metastore_default, idle: 0.42679691314697266s, acqrelcnt: 1, lang: None, thrd: (70691, 11156877312), cmpt: ``, lut: 1710790251.308136
[0m16:30:51.735252 [debug] [ThreadPool]: Databricks adapter: conn: 5490674320: get_thread_connection: sess: 01eee55e-073c-1e12-aab1-5a18bca6d40e, name: list_hive_metastore_default, idle: 0.4270510673522949s, acqrelcnt: 1, lang: None, thrd: (70691, 11156877312), cmpt: ``, lut: 1710790251.308136
[0m16:30:51.735472 [debug] [ThreadPool]: Databricks adapter: conn: 5490674320: idle check connection: sess: 01eee55e-073c-1e12-aab1-5a18bca6d40e, name: list_hive_metastore_default, idle: 0.4272739887237549s, acqrelcnt: 1, lang: None, thrd: (70691, 11156877312), cmpt: ``, lut: 1710790251.308136
[0m16:30:51.735693 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m16:30:51.735888 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m16:30:51.736111 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m16:30:52.228000 [debug] [ThreadPool]: SQL status: OK in 0.49000000953674316 seconds
[0m16:30:52.232871 [debug] [ThreadPool]: Databricks adapter: conn: 5490674320: get_thread_connection: sess: 01eee55e-073c-1e12-aab1-5a18bca6d40e, name: list_hive_metastore_default, idle: 0.9246129989624023s, acqrelcnt: 1, lang: None, thrd: (70691, 11156877312), cmpt: ``, lut: 1710790251.308136
[0m16:30:52.233234 [debug] [ThreadPool]: Databricks adapter: conn: 5490674320: idle check connection: sess: 01eee55e-073c-1e12-aab1-5a18bca6d40e, name: list_hive_metastore_default, idle: 0.9250328540802002s, acqrelcnt: 1, lang: None, thrd: (70691, 11156877312), cmpt: ``, lut: 1710790251.308136
[0m16:30:52.233444 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m16:30:52.233638 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m16:30:52.810160 [debug] [ThreadPool]: SQL status: OK in 0.5799999833106995 seconds
[0m16:30:52.814610 [debug] [ThreadPool]: Databricks adapter: conn: 5490674320: _release sess: 01eee55e-073c-1e12-aab1-5a18bca6d40e, name: list_hive_metastore_default, idle: 1.9073486328125e-06s, acqrelcnt: 0, lang: None, thrd: (70691, 11156877312), cmpt: ``, lut: 1710790252.814464
[0m16:30:52.818354 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ede38d45-3dcb-4e70-9081-f2cad44e4078', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1476a0110>]}
[0m16:30:52.819015 [debug] [MainThread]: Databricks adapter: conn: 5493624528: get_thread_connection: sess: None, name: master, idle: 2.679744005203247s, acqrelcnt: 1, lang: None, thrd: (70691, 7965269056), cmpt: ``, lut: 1710790250.139158
[0m16:30:52.819300 [debug] [MainThread]: Databricks adapter: conn: 5493624528: idle check connection: sess: None, name: master, idle: 2.680060863494873s, acqrelcnt: 1, lang: None, thrd: (70691, 7965269056), cmpt: ``, lut: 1710790250.139158
[0m16:30:52.819559 [debug] [MainThread]: Databricks adapter: conn: 5493624528: get_thread_connection: sess: None, name: master, idle: 2.6803247928619385s, acqrelcnt: 1, lang: None, thrd: (70691, 7965269056), cmpt: ``, lut: 1710790250.139158
[0m16:30:52.819807 [debug] [MainThread]: Databricks adapter: conn: 5493624528: idle check connection: sess: None, name: master, idle: 2.68057918548584s, acqrelcnt: 1, lang: None, thrd: (70691, 7965269056), cmpt: ``, lut: 1710790250.139158
[0m16:30:52.820035 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:30:52.820251 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:30:52.820500 [debug] [MainThread]: Databricks adapter: conn: 5493624528: _release sess: None, name: master, idle: 2.1457672119140625e-06s, acqrelcnt: 0, lang: None, thrd: (70691, 7965269056), cmpt: ``, lut: 1710790252.820423
[0m16:30:52.821165 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:30:52.821467 [info ] [MainThread]: 
[0m16:30:52.826043 [debug] [Thread-1 (]: Began running node model.default.stage_users
[0m16:30:52.826557 [info ] [Thread-1 (]: 1 of 1 START sql view model default.stage_users ................................ [RUN]
[0m16:30:52.827689 [debug] [Thread-1 (]: Databricks adapter: conn: 5490674320: idle check connection: sess: 01eee55e-073c-1e12-aab1-5a18bca6d40e, name: list_hive_metastore_default, idle: 0.013077974319458008s, acqrelcnt: 0, lang: None, thrd: (70691, 11156877312), cmpt: ``, lut: 1710790252.814464
[0m16:30:52.827976 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.default.stage_users)
[0m16:30:52.828242 [debug] [Thread-1 (]: Databricks adapter: conn: 5490674320: reusing connection list_hive_metastore_default sess: 01eee55e-073c-1e12-aab1-5a18bca6d40e, name: model.default.stage_users, idle: 0.01367497444152832s, acqrelcnt: 0, lang: None, thrd: (70691, 11156877312), cmpt: ``, lut: 1710790252.814464
[0m16:30:52.828483 [debug] [Thread-1 (]: Databricks adapter: On thread (70691, 11156877312): `hive_metastore`.`default`.`stage_users` using default compute resource.
[0m16:30:52.828724 [debug] [Thread-1 (]: Databricks adapter: conn: 5490674320: _acquire sess: 01eee55e-073c-1e12-aab1-5a18bca6d40e, name: model.default.stage_users, idle: 0.014161825180053711s, acqrelcnt: 1, lang: sql, thrd: (70691, 11156877312), cmpt: ``, lut: 1710790252.814464
[0m16:30:52.828972 [debug] [Thread-1 (]: Began compiling node model.default.stage_users
[0m16:30:52.834431 [debug] [Thread-1 (]: Writing injected SQL for node "model.default.stage_users"
[0m16:30:52.838621 [debug] [Thread-1 (]: Timing info for model.default.stage_users (compile): 16:30:52.829129 => 16:30:52.838404
[0m16:30:52.838905 [debug] [Thread-1 (]: Began executing node model.default.stage_users
[0m16:30:52.854816 [debug] [Thread-1 (]: Writing runtime sql for node "model.default.stage_users"
[0m16:30:52.860242 [debug] [Thread-1 (]: Databricks adapter: conn: 5490674320: get_thread_connection: sess: 01eee55e-073c-1e12-aab1-5a18bca6d40e, name: model.default.stage_users, idle: 0.04567074775695801s, acqrelcnt: 1, lang: sql, thrd: (70691, 11156877312), cmpt: ``, lut: 1710790252.814464
[0m16:30:52.860466 [debug] [Thread-1 (]: Databricks adapter: conn: 5490674320: idle check connection: sess: 01eee55e-073c-1e12-aab1-5a18bca6d40e, name: model.default.stage_users, idle: 0.045925140380859375s, acqrelcnt: 1, lang: sql, thrd: (70691, 11156877312), cmpt: ``, lut: 1710790252.814464
[0m16:30:52.860634 [debug] [Thread-1 (]: Using databricks connection "model.default.stage_users"
[0m16:30:52.860849 [debug] [Thread-1 (]: On model.default.stage_users: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_users"} */
create or replace view `hive_metastore`.`default`.`stage_users`
  
  
  
  as
    select user_id AS user_id,
       name AS name,
       city AS city,
       phone_number AS phone_number,
       gender AS gender,
       nationality AS nationality,
       state AS state
from `pythiandbsql`.`default`.`users`

[0m16:30:53.252780 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_users"} */
create or replace view `hive_metastore`.`default`.`stage_users`
  
  
  
  as
    select user_id AS user_id,
       name AS name,
       city AS city,
       phone_number AS phone_number,
       gender AS gender,
       nationality AS nationality,
       state AS state
from `pythiandbsql`.`default`.`users`

[0m16:30:53.254213 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
[0m16:30:53.255600 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UC_NOT_ENABLED] org.apache.spark.sql.AnalysisException: [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:663)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:540)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:389)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:353)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:401)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:152)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$2(Analyzer.scala:1690)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1689)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.$anonfun$applyOrElse$82(Analyzer.scala:1442)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$handleGlueError(Analyzer.scala:1556)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1442)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1402)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:199)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:188)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:188)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:83)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:377)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1277)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1276)
	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:85)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:377)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1306)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1303)
	at org.apache.spark.sql.catalyst.plans.logical.CreateView.mapChildren(v2Commands.scala:1408)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:377)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:203)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:184)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruningByConf(AnalysisHelper.scala:173)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruningByConf$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruningByConf(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1402)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1372)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1289)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:309)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:309)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:289)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$9(RuleExecutor.scala:382)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$9$adapted(RuleExecutor.scala:382)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:382)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:256)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:414)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:407)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:321)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:407)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:248)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:248)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:392)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:384)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:391)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:230)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:394)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:542)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1048)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:542)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:538)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1173)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:538)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:224)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:223)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:481)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:503)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:576)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:531)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:576)
	... 35 more

[0m16:30:53.256917 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01eee55e-08a2-132c-95e3-1f44e0cf86e3
[0m16:30:53.257660 [debug] [Thread-1 (]: Timing info for model.default.stage_users (execute): 16:30:52.839049 => 16:30:53.257413
[0m16:30:53.258226 [debug] [Thread-1 (]: Databricks adapter: conn: 5490674320: _release sess: 01eee55e-073c-1e12-aab1-5a18bca6d40e, name: model.default.stage_users, idle: 7.867813110351562e-06s, acqrelcnt: 0, lang: sql, thrd: (70691, 11156877312), cmpt: ``, lut: 1710790253.2580621
[0m16:30:53.333235 [debug] [Thread-1 (]: Runtime Error in model stage_users (models/stage/stage_users.sql)
  [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
[0m16:30:53.335341 [debug] [Thread-1 (]: Databricks adapter: conn: 5490674320: _release sess: 01eee55e-073c-1e12-aab1-5a18bca6d40e, name: model.default.stage_users, idle: 1.0013580322265625e-05s, acqrelcnt: 0, lang: sql, thrd: (70691, 11156877312), cmpt: ``, lut: 1710790253.3344839
[0m16:30:53.336553 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ede38d45-3dcb-4e70-9081-f2cad44e4078', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1477c8c90>]}
[0m16:30:53.337362 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model default.stage_users ....................... [[31mERROR[0m in 0.51s]
[0m16:30:53.337947 [debug] [Thread-1 (]: Finished running node model.default.stage_users
[0m16:30:53.339974 [debug] [MainThread]: Databricks adapter: conn: 5493624528: idle check connection: sess: None, name: master, idle: 0.5193440914154053s, acqrelcnt: 0, lang: None, thrd: (70691, 7965269056), cmpt: ``, lut: 1710790252.820423
[0m16:30:53.340594 [debug] [MainThread]: Databricks adapter: conn: 5493624528: reusing connection master sess: None, name: master, idle: 0.5200672149658203s, acqrelcnt: 0, lang: None, thrd: (70691, 7965269056), cmpt: ``, lut: 1710790252.820423
[0m16:30:53.340915 [debug] [MainThread]: Databricks adapter: Thread (70691, 7965269056) using default compute resource.
[0m16:30:53.341199 [debug] [MainThread]: Databricks adapter: conn: 5493624528: _acquire sess: None, name: master, idle: 0.5206902027130127s, acqrelcnt: 1, lang: None, thrd: (70691, 7965269056), cmpt: ``, lut: 1710790252.820423
[0m16:30:53.341514 [debug] [MainThread]: Databricks adapter: conn: 5493624528: get_thread_connection: sess: None, name: master, idle: 0.521003007888794s, acqrelcnt: 1, lang: None, thrd: (70691, 7965269056), cmpt: ``, lut: 1710790252.820423
[0m16:30:53.341806 [debug] [MainThread]: Databricks adapter: conn: 5493624528: idle check connection: sess: None, name: master, idle: 0.5213010311126709s, acqrelcnt: 1, lang: None, thrd: (70691, 7965269056), cmpt: ``, lut: 1710790252.820423
[0m16:30:53.342078 [debug] [MainThread]: On master: ROLLBACK
[0m16:30:53.342350 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:30:53.810297 [debug] [MainThread]: Databricks adapter: conn: 5493624528: session opened sess: 01eee55e-0918-17cd-9215-fad51e97571e, name: master, idle: 3.814697265625e-06s, acqrelcnt: 1, lang: None, thrd: (70691, 7965269056), cmpt: ``, lut: 1710790253.8100832
[0m16:30:53.810868 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:30:53.811174 [debug] [MainThread]: Databricks adapter: conn: 5493624528: get_thread_connection: sess: 01eee55e-0918-17cd-9215-fad51e97571e, name: master, idle: 0.0010077953338623047s, acqrelcnt: 1, lang: None, thrd: (70691, 7965269056), cmpt: ``, lut: 1710790253.8100832
[0m16:30:53.811446 [debug] [MainThread]: Databricks adapter: conn: 5493624528: idle check connection: sess: 01eee55e-0918-17cd-9215-fad51e97571e, name: master, idle: 0.0012848377227783203s, acqrelcnt: 1, lang: None, thrd: (70691, 7965269056), cmpt: ``, lut: 1710790253.8100832
[0m16:30:53.811700 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:30:53.811922 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:30:53.812192 [debug] [MainThread]: Databricks adapter: conn: 5493624528: _release sess: 01eee55e-0918-17cd-9215-fad51e97571e, name: master, idle: 9.5367431640625e-07s, acqrelcnt: 0, lang: None, thrd: (70691, 7965269056), cmpt: ``, lut: 1710790253.8121119
[0m16:30:53.812848 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:30:53.813101 [debug] [MainThread]: On master: ROLLBACK
[0m16:30:53.813326 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:30:53.813552 [debug] [MainThread]: On master: Close
[0m16:30:53.974968 [debug] [MainThread]: Connection 'model.default.stage_users' was properly closed.
[0m16:30:53.976114 [debug] [MainThread]: On model.default.stage_users: ROLLBACK
[0m16:30:53.977039 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:30:53.977745 [debug] [MainThread]: On model.default.stage_users: Close
[0m16:30:54.158881 [info ] [MainThread]: 
[0m16:30:54.160161 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 4.02 seconds (4.02s).
[0m16:30:54.161942 [debug] [MainThread]: Command end result
[0m16:30:54.229777 [info ] [MainThread]: 
[0m16:30:54.230323 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m16:30:54.230593 [info ] [MainThread]: 
[0m16:30:54.230860 [error] [MainThread]:   Runtime Error in model stage_users (models/stage/stage_users.sql)
  [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
[0m16:30:54.231114 [info ] [MainThread]: 
[0m16:30:54.231416 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m16:30:54.240204 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 5.8597198, "process_user_time": 2.213575, "process_kernel_time": 3.45873, "process_mem_max_rss": "222707712", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:30:54.240763 [debug] [MainThread]: Command `cli run` failed at 16:30:54.240650 after 5.86 seconds
[0m16:30:54.241169 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12119dbd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1211c1d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1027bbcd0>]}
[0m16:30:54.241503 [debug] [MainThread]: Flushing usage events
[0m16:33:05.244660 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111b7e3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111bfd9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111bfe010>]}


============================== 16:33:05.247518 | 5ab2a8b6-1374-430d-8a9e-a90f5597b002 ==============================
[0m16:33:05.247518 [info ] [MainThread]: Running with dbt=1.7.8
[0m16:33:05.247810 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'version_check': 'True', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:33:07.028866 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5ab2a8b6-1374-430d-8a9e-a90f5597b002', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x157975090>]}
[0m16:33:07.057022 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5ab2a8b6-1374-430d-8a9e-a90f5597b002', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111245590>]}
[0m16:33:07.057277 [info ] [MainThread]: Registered adapter: databricks=1.7.9
[0m16:33:07.076888 [debug] [MainThread]: checksum: 67f0013ca5f0bd43af9a0873dd50792fde83ef69de63b71cacd0b4ac656c52e5, vars: {}, profile: , target: , version: 1.7.8
[0m16:33:07.087983 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m16:33:07.088267 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '5ab2a8b6-1374-430d-8a9e-a90f5597b002', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x157986110>]}
[0m16:33:07.653049 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5ab2a8b6-1374-430d-8a9e-a90f5597b002', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x158cb9e10>]}
[0m16:33:07.667324 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5ab2a8b6-1374-430d-8a9e-a90f5597b002', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x158f8b210>]}
[0m16:33:07.667534 [info ] [MainThread]: Found 3 models, 3 sources, 0 exposures, 0 metrics, 538 macros, 0 groups, 0 semantic models
[0m16:33:07.667709 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5ab2a8b6-1374-430d-8a9e-a90f5597b002', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1118028d0>]}
[0m16:33:07.668293 [info ] [MainThread]: 
[0m16:33:07.668451 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m16:33:07.668666 [debug] [MainThread]: Command end result
[0m16:33:07.707953 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 2.4922674, "process_user_time": 2.507753, "process_kernel_time": 3.36256, "process_mem_max_rss": "212992000", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:33:07.708284 [debug] [MainThread]: Command `cli test` succeeded at 16:33:07.708223 after 2.49 seconds
[0m16:33:07.708497 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111bde510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111bddcd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111b6cb50>]}
[0m16:33:07.708677 [debug] [MainThread]: Flushing usage events
[0m16:33:16.176360 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b6ced90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b6cfb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b6fe5d0>]}


============================== 16:33:16.178683 | 3cb55e34-d5bf-4ba5-adf8-799a051a777a ==============================
[0m16:33:16.178683 [info ] [MainThread]: Running with dbt=1.7.8
[0m16:33:16.178976 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m16:33:17.323495 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3cb55e34-d5bf-4ba5-adf8-799a051a777a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12f8f4710>]}
[0m16:33:17.351772 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3cb55e34-d5bf-4ba5-adf8-799a051a777a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12fa816d0>]}
[0m16:33:17.352036 [info ] [MainThread]: Registered adapter: databricks=1.7.9
[0m16:33:17.368302 [debug] [MainThread]: checksum: 67f0013ca5f0bd43af9a0873dd50792fde83ef69de63b71cacd0b4ac656c52e5, vars: {}, profile: , target: , version: 1.7.8
[0m16:33:17.450851 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:33:17.451091 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:33:17.454250 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3cb55e34-d5bf-4ba5-adf8-799a051a777a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12fa812d0>]}
[0m16:33:17.461988 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3cb55e34-d5bf-4ba5-adf8-799a051a777a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12fa35b50>]}
[0m16:33:17.462208 [info ] [MainThread]: Found 3 models, 3 sources, 0 exposures, 0 metrics, 538 macros, 0 groups, 0 semantic models
[0m16:33:17.462370 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3cb55e34-d5bf-4ba5-adf8-799a051a777a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12fb506d0>]}
[0m16:33:17.463089 [info ] [MainThread]: 
[0m16:33:17.463524 [debug] [MainThread]: Databricks adapter: conn: 5094008592: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (71547, 7965269056), cmpt: ``, lut: None
[0m16:33:17.463671 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:33:17.463790 [debug] [MainThread]: Databricks adapter: Thread (71547, 7965269056) using default compute resource.
[0m16:33:17.463905 [debug] [MainThread]: Databricks adapter: conn: 5094008592: _acquire sess: None, name: master, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (71547, 7965269056), cmpt: ``, lut: 1710790397.463867
[0m16:33:17.464419 [debug] [ThreadPool]: Databricks adapter: conn: 5095672976: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore, idle: 0s, acqrelcnt: 0, lang: None, thrd: (71547, 11425312768), cmpt: ``, lut: None
[0m16:33:17.464595 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m16:33:17.464715 [debug] [ThreadPool]: Databricks adapter: Thread (71547, 11425312768) using default compute resource.
[0m16:33:17.464832 [debug] [ThreadPool]: Databricks adapter: conn: 5095672976: _acquire sess: None, name: list_hive_metastore, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (71547, 11425312768), cmpt: ``, lut: 1710790397.4647949
[0m16:33:17.464974 [debug] [ThreadPool]: Databricks adapter: conn: 5095672976: get_thread_connection: sess: None, name: list_hive_metastore, idle: 0.000141143798828125s, acqrelcnt: 1, lang: None, thrd: (71547, 11425312768), cmpt: ``, lut: 1710790397.4647949
[0m16:33:17.465096 [debug] [ThreadPool]: Databricks adapter: conn: 5095672976: idle check connection: sess: None, name: list_hive_metastore, idle: 0.00026416778564453125s, acqrelcnt: 1, lang: None, thrd: (71547, 11425312768), cmpt: ``, lut: 1710790397.4647949
[0m16:33:17.465204 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m16:33:17.465312 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m16:33:17.465416 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:33:17.996386 [debug] [ThreadPool]: Databricks adapter: conn: 5095672976: session opened sess: 01eee55e-5f09-191f-9f20-0d7251d3836d, name: list_hive_metastore, idle: 4.0531158447265625e-06s, acqrelcnt: 1, lang: None, thrd: (71547, 11425312768), cmpt: ``, lut: 1710790397.996234
[0m16:33:18.438096 [debug] [ThreadPool]: SQL status: OK in 0.9700000286102295 seconds
[0m16:33:18.447519 [debug] [ThreadPool]: Databricks adapter: conn: 5095672976: _release sess: 01eee55e-5f09-191f-9f20-0d7251d3836d, name: list_hive_metastore, idle: 4.76837158203125e-06s, acqrelcnt: 0, lang: None, thrd: (71547, 11425312768), cmpt: ``, lut: 1710790398.4473631
[0m16:33:18.448963 [debug] [ThreadPool]: Databricks adapter: conn: 5095672976: idle check connection: sess: 01eee55e-5f09-191f-9f20-0d7251d3836d, name: list_hive_metastore, idle: 0.0015077590942382812s, acqrelcnt: 0, lang: None, thrd: (71547, 11425312768), cmpt: ``, lut: 1710790398.4473631
[0m16:33:18.449269 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now list_hive_metastore_default)
[0m16:33:18.449513 [debug] [ThreadPool]: Databricks adapter: conn: 5095672976: reusing connection list_hive_metastore sess: 01eee55e-5f09-191f-9f20-0d7251d3836d, name: list_hive_metastore_default, idle: 0.0020759105682373047s, acqrelcnt: 0, lang: None, thrd: (71547, 11425312768), cmpt: ``, lut: 1710790398.4473631
[0m16:33:18.449744 [debug] [ThreadPool]: Databricks adapter: Thread (71547, 11425312768) using default compute resource.
[0m16:33:18.449956 [debug] [ThreadPool]: Databricks adapter: conn: 5095672976: _acquire sess: 01eee55e-5f09-191f-9f20-0d7251d3836d, name: list_hive_metastore_default, idle: 0.002526998519897461s, acqrelcnt: 1, lang: None, thrd: (71547, 11425312768), cmpt: ``, lut: 1710790398.4473631
[0m16:33:18.453579 [debug] [ThreadPool]: Databricks adapter: conn: 5095672976: get_thread_connection: sess: 01eee55e-5f09-191f-9f20-0d7251d3836d, name: list_hive_metastore_default, idle: 0.006140947341918945s, acqrelcnt: 1, lang: None, thrd: (71547, 11425312768), cmpt: ``, lut: 1710790398.4473631
[0m16:33:18.453806 [debug] [ThreadPool]: Databricks adapter: conn: 5095672976: idle check connection: sess: 01eee55e-5f09-191f-9f20-0d7251d3836d, name: list_hive_metastore_default, idle: 0.006381034851074219s, acqrelcnt: 1, lang: None, thrd: (71547, 11425312768), cmpt: ``, lut: 1710790398.4473631
[0m16:33:18.453983 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m16:33:18.454157 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m16:33:18.775783 [debug] [ThreadPool]: SQL status: OK in 0.3199999928474426 seconds
[0m16:33:18.784913 [debug] [ThreadPool]: Databricks adapter: conn: 5095672976: get_thread_connection: sess: 01eee55e-5f09-191f-9f20-0d7251d3836d, name: list_hive_metastore_default, idle: 0.3374347686767578s, acqrelcnt: 1, lang: None, thrd: (71547, 11425312768), cmpt: ``, lut: 1710790398.4473631
[0m16:33:18.785255 [debug] [ThreadPool]: Databricks adapter: conn: 5095672976: idle check connection: sess: 01eee55e-5f09-191f-9f20-0d7251d3836d, name: list_hive_metastore_default, idle: 0.337817907333374s, acqrelcnt: 1, lang: None, thrd: (71547, 11425312768), cmpt: ``, lut: 1710790398.4473631
[0m16:33:18.785517 [debug] [ThreadPool]: Databricks adapter: conn: 5095672976: get_thread_connection: sess: 01eee55e-5f09-191f-9f20-0d7251d3836d, name: list_hive_metastore_default, idle: 0.3380889892578125s, acqrelcnt: 1, lang: None, thrd: (71547, 11425312768), cmpt: ``, lut: 1710790398.4473631
[0m16:33:18.785737 [debug] [ThreadPool]: Databricks adapter: conn: 5095672976: idle check connection: sess: 01eee55e-5f09-191f-9f20-0d7251d3836d, name: list_hive_metastore_default, idle: 0.33830976486206055s, acqrelcnt: 1, lang: None, thrd: (71547, 11425312768), cmpt: ``, lut: 1710790398.4473631
[0m16:33:18.785946 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m16:33:18.786138 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m16:33:18.786344 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m16:33:19.148907 [debug] [ThreadPool]: SQL status: OK in 0.36000001430511475 seconds
[0m16:33:19.157566 [debug] [ThreadPool]: Databricks adapter: conn: 5095672976: get_thread_connection: sess: 01eee55e-5f09-191f-9f20-0d7251d3836d, name: list_hive_metastore_default, idle: 0.710075855255127s, acqrelcnt: 1, lang: None, thrd: (71547, 11425312768), cmpt: ``, lut: 1710790398.4473631
[0m16:33:19.157964 [debug] [ThreadPool]: Databricks adapter: conn: 5095672976: idle check connection: sess: 01eee55e-5f09-191f-9f20-0d7251d3836d, name: list_hive_metastore_default, idle: 0.7105178833007812s, acqrelcnt: 1, lang: None, thrd: (71547, 11425312768), cmpt: ``, lut: 1710790398.4473631
[0m16:33:19.158212 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m16:33:19.158465 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m16:33:19.462385 [debug] [ThreadPool]: SQL status: OK in 0.30000001192092896 seconds
[0m16:33:19.468499 [debug] [ThreadPool]: Databricks adapter: conn: 5095672976: _release sess: 01eee55e-5f09-191f-9f20-0d7251d3836d, name: list_hive_metastore_default, idle: 9.059906005859375e-06s, acqrelcnt: 0, lang: None, thrd: (71547, 11425312768), cmpt: ``, lut: 1710790399.468261
[0m16:33:19.472937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3cb55e34-d5bf-4ba5-adf8-799a051a777a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12fb26650>]}
[0m16:33:19.473763 [debug] [MainThread]: Databricks adapter: conn: 5094008592: get_thread_connection: sess: None, name: master, idle: 2.0097849369049072s, acqrelcnt: 1, lang: None, thrd: (71547, 7965269056), cmpt: ``, lut: 1710790397.463867
[0m16:33:19.474057 [debug] [MainThread]: Databricks adapter: conn: 5094008592: idle check connection: sess: None, name: master, idle: 2.0101101398468018s, acqrelcnt: 1, lang: None, thrd: (71547, 7965269056), cmpt: ``, lut: 1710790397.463867
[0m16:33:19.474326 [debug] [MainThread]: Databricks adapter: conn: 5094008592: get_thread_connection: sess: None, name: master, idle: 2.0103840827941895s, acqrelcnt: 1, lang: None, thrd: (71547, 7965269056), cmpt: ``, lut: 1710790397.463867
[0m16:33:19.474571 [debug] [MainThread]: Databricks adapter: conn: 5094008592: idle check connection: sess: None, name: master, idle: 2.0106332302093506s, acqrelcnt: 1, lang: None, thrd: (71547, 7965269056), cmpt: ``, lut: 1710790397.463867
[0m16:33:19.474828 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:33:19.475048 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:33:19.475300 [debug] [MainThread]: Databricks adapter: conn: 5094008592: _release sess: None, name: master, idle: 1.1920928955078125e-06s, acqrelcnt: 0, lang: None, thrd: (71547, 7965269056), cmpt: ``, lut: 1710790399.475221
[0m16:33:19.475983 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:33:19.476280 [info ] [MainThread]: 
[0m16:33:19.481904 [debug] [Thread-1 (]: Began running node model.default.stage_users
[0m16:33:19.482444 [info ] [Thread-1 (]: 1 of 1 START sql view model default.stage_users ................................ [RUN]
[0m16:33:19.483181 [debug] [Thread-1 (]: Databricks adapter: conn: 5095672976: idle check connection: sess: 01eee55e-5f09-191f-9f20-0d7251d3836d, name: list_hive_metastore_default, idle: 0.014783143997192383s, acqrelcnt: 0, lang: None, thrd: (71547, 11425312768), cmpt: ``, lut: 1710790399.468261
[0m16:33:19.483468 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.default.stage_users)
[0m16:33:19.483761 [debug] [Thread-1 (]: Databricks adapter: conn: 5095672976: reusing connection list_hive_metastore_default sess: 01eee55e-5f09-191f-9f20-0d7251d3836d, name: model.default.stage_users, idle: 0.01538395881652832s, acqrelcnt: 0, lang: None, thrd: (71547, 11425312768), cmpt: ``, lut: 1710790399.468261
[0m16:33:19.484042 [debug] [Thread-1 (]: Databricks adapter: On thread (71547, 11425312768): `hive_metastore`.`default`.`stage_users` using default compute resource.
[0m16:33:19.484309 [debug] [Thread-1 (]: Databricks adapter: conn: 5095672976: _acquire sess: 01eee55e-5f09-191f-9f20-0d7251d3836d, name: model.default.stage_users, idle: 0.015943050384521484s, acqrelcnt: 1, lang: sql, thrd: (71547, 11425312768), cmpt: ``, lut: 1710790399.468261
[0m16:33:19.484594 [debug] [Thread-1 (]: Began compiling node model.default.stage_users
[0m16:33:19.490256 [debug] [Thread-1 (]: Writing injected SQL for node "model.default.stage_users"
[0m16:33:19.497544 [debug] [Thread-1 (]: Timing info for model.default.stage_users (compile): 16:33:19.484776 => 16:33:19.497287
[0m16:33:19.497939 [debug] [Thread-1 (]: Began executing node model.default.stage_users
[0m16:33:19.515114 [debug] [Thread-1 (]: Writing runtime sql for node "model.default.stage_users"
[0m16:33:19.518482 [debug] [Thread-1 (]: Databricks adapter: conn: 5095672976: get_thread_connection: sess: 01eee55e-5f09-191f-9f20-0d7251d3836d, name: model.default.stage_users, idle: 0.05006909370422363s, acqrelcnt: 1, lang: sql, thrd: (71547, 11425312768), cmpt: ``, lut: 1710790399.468261
[0m16:33:19.518811 [debug] [Thread-1 (]: Databricks adapter: conn: 5095672976: idle check connection: sess: 01eee55e-5f09-191f-9f20-0d7251d3836d, name: model.default.stage_users, idle: 0.05045890808105469s, acqrelcnt: 1, lang: sql, thrd: (71547, 11425312768), cmpt: ``, lut: 1710790399.468261
[0m16:33:19.518999 [debug] [Thread-1 (]: Using databricks connection "model.default.stage_users"
[0m16:33:19.519233 [debug] [Thread-1 (]: On model.default.stage_users: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_users"} */
create or replace view `hive_metastore`.`default`.`stage_users`
  
  
  
  as
    

select user_id AS user_id,
       name AS name,
       city AS city,
       phone_number AS phone_number,
       gender AS gender,
       nationality AS nationality,
       state AS state
from `pythiandbsql`.`default`.`users`

[0m16:33:19.850654 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_users"} */
create or replace view `hive_metastore`.`default`.`stage_users`
  
  
  
  as
    

select user_id AS user_id,
       name AS name,
       city AS city,
       phone_number AS phone_number,
       gender AS gender,
       nationality AS nationality,
       state AS state
from `pythiandbsql`.`default`.`users`

[0m16:33:19.851981 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
[0m16:33:19.853580 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UC_NOT_ENABLED] org.apache.spark.sql.AnalysisException: [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:663)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:540)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:389)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:353)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:401)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:152)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$2(Analyzer.scala:1690)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1689)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.$anonfun$applyOrElse$82(Analyzer.scala:1442)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$handleGlueError(Analyzer.scala:1556)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1442)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1402)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:199)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:188)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:188)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:83)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:377)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1277)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1276)
	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:85)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:377)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1306)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1303)
	at org.apache.spark.sql.catalyst.plans.logical.CreateView.mapChildren(v2Commands.scala:1408)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:377)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:203)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:184)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruningByConf(AnalysisHelper.scala:173)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruningByConf$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruningByConf(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1402)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1372)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1289)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:309)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:309)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:289)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$9(RuleExecutor.scala:382)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$9$adapted(RuleExecutor.scala:382)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:382)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:256)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:414)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:407)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:321)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:407)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:248)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:248)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:392)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:384)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:391)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:230)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:394)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:542)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1048)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:542)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:538)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1173)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:538)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:224)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:223)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:481)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:503)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:576)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:531)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:576)
	... 35 more

[0m16:33:19.855330 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01eee55e-600b-1991-b543-48a70252940c
[0m16:33:19.856132 [debug] [Thread-1 (]: Timing info for model.default.stage_users (execute): 16:33:19.498083 => 16:33:19.855856
[0m16:33:19.857002 [debug] [Thread-1 (]: Databricks adapter: conn: 5095672976: _release sess: 01eee55e-5f09-191f-9f20-0d7251d3836d, name: model.default.stage_users, idle: 7.152557373046875e-06s, acqrelcnt: 0, lang: sql, thrd: (71547, 11425312768), cmpt: ``, lut: 1710790399.856787
[0m16:33:19.889050 [debug] [Thread-1 (]: Runtime Error in model stage_users (models/stage/stage_users.sql)
  [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
[0m16:33:19.889712 [debug] [Thread-1 (]: Databricks adapter: conn: 5095672976: _release sess: 01eee55e-5f09-191f-9f20-0d7251d3836d, name: model.default.stage_users, idle: 3.0994415283203125e-06s, acqrelcnt: 0, lang: sql, thrd: (71547, 11425312768), cmpt: ``, lut: 1710790399.889534
[0m16:33:19.890168 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3cb55e34-d5bf-4ba5-adf8-799a051a777a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12fbead90>]}
[0m16:33:19.890747 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model default.stage_users ....................... [[31mERROR[0m in 0.41s]
[0m16:33:19.891227 [debug] [Thread-1 (]: Finished running node model.default.stage_users
[0m16:33:19.892701 [debug] [MainThread]: Databricks adapter: conn: 5094008592: idle check connection: sess: None, name: master, idle: 0.4173581600189209s, acqrelcnt: 0, lang: None, thrd: (71547, 7965269056), cmpt: ``, lut: 1710790399.475221
[0m16:33:19.893071 [debug] [MainThread]: Databricks adapter: conn: 5094008592: reusing connection master sess: None, name: master, idle: 0.41776204109191895s, acqrelcnt: 0, lang: None, thrd: (71547, 7965269056), cmpt: ``, lut: 1710790399.475221
[0m16:33:19.893335 [debug] [MainThread]: Databricks adapter: Thread (71547, 7965269056) using default compute resource.
[0m16:33:19.893581 [debug] [MainThread]: Databricks adapter: conn: 5094008592: _acquire sess: None, name: master, idle: 0.4182889461517334s, acqrelcnt: 1, lang: None, thrd: (71547, 7965269056), cmpt: ``, lut: 1710790399.475221
[0m16:33:19.893856 [debug] [MainThread]: Databricks adapter: conn: 5094008592: get_thread_connection: sess: None, name: master, idle: 0.4185621738433838s, acqrelcnt: 1, lang: None, thrd: (71547, 7965269056), cmpt: ``, lut: 1710790399.475221
[0m16:33:19.894101 [debug] [MainThread]: Databricks adapter: conn: 5094008592: idle check connection: sess: None, name: master, idle: 0.4188082218170166s, acqrelcnt: 1, lang: None, thrd: (71547, 7965269056), cmpt: ``, lut: 1710790399.475221
[0m16:33:19.894335 [debug] [MainThread]: On master: ROLLBACK
[0m16:33:19.894560 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:33:20.373667 [debug] [MainThread]: Databricks adapter: conn: 5094008592: session opened sess: 01eee55e-6073-1689-987e-2d893de364d1, name: master, idle: 1.3828277587890625e-05s, acqrelcnt: 1, lang: None, thrd: (71547, 7965269056), cmpt: ``, lut: 1710790400.3731341
[0m16:33:20.375391 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:33:20.376406 [debug] [MainThread]: Databricks adapter: conn: 5094008592: get_thread_connection: sess: 01eee55e-6073-1689-987e-2d893de364d1, name: master, idle: 0.0030438899993896484s, acqrelcnt: 1, lang: None, thrd: (71547, 7965269056), cmpt: ``, lut: 1710790400.3731341
[0m16:33:20.377281 [debug] [MainThread]: Databricks adapter: conn: 5094008592: idle check connection: sess: 01eee55e-6073-1689-987e-2d893de364d1, name: master, idle: 0.004002809524536133s, acqrelcnt: 1, lang: None, thrd: (71547, 7965269056), cmpt: ``, lut: 1710790400.3731341
[0m16:33:20.377851 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:33:20.378333 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:33:20.378884 [debug] [MainThread]: Databricks adapter: conn: 5094008592: _release sess: 01eee55e-6073-1689-987e-2d893de364d1, name: master, idle: 4.291534423828125e-06s, acqrelcnt: 0, lang: None, thrd: (71547, 7965269056), cmpt: ``, lut: 1710790400.3787708
[0m16:33:20.380041 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:33:20.380486 [debug] [MainThread]: On master: ROLLBACK
[0m16:33:20.380875 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:33:20.381254 [debug] [MainThread]: On master: Close
[0m16:33:20.563645 [debug] [MainThread]: Connection 'model.default.stage_users' was properly closed.
[0m16:33:20.564692 [debug] [MainThread]: On model.default.stage_users: ROLLBACK
[0m16:33:20.565347 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:33:20.565958 [debug] [MainThread]: On model.default.stage_users: Close
[0m16:33:20.733552 [info ] [MainThread]: 
[0m16:33:20.734753 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.27 seconds (3.27s).
[0m16:33:20.736810 [debug] [MainThread]: Command end result
[0m16:33:20.773721 [info ] [MainThread]: 
[0m16:33:20.774226 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m16:33:20.774525 [info ] [MainThread]: 
[0m16:33:20.774970 [error] [MainThread]:   Runtime Error in model stage_users (models/stage/stage_users.sql)
  [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
[0m16:33:20.775282 [info ] [MainThread]: 
[0m16:33:20.775584 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m16:33:20.782966 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 4.6337395, "process_user_time": 2.082425, "process_kernel_time": 3.392222, "process_mem_max_rss": "221446144", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:33:20.783544 [debug] [MainThread]: Command `cli run` failed at 16:33:20.783436 after 4.63 seconds
[0m16:33:20.783951 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b6ce750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b6fddd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x101293cd0>]}
[0m16:33:20.784277 [debug] [MainThread]: Flushing usage events
[0m16:36:03.257272 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b996f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b941290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a7e7d10>]}


============================== 16:36:03.260179 | f8fe6832-1988-4487-b43e-ee42afbcc4d5 ==============================
[0m16:36:03.260179 [info ] [MainThread]: Running with dbt=1.7.8
[0m16:36:03.260480 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:36:05.045639 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f8fe6832-1988-4487-b43e-ee42afbcc4d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x2862f4b10>]}
[0m16:36:05.074104 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f8fe6832-1988-4487-b43e-ee42afbcc4d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b9c1a10>]}
[0m16:36:05.074410 [info ] [MainThread]: Registered adapter: databricks=1.7.9
[0m16:36:05.093514 [debug] [MainThread]: checksum: 67f0013ca5f0bd43af9a0873dd50792fde83ef69de63b71cacd0b4ac656c52e5, vars: {}, profile: , target: , version: 1.7.8
[0m16:36:05.177705 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:36:05.178161 [debug] [MainThread]: Partial parsing: updated file: default://models/sources.yml
[0m16:36:05.259005 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f8fe6832-1988-4487-b43e-ee42afbcc4d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16ba8ded0>]}
[0m16:36:05.272796 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f8fe6832-1988-4487-b43e-ee42afbcc4d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x286551450>]}
[0m16:36:05.273046 [info ] [MainThread]: Found 3 models, 3 sources, 0 exposures, 0 metrics, 538 macros, 0 groups, 0 semantic models
[0m16:36:05.273219 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f8fe6832-1988-4487-b43e-ee42afbcc4d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16ba2a490>]}
[0m16:36:05.273880 [info ] [MainThread]: 
[0m16:36:05.274352 [debug] [MainThread]: Databricks adapter: conn: 10842354768: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (72509, 7965269056), cmpt: ``, lut: None
[0m16:36:05.274546 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:36:05.274682 [debug] [MainThread]: Databricks adapter: Thread (72509, 7965269056) using default compute resource.
[0m16:36:05.274802 [debug] [MainThread]: Databricks adapter: conn: 10842354768: _acquire sess: None, name: master, idle: 1.1920928955078125e-06s, acqrelcnt: 1, lang: None, thrd: (72509, 7965269056), cmpt: ``, lut: 1710790565.274762
[0m16:36:05.275316 [debug] [ThreadPool]: Databricks adapter: conn: 10843195984: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore, idle: 0s, acqrelcnt: 0, lang: None, thrd: (72509, 11428507648), cmpt: ``, lut: None
[0m16:36:05.275497 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m16:36:05.275623 [debug] [ThreadPool]: Databricks adapter: Thread (72509, 11428507648) using default compute resource.
[0m16:36:05.275751 [debug] [ThreadPool]: Databricks adapter: conn: 10843195984: _acquire sess: None, name: list_hive_metastore, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (72509, 11428507648), cmpt: ``, lut: 1710790565.275712
[0m16:36:05.275890 [debug] [ThreadPool]: Databricks adapter: conn: 10843195984: get_thread_connection: sess: None, name: list_hive_metastore, idle: 0.000141143798828125s, acqrelcnt: 1, lang: None, thrd: (72509, 11428507648), cmpt: ``, lut: 1710790565.275712
[0m16:36:05.276018 [debug] [ThreadPool]: Databricks adapter: conn: 10843195984: idle check connection: sess: None, name: list_hive_metastore, idle: 0.00026297569274902344s, acqrelcnt: 1, lang: None, thrd: (72509, 11428507648), cmpt: ``, lut: 1710790565.275712
[0m16:36:05.276132 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m16:36:05.276241 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m16:36:05.276353 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:36:05.797913 [debug] [ThreadPool]: Databricks adapter: conn: 10843195984: session opened sess: 01eee55e-c30e-1301-bc51-9d018e76ff8f, name: list_hive_metastore, idle: 1.0967254638671875e-05s, acqrelcnt: 1, lang: None, thrd: (72509, 11428507648), cmpt: ``, lut: 1710790565.797714
[0m16:36:06.198415 [debug] [ThreadPool]: SQL status: OK in 0.9200000166893005 seconds
[0m16:36:06.209534 [debug] [ThreadPool]: Databricks adapter: conn: 10843195984: _release sess: 01eee55e-c30e-1301-bc51-9d018e76ff8f, name: list_hive_metastore, idle: 7.867813110351562e-06s, acqrelcnt: 0, lang: None, thrd: (72509, 11428507648), cmpt: ``, lut: 1710790566.209363
[0m16:36:06.211648 [debug] [ThreadPool]: Databricks adapter: conn: 10843195984: idle check connection: sess: 01eee55e-c30e-1301-bc51-9d018e76ff8f, name: list_hive_metastore, idle: 0.002157926559448242s, acqrelcnt: 0, lang: None, thrd: (72509, 11428507648), cmpt: ``, lut: 1710790566.209363
[0m16:36:06.212062 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now list_hive_metastore_default)
[0m16:36:06.212294 [debug] [ThreadPool]: Databricks adapter: conn: 10843195984: reusing connection list_hive_metastore sess: 01eee55e-c30e-1301-bc51-9d018e76ff8f, name: list_hive_metastore_default, idle: 0.0028641223907470703s, acqrelcnt: 0, lang: None, thrd: (72509, 11428507648), cmpt: ``, lut: 1710790566.209363
[0m16:36:06.212498 [debug] [ThreadPool]: Databricks adapter: Thread (72509, 11428507648) using default compute resource.
[0m16:36:06.212708 [debug] [ThreadPool]: Databricks adapter: conn: 10843195984: _acquire sess: 01eee55e-c30e-1301-bc51-9d018e76ff8f, name: list_hive_metastore_default, idle: 0.0032770633697509766s, acqrelcnt: 1, lang: None, thrd: (72509, 11428507648), cmpt: ``, lut: 1710790566.209363
[0m16:36:06.216672 [debug] [ThreadPool]: Databricks adapter: conn: 10843195984: get_thread_connection: sess: 01eee55e-c30e-1301-bc51-9d018e76ff8f, name: list_hive_metastore_default, idle: 0.00722813606262207s, acqrelcnt: 1, lang: None, thrd: (72509, 11428507648), cmpt: ``, lut: 1710790566.209363
[0m16:36:06.216917 [debug] [ThreadPool]: Databricks adapter: conn: 10843195984: idle check connection: sess: 01eee55e-c30e-1301-bc51-9d018e76ff8f, name: list_hive_metastore_default, idle: 0.00749516487121582s, acqrelcnt: 1, lang: None, thrd: (72509, 11428507648), cmpt: ``, lut: 1710790566.209363
[0m16:36:06.217109 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m16:36:06.217298 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m16:36:06.580015 [debug] [ThreadPool]: SQL status: OK in 0.36000001430511475 seconds
[0m16:36:06.594149 [debug] [ThreadPool]: Databricks adapter: conn: 10843195984: get_thread_connection: sess: 01eee55e-c30e-1301-bc51-9d018e76ff8f, name: list_hive_metastore_default, idle: 0.38463783264160156s, acqrelcnt: 1, lang: None, thrd: (72509, 11428507648), cmpt: ``, lut: 1710790566.209363
[0m16:36:06.594768 [debug] [ThreadPool]: Databricks adapter: conn: 10843195984: idle check connection: sess: 01eee55e-c30e-1301-bc51-9d018e76ff8f, name: list_hive_metastore_default, idle: 0.38521909713745117s, acqrelcnt: 1, lang: None, thrd: (72509, 11428507648), cmpt: ``, lut: 1710790566.209363
[0m16:36:06.595235 [debug] [ThreadPool]: Databricks adapter: conn: 10843195984: get_thread_connection: sess: 01eee55e-c30e-1301-bc51-9d018e76ff8f, name: list_hive_metastore_default, idle: 0.3857839107513428s, acqrelcnt: 1, lang: None, thrd: (72509, 11428507648), cmpt: ``, lut: 1710790566.209363
[0m16:36:06.595508 [debug] [ThreadPool]: Databricks adapter: conn: 10843195984: idle check connection: sess: 01eee55e-c30e-1301-bc51-9d018e76ff8f, name: list_hive_metastore_default, idle: 0.3860771656036377s, acqrelcnt: 1, lang: None, thrd: (72509, 11428507648), cmpt: ``, lut: 1710790566.209363
[0m16:36:06.595732 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m16:36:06.595938 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m16:36:06.596179 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m16:36:06.884904 [debug] [ThreadPool]: SQL status: OK in 0.28999999165534973 seconds
[0m16:36:06.890065 [debug] [ThreadPool]: Databricks adapter: conn: 10843195984: get_thread_connection: sess: 01eee55e-c30e-1301-bc51-9d018e76ff8f, name: list_hive_metastore_default, idle: 0.6806020736694336s, acqrelcnt: 1, lang: None, thrd: (72509, 11428507648), cmpt: ``, lut: 1710790566.209363
[0m16:36:06.890365 [debug] [ThreadPool]: Databricks adapter: conn: 10843195984: idle check connection: sess: 01eee55e-c30e-1301-bc51-9d018e76ff8f, name: list_hive_metastore_default, idle: 0.6809499263763428s, acqrelcnt: 1, lang: None, thrd: (72509, 11428507648), cmpt: ``, lut: 1710790566.209363
[0m16:36:06.890528 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m16:36:06.890691 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m16:36:07.210301 [debug] [ThreadPool]: SQL status: OK in 0.3199999928474426 seconds
[0m16:36:07.215130 [debug] [ThreadPool]: Databricks adapter: conn: 10843195984: _release sess: 01eee55e-c30e-1301-bc51-9d018e76ff8f, name: list_hive_metastore_default, idle: 9.059906005859375e-06s, acqrelcnt: 0, lang: None, thrd: (72509, 11428507648), cmpt: ``, lut: 1710790567.214883
[0m16:36:07.219990 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f8fe6832-1988-4487-b43e-ee42afbcc4d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16ba59710>]}
[0m16:36:07.220613 [debug] [MainThread]: Databricks adapter: conn: 10842354768: get_thread_connection: sess: None, name: master, idle: 1.9457290172576904s, acqrelcnt: 1, lang: None, thrd: (72509, 7965269056), cmpt: ``, lut: 1710790565.274762
[0m16:36:07.220980 [debug] [MainThread]: Databricks adapter: conn: 10842354768: idle check connection: sess: None, name: master, idle: 1.9461171627044678s, acqrelcnt: 1, lang: None, thrd: (72509, 7965269056), cmpt: ``, lut: 1710790565.274762
[0m16:36:07.221292 [debug] [MainThread]: Databricks adapter: conn: 10842354768: get_thread_connection: sess: None, name: master, idle: 1.9464430809020996s, acqrelcnt: 1, lang: None, thrd: (72509, 7965269056), cmpt: ``, lut: 1710790565.274762
[0m16:36:07.221584 [debug] [MainThread]: Databricks adapter: conn: 10842354768: idle check connection: sess: None, name: master, idle: 1.9467380046844482s, acqrelcnt: 1, lang: None, thrd: (72509, 7965269056), cmpt: ``, lut: 1710790565.274762
[0m16:36:07.221866 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:36:07.222132 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:36:07.222378 [debug] [MainThread]: Databricks adapter: conn: 10842354768: _release sess: None, name: master, idle: 1.9073486328125e-06s, acqrelcnt: 0, lang: None, thrd: (72509, 7965269056), cmpt: ``, lut: 1710790567.222301
[0m16:36:07.222992 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:36:07.223273 [info ] [MainThread]: 
[0m16:36:07.228053 [debug] [Thread-1 (]: Began running node model.default.stage_users
[0m16:36:07.228582 [info ] [Thread-1 (]: 1 of 1 START sql view model default.stage_users ................................ [RUN]
[0m16:36:07.229310 [debug] [Thread-1 (]: Databricks adapter: conn: 10843195984: idle check connection: sess: 01eee55e-c30e-1301-bc51-9d018e76ff8f, name: list_hive_metastore_default, idle: 0.01428985595703125s, acqrelcnt: 0, lang: None, thrd: (72509, 11428507648), cmpt: ``, lut: 1710790567.214883
[0m16:36:07.229586 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.default.stage_users)
[0m16:36:07.229888 [debug] [Thread-1 (]: Databricks adapter: conn: 10843195984: reusing connection list_hive_metastore_default sess: 01eee55e-c30e-1301-bc51-9d018e76ff8f, name: model.default.stage_users, idle: 0.014889955520629883s, acqrelcnt: 0, lang: None, thrd: (72509, 11428507648), cmpt: ``, lut: 1710790567.214883
[0m16:36:07.230171 [debug] [Thread-1 (]: Databricks adapter: On thread (72509, 11428507648): `hive_metastore`.`default`.`stage_users` using default compute resource.
[0m16:36:07.230444 [debug] [Thread-1 (]: Databricks adapter: conn: 10843195984: _acquire sess: 01eee55e-c30e-1301-bc51-9d018e76ff8f, name: model.default.stage_users, idle: 0.015451908111572266s, acqrelcnt: 1, lang: sql, thrd: (72509, 11428507648), cmpt: ``, lut: 1710790567.214883
[0m16:36:07.230716 [debug] [Thread-1 (]: Began compiling node model.default.stage_users
[0m16:36:07.236643 [debug] [Thread-1 (]: Writing injected SQL for node "model.default.stage_users"
[0m16:36:07.242074 [debug] [Thread-1 (]: Timing info for model.default.stage_users (compile): 16:36:07.230898 => 16:36:07.241887
[0m16:36:07.242356 [debug] [Thread-1 (]: Began executing node model.default.stage_users
[0m16:36:07.260595 [debug] [Thread-1 (]: Writing runtime sql for node "model.default.stage_users"
[0m16:36:07.262945 [debug] [Thread-1 (]: Databricks adapter: conn: 10843195984: get_thread_connection: sess: 01eee55e-c30e-1301-bc51-9d018e76ff8f, name: model.default.stage_users, idle: 0.04793095588684082s, acqrelcnt: 1, lang: sql, thrd: (72509, 11428507648), cmpt: ``, lut: 1710790567.214883
[0m16:36:07.263209 [debug] [Thread-1 (]: Databricks adapter: conn: 10843195984: idle check connection: sess: 01eee55e-c30e-1301-bc51-9d018e76ff8f, name: model.default.stage_users, idle: 0.048243045806884766s, acqrelcnt: 1, lang: sql, thrd: (72509, 11428507648), cmpt: ``, lut: 1710790567.214883
[0m16:36:07.263392 [debug] [Thread-1 (]: Using databricks connection "model.default.stage_users"
[0m16:36:07.263613 [debug] [Thread-1 (]: On model.default.stage_users: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_users"} */
create or replace view `hive_metastore`.`default`.`stage_users`
  
  
  
  as
    

select user_id AS user_id,
       name AS name,
       city AS city,
       phone_number AS phone_number,
       gender AS gender,
       nationality AS nationality,
       state AS state
from `pythiandbsql`.`raw`.`users`

[0m16:36:07.599132 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_users"} */
create or replace view `hive_metastore`.`default`.`stage_users`
  
  
  
  as
    

select user_id AS user_id,
       name AS name,
       city AS city,
       phone_number AS phone_number,
       gender AS gender,
       nationality AS nationality,
       state AS state
from `pythiandbsql`.`raw`.`users`

[0m16:36:07.599668 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
[0m16:36:07.600670 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UC_NOT_ENABLED] org.apache.spark.sql.AnalysisException: [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:663)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:540)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:389)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:353)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:401)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:152)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$2(Analyzer.scala:1690)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1689)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.$anonfun$applyOrElse$82(Analyzer.scala:1442)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$handleGlueError(Analyzer.scala:1556)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1442)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1402)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:199)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:188)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:188)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:83)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:377)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1277)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1276)
	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:85)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:377)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1306)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1303)
	at org.apache.spark.sql.catalyst.plans.logical.CreateView.mapChildren(v2Commands.scala:1408)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:377)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:203)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:184)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruningByConf(AnalysisHelper.scala:173)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruningByConf$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruningByConf(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1402)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1372)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1289)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:309)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:309)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:289)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$9(RuleExecutor.scala:382)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$9$adapted(RuleExecutor.scala:382)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:382)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:256)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:414)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:407)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:321)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:407)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:248)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:248)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:392)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:384)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:391)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:230)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:394)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:542)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1048)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:542)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:538)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1173)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:538)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:224)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:223)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:481)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:503)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:576)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:531)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:576)
	... 35 more

[0m16:36:07.601670 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01eee55e-c406-153f-8524-4199e73f868a
[0m16:36:07.602091 [debug] [Thread-1 (]: Timing info for model.default.stage_users (execute): 16:36:07.242507 => 16:36:07.601926
[0m16:36:07.602436 [debug] [Thread-1 (]: Databricks adapter: conn: 10843195984: _release sess: 01eee55e-c30e-1301-bc51-9d018e76ff8f, name: model.default.stage_users, idle: 2.1457672119140625e-06s, acqrelcnt: 0, lang: sql, thrd: (72509, 11428507648), cmpt: ``, lut: 1710790567.602304
[0m16:36:07.628317 [debug] [Thread-1 (]: Runtime Error in model stage_users (models/stage/stage_users.sql)
  [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
[0m16:36:07.628815 [debug] [Thread-1 (]: Databricks adapter: conn: 10843195984: _release sess: 01eee55e-c30e-1301-bc51-9d018e76ff8f, name: model.default.stage_users, idle: 3.0994415283203125e-06s, acqrelcnt: 0, lang: sql, thrd: (72509, 11428507648), cmpt: ``, lut: 1710790567.62866
[0m16:36:07.629205 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f8fe6832-1988-4487-b43e-ee42afbcc4d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16bc30e90>]}
[0m16:36:07.629760 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model default.stage_users ....................... [[31mERROR[0m in 0.40s]
[0m16:36:07.630249 [debug] [Thread-1 (]: Finished running node model.default.stage_users
[0m16:36:07.631571 [debug] [MainThread]: Databricks adapter: conn: 10842354768: idle check connection: sess: None, name: master, idle: 0.40916895866394043s, acqrelcnt: 0, lang: None, thrd: (72509, 7965269056), cmpt: ``, lut: 1710790567.222301
[0m16:36:07.631860 [debug] [MainThread]: Databricks adapter: conn: 10842354768: reusing connection master sess: None, name: master, idle: 0.40947794914245605s, acqrelcnt: 0, lang: None, thrd: (72509, 7965269056), cmpt: ``, lut: 1710790567.222301
[0m16:36:07.632074 [debug] [MainThread]: Databricks adapter: Thread (72509, 7965269056) using default compute resource.
[0m16:36:07.632284 [debug] [MainThread]: Databricks adapter: conn: 10842354768: _acquire sess: None, name: master, idle: 0.40991783142089844s, acqrelcnt: 1, lang: None, thrd: (72509, 7965269056), cmpt: ``, lut: 1710790567.222301
[0m16:36:07.632515 [debug] [MainThread]: Databricks adapter: conn: 10842354768: get_thread_connection: sess: None, name: master, idle: 0.41015005111694336s, acqrelcnt: 1, lang: None, thrd: (72509, 7965269056), cmpt: ``, lut: 1710790567.222301
[0m16:36:07.632724 [debug] [MainThread]: Databricks adapter: conn: 10842354768: idle check connection: sess: None, name: master, idle: 0.41036200523376465s, acqrelcnt: 1, lang: None, thrd: (72509, 7965269056), cmpt: ``, lut: 1710790567.222301
[0m16:36:07.632925 [debug] [MainThread]: On master: ROLLBACK
[0m16:36:07.633118 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:36:08.104266 [debug] [MainThread]: Databricks adapter: conn: 10842354768: session opened sess: 01eee55e-c46c-1e5d-a1a8-98bb659382d2, name: master, idle: 1.0013580322265625e-05s, acqrelcnt: 1, lang: None, thrd: (72509, 7965269056), cmpt: ``, lut: 1710790568.103709
[0m16:36:08.106426 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:36:08.107610 [debug] [MainThread]: Databricks adapter: conn: 10842354768: get_thread_connection: sess: 01eee55e-c46c-1e5d-a1a8-98bb659382d2, name: master, idle: 0.0037560462951660156s, acqrelcnt: 1, lang: None, thrd: (72509, 7965269056), cmpt: ``, lut: 1710790568.103709
[0m16:36:08.108263 [debug] [MainThread]: Databricks adapter: conn: 10842354768: idle check connection: sess: 01eee55e-c46c-1e5d-a1a8-98bb659382d2, name: master, idle: 0.004427909851074219s, acqrelcnt: 1, lang: None, thrd: (72509, 7965269056), cmpt: ``, lut: 1710790568.103709
[0m16:36:08.108796 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:36:08.109287 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:36:08.109802 [debug] [MainThread]: Databricks adapter: conn: 10842354768: _release sess: 01eee55e-c46c-1e5d-a1a8-98bb659382d2, name: master, idle: 3.814697265625e-06s, acqrelcnt: 0, lang: None, thrd: (72509, 7965269056), cmpt: ``, lut: 1710790568.109689
[0m16:36:08.111121 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:36:08.111661 [debug] [MainThread]: On master: ROLLBACK
[0m16:36:08.112049 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:36:08.112423 [debug] [MainThread]: On master: Close
[0m16:36:08.304285 [debug] [MainThread]: Connection 'model.default.stage_users' was properly closed.
[0m16:36:08.305666 [debug] [MainThread]: On model.default.stage_users: ROLLBACK
[0m16:36:08.306962 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:36:08.307717 [debug] [MainThread]: On model.default.stage_users: Close
[0m16:36:08.535042 [info ] [MainThread]: 
[0m16:36:08.535747 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.26 seconds (3.26s).
[0m16:36:08.536461 [debug] [MainThread]: Command end result
[0m16:36:08.562553 [info ] [MainThread]: 
[0m16:36:08.563135 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m16:36:08.563358 [info ] [MainThread]: 
[0m16:36:08.563694 [error] [MainThread]:   Runtime Error in model stage_users (models/stage/stage_users.sql)
  [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
[0m16:36:08.563976 [info ] [MainThread]: 
[0m16:36:08.564318 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m16:36:08.568636 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 5.3384643, "process_user_time": 2.254398, "process_kernel_time": 3.237242, "process_mem_max_rss": "227131392", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:36:08.569038 [debug] [MainThread]: Command `cli run` failed at 16:36:08.568945 after 5.34 seconds
[0m16:36:08.569345 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a7e7c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b9a06d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104b43cd0>]}
[0m16:36:08.569616 [debug] [MainThread]: Flushing usage events
[0m16:37:27.146970 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108692b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1086c1d10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1086c2310>]}


============================== 16:37:27.149723 | 3aee1c78-50ad-442c-b843-8ca868228933 ==============================
[0m16:37:27.149723 [info ] [MainThread]: Running with dbt=1.7.8
[0m16:37:27.150011 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'invocation_command': 'dbt ', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:37:28.607864 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3aee1c78-50ad-442c-b843-8ca868228933', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x168804510>]}
[0m16:37:28.637002 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3aee1c78-50ad-442c-b843-8ca868228933', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1687e5b10>]}
[0m16:37:28.637323 [info ] [MainThread]: Registered adapter: databricks=1.7.9
[0m16:37:28.658608 [debug] [MainThread]: checksum: 67f0013ca5f0bd43af9a0873dd50792fde83ef69de63b71cacd0b4ac656c52e5, vars: {}, profile: , target: , version: 1.7.8
[0m16:37:28.667677 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m16:37:28.667945 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '3aee1c78-50ad-442c-b843-8ca868228933', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16882c3d0>]}
[0m16:37:29.232081 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3aee1c78-50ad-442c-b843-8ca868228933', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x168a98290>]}
[0m16:37:29.243018 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3aee1c78-50ad-442c-b843-8ca868228933', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x168e8b390>]}
[0m16:37:29.243232 [info ] [MainThread]: Found 3 models, 3 sources, 0 exposures, 0 metrics, 538 macros, 0 groups, 0 semantic models
[0m16:37:29.243407 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3aee1c78-50ad-442c-b843-8ca868228933', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x168a46410>]}
[0m16:37:29.244041 [info ] [MainThread]: 
[0m16:37:29.244452 [debug] [MainThread]: Databricks adapter: conn: 6050216208: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (73062, 7965269056), cmpt: ``, lut: None
[0m16:37:29.244593 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:37:29.244710 [debug] [MainThread]: Databricks adapter: Thread (73062, 7965269056) using default compute resource.
[0m16:37:29.244831 [debug] [MainThread]: Databricks adapter: conn: 6050216208: _acquire sess: None, name: master, idle: 1.1920928955078125e-06s, acqrelcnt: 1, lang: None, thrd: (73062, 7965269056), cmpt: ``, lut: 1710790649.244792
[0m16:37:29.245286 [debug] [ThreadPool]: Databricks adapter: conn: 6055043984: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore, idle: 0s, acqrelcnt: 0, lang: None, thrd: (73062, 10916343808), cmpt: ``, lut: None
[0m16:37:29.245447 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m16:37:29.245556 [debug] [ThreadPool]: Databricks adapter: Thread (73062, 10916343808) using default compute resource.
[0m16:37:29.245667 [debug] [ThreadPool]: Databricks adapter: conn: 6055043984: _acquire sess: None, name: list_hive_metastore, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (73062, 10916343808), cmpt: ``, lut: 1710790649.245631
[0m16:37:29.245795 [debug] [ThreadPool]: Databricks adapter: conn: 6055043984: get_thread_connection: sess: None, name: list_hive_metastore, idle: 0.0001289844512939453s, acqrelcnt: 1, lang: None, thrd: (73062, 10916343808), cmpt: ``, lut: 1710790649.245631
[0m16:37:29.245923 [debug] [ThreadPool]: Databricks adapter: conn: 6055043984: idle check connection: sess: None, name: list_hive_metastore, idle: 0.0002510547637939453s, acqrelcnt: 1, lang: None, thrd: (73062, 10916343808), cmpt: ``, lut: 1710790649.245631
[0m16:37:29.246039 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m16:37:29.246160 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m16:37:29.246271 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:37:29.766059 [debug] [ThreadPool]: Databricks adapter: conn: 6055043984: session opened sess: 01eee55e-f518-1807-bcc9-38547aa52b02, name: list_hive_metastore, idle: 1.6927719116210938e-05s, acqrelcnt: 1, lang: None, thrd: (73062, 10916343808), cmpt: ``, lut: 1710790649.765551
[0m16:37:30.068924 [debug] [ThreadPool]: SQL status: OK in 0.8199999928474426 seconds
[0m16:37:30.081247 [debug] [ThreadPool]: Databricks adapter: conn: 6055043984: _release sess: 01eee55e-f518-1807-bcc9-38547aa52b02, name: list_hive_metastore, idle: 6.198883056640625e-06s, acqrelcnt: 0, lang: None, thrd: (73062, 10916343808), cmpt: ``, lut: 1710790650.0811038
[0m16:37:30.082645 [debug] [ThreadPool]: Databricks adapter: conn: 6055043984: idle check connection: sess: 01eee55e-f518-1807-bcc9-38547aa52b02, name: list_hive_metastore, idle: 0.0014650821685791016s, acqrelcnt: 0, lang: None, thrd: (73062, 10916343808), cmpt: ``, lut: 1710790650.0811038
[0m16:37:30.082950 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now list_hive_metastore_default)
[0m16:37:30.083214 [debug] [ThreadPool]: Databricks adapter: conn: 6055043984: reusing connection list_hive_metastore sess: 01eee55e-f518-1807-bcc9-38547aa52b02, name: list_hive_metastore_default, idle: 0.002039194107055664s, acqrelcnt: 0, lang: None, thrd: (73062, 10916343808), cmpt: ``, lut: 1710790650.0811038
[0m16:37:30.083432 [debug] [ThreadPool]: Databricks adapter: Thread (73062, 10916343808) using default compute resource.
[0m16:37:30.083629 [debug] [ThreadPool]: Databricks adapter: conn: 6055043984: _acquire sess: 01eee55e-f518-1807-bcc9-38547aa52b02, name: list_hive_metastore_default, idle: 0.00246429443359375s, acqrelcnt: 1, lang: None, thrd: (73062, 10916343808), cmpt: ``, lut: 1710790650.0811038
[0m16:37:30.087197 [debug] [ThreadPool]: Databricks adapter: conn: 6055043984: get_thread_connection: sess: 01eee55e-f518-1807-bcc9-38547aa52b02, name: list_hive_metastore_default, idle: 0.0060160160064697266s, acqrelcnt: 1, lang: None, thrd: (73062, 10916343808), cmpt: ``, lut: 1710790650.0811038
[0m16:37:30.087436 [debug] [ThreadPool]: Databricks adapter: conn: 6055043984: idle check connection: sess: 01eee55e-f518-1807-bcc9-38547aa52b02, name: list_hive_metastore_default, idle: 0.0062673091888427734s, acqrelcnt: 1, lang: None, thrd: (73062, 10916343808), cmpt: ``, lut: 1710790650.0811038
[0m16:37:30.087651 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m16:37:30.087835 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m16:37:30.380211 [debug] [ThreadPool]: SQL status: OK in 0.28999999165534973 seconds
[0m16:37:30.389664 [debug] [ThreadPool]: Databricks adapter: conn: 6055043984: get_thread_connection: sess: 01eee55e-f518-1807-bcc9-38547aa52b02, name: list_hive_metastore_default, idle: 0.3084452152252197s, acqrelcnt: 1, lang: None, thrd: (73062, 10916343808), cmpt: ``, lut: 1710790650.0811038
[0m16:37:30.389981 [debug] [ThreadPool]: Databricks adapter: conn: 6055043984: idle check connection: sess: 01eee55e-f518-1807-bcc9-38547aa52b02, name: list_hive_metastore_default, idle: 0.3088102340698242s, acqrelcnt: 1, lang: None, thrd: (73062, 10916343808), cmpt: ``, lut: 1710790650.0811038
[0m16:37:30.390197 [debug] [ThreadPool]: Databricks adapter: conn: 6055043984: get_thread_connection: sess: 01eee55e-f518-1807-bcc9-38547aa52b02, name: list_hive_metastore_default, idle: 0.309035062789917s, acqrelcnt: 1, lang: None, thrd: (73062, 10916343808), cmpt: ``, lut: 1710790650.0811038
[0m16:37:30.390391 [debug] [ThreadPool]: Databricks adapter: conn: 6055043984: idle check connection: sess: 01eee55e-f518-1807-bcc9-38547aa52b02, name: list_hive_metastore_default, idle: 0.3092312812805176s, acqrelcnt: 1, lang: None, thrd: (73062, 10916343808), cmpt: ``, lut: 1710790650.0811038
[0m16:37:30.390578 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m16:37:30.390744 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m16:37:30.390928 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m16:37:30.663157 [debug] [ThreadPool]: SQL status: OK in 0.27000001072883606 seconds
[0m16:37:30.670465 [debug] [ThreadPool]: Databricks adapter: conn: 6055043984: get_thread_connection: sess: 01eee55e-f518-1807-bcc9-38547aa52b02, name: list_hive_metastore_default, idle: 0.5892291069030762s, acqrelcnt: 1, lang: None, thrd: (73062, 10916343808), cmpt: ``, lut: 1710790650.0811038
[0m16:37:30.670869 [debug] [ThreadPool]: Databricks adapter: conn: 6055043984: idle check connection: sess: 01eee55e-f518-1807-bcc9-38547aa52b02, name: list_hive_metastore_default, idle: 0.589684247970581s, acqrelcnt: 1, lang: None, thrd: (73062, 10916343808), cmpt: ``, lut: 1710790650.0811038
[0m16:37:30.671118 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m16:37:30.671375 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m16:37:30.972189 [debug] [ThreadPool]: SQL status: OK in 0.30000001192092896 seconds
[0m16:37:30.975323 [debug] [ThreadPool]: Databricks adapter: conn: 6055043984: _release sess: 01eee55e-f518-1807-bcc9-38547aa52b02, name: list_hive_metastore_default, idle: 1.9073486328125e-06s, acqrelcnt: 0, lang: None, thrd: (73062, 10916343808), cmpt: ``, lut: 1710790650.975188
[0m16:37:30.977936 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3aee1c78-50ad-442c-b843-8ca868228933', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x168998550>]}
[0m16:37:30.978315 [debug] [MainThread]: Databricks adapter: conn: 6050216208: get_thread_connection: sess: None, name: master, idle: 1.7334480285644531s, acqrelcnt: 1, lang: None, thrd: (73062, 7965269056), cmpt: ``, lut: 1710790649.244792
[0m16:37:30.978547 [debug] [MainThread]: Databricks adapter: conn: 6050216208: idle check connection: sess: None, name: master, idle: 1.7336888313293457s, acqrelcnt: 1, lang: None, thrd: (73062, 7965269056), cmpt: ``, lut: 1710790649.244792
[0m16:37:30.978780 [debug] [MainThread]: Databricks adapter: conn: 6050216208: get_thread_connection: sess: None, name: master, idle: 1.7339248657226562s, acqrelcnt: 1, lang: None, thrd: (73062, 7965269056), cmpt: ``, lut: 1710790649.244792
[0m16:37:30.978993 [debug] [MainThread]: Databricks adapter: conn: 6050216208: idle check connection: sess: None, name: master, idle: 1.7341389656066895s, acqrelcnt: 1, lang: None, thrd: (73062, 7965269056), cmpt: ``, lut: 1710790649.244792
[0m16:37:30.979198 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:37:30.979386 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:37:30.979600 [debug] [MainThread]: Databricks adapter: conn: 6050216208: _release sess: None, name: master, idle: 1.1920928955078125e-06s, acqrelcnt: 0, lang: None, thrd: (73062, 7965269056), cmpt: ``, lut: 1710790650.9795358
[0m16:37:30.980055 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:37:30.980291 [info ] [MainThread]: 
[0m16:37:30.983717 [debug] [Thread-1 (]: Began running node model.default.stage_users
[0m16:37:30.984059 [info ] [Thread-1 (]: 1 of 1 START sql view model default.stage_users ................................ [RUN]
[0m16:37:30.984619 [debug] [Thread-1 (]: Databricks adapter: conn: 6055043984: idle check connection: sess: 01eee55e-f518-1807-bcc9-38547aa52b02, name: list_hive_metastore_default, idle: 0.009324073791503906s, acqrelcnt: 0, lang: None, thrd: (73062, 10916343808), cmpt: ``, lut: 1710790650.975188
[0m16:37:30.984894 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.default.stage_users)
[0m16:37:30.985267 [debug] [Thread-1 (]: Databricks adapter: conn: 6055043984: reusing connection list_hive_metastore_default sess: 01eee55e-f518-1807-bcc9-38547aa52b02, name: model.default.stage_users, idle: 0.009950876235961914s, acqrelcnt: 0, lang: None, thrd: (73062, 10916343808), cmpt: ``, lut: 1710790650.975188
[0m16:37:30.985538 [debug] [Thread-1 (]: Databricks adapter: On thread (73062, 10916343808): `hive_metastore`.`default`.`stage_users` using default compute resource.
[0m16:37:30.985791 [debug] [Thread-1 (]: Databricks adapter: conn: 6055043984: _acquire sess: 01eee55e-f518-1807-bcc9-38547aa52b02, name: model.default.stage_users, idle: 0.010504961013793945s, acqrelcnt: 1, lang: sql, thrd: (73062, 10916343808), cmpt: ``, lut: 1710790650.975188
[0m16:37:30.986041 [debug] [Thread-1 (]: Began compiling node model.default.stage_users
[0m16:37:30.991221 [debug] [Thread-1 (]: Writing injected SQL for node "model.default.stage_users"
[0m16:37:30.992935 [debug] [Thread-1 (]: Timing info for model.default.stage_users (compile): 16:37:30.986197 => 16:37:30.992788
[0m16:37:30.993160 [debug] [Thread-1 (]: Began executing node model.default.stage_users
[0m16:37:31.008582 [debug] [Thread-1 (]: Writing runtime sql for node "model.default.stage_users"
[0m16:37:31.010540 [debug] [Thread-1 (]: Databricks adapter: conn: 6055043984: get_thread_connection: sess: 01eee55e-f518-1807-bcc9-38547aa52b02, name: model.default.stage_users, idle: 0.03525495529174805s, acqrelcnt: 1, lang: sql, thrd: (73062, 10916343808), cmpt: ``, lut: 1710790650.975188
[0m16:37:31.010754 [debug] [Thread-1 (]: Databricks adapter: conn: 6055043984: idle check connection: sess: 01eee55e-f518-1807-bcc9-38547aa52b02, name: model.default.stage_users, idle: 0.03549003601074219s, acqrelcnt: 1, lang: sql, thrd: (73062, 10916343808), cmpt: ``, lut: 1710790650.975188
[0m16:37:31.010907 [debug] [Thread-1 (]: Using databricks connection "model.default.stage_users"
[0m16:37:31.011108 [debug] [Thread-1 (]: On model.default.stage_users: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_users"} */
create or replace view `hive_metastore`.`default`.`stage_users`
  
  
  
  as
    

select user_id AS user_id,
       name AS name,
       city AS city,
       phone_number AS phone_number,
       gender AS gender,
       nationality AS nationality,
       state AS state
from `pythiandbsql`.`raw`.`users`

[0m16:37:31.354673 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_users"} */
create or replace view `hive_metastore`.`default`.`stage_users`
  
  
  
  as
    

select user_id AS user_id,
       name AS name,
       city AS city,
       phone_number AS phone_number,
       gender AS gender,
       nationality AS nationality,
       state AS state
from `pythiandbsql`.`raw`.`users`

[0m16:37:31.355718 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
[0m16:37:31.357156 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UC_NOT_ENABLED] org.apache.spark.sql.AnalysisException: [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:663)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:540)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:389)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:353)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:401)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:152)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$2(Analyzer.scala:1690)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1689)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.$anonfun$applyOrElse$82(Analyzer.scala:1442)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$handleGlueError(Analyzer.scala:1556)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1442)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1402)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:199)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:188)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:188)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:83)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:377)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1277)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1276)
	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:85)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:377)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1306)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1303)
	at org.apache.spark.sql.catalyst.plans.logical.CreateView.mapChildren(v2Commands.scala:1408)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:377)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:203)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:184)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruningByConf(AnalysisHelper.scala:173)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruningByConf$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruningByConf(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1402)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1372)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1289)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:309)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:309)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:289)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$9(RuleExecutor.scala:382)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$9$adapted(RuleExecutor.scala:382)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:382)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:256)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:414)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:407)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:321)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:407)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:248)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:248)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:392)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:384)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:391)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:230)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:394)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:542)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1048)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:542)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:538)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1173)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:538)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:224)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:223)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:481)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:503)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:576)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:531)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:576)
	... 35 more

[0m16:37:31.358461 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01eee55e-f5f2-1709-b7bc-1d9a1b0578e4
[0m16:37:31.359208 [debug] [Thread-1 (]: Timing info for model.default.stage_users (execute): 16:37:30.993287 => 16:37:31.358946
[0m16:37:31.359792 [debug] [Thread-1 (]: Databricks adapter: conn: 6055043984: _release sess: 01eee55e-f518-1807-bcc9-38547aa52b02, name: model.default.stage_users, idle: 7.867813110351562e-06s, acqrelcnt: 0, lang: sql, thrd: (73062, 10916343808), cmpt: ``, lut: 1710790651.35963
[0m16:37:31.392734 [debug] [Thread-1 (]: Runtime Error in model stage_users (models/stage/stage_users.sql)
  [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
[0m16:37:31.393378 [debug] [Thread-1 (]: Databricks adapter: conn: 6055043984: _release sess: 01eee55e-f518-1807-bcc9-38547aa52b02, name: model.default.stage_users, idle: 4.76837158203125e-06s, acqrelcnt: 0, lang: sql, thrd: (73062, 10916343808), cmpt: ``, lut: 1710790651.3931992
[0m16:37:31.393850 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3aee1c78-50ad-442c-b843-8ca868228933', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x168ecf850>]}
[0m16:37:31.394448 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model default.stage_users ....................... [[31mERROR[0m in 0.41s]
[0m16:37:31.395018 [debug] [Thread-1 (]: Finished running node model.default.stage_users
[0m16:37:31.396554 [debug] [MainThread]: Databricks adapter: conn: 6050216208: idle check connection: sess: None, name: master, idle: 0.4169011116027832s, acqrelcnt: 0, lang: None, thrd: (73062, 7965269056), cmpt: ``, lut: 1710790650.9795358
[0m16:37:31.396928 [debug] [MainThread]: Databricks adapter: conn: 6050216208: reusing connection master sess: None, name: master, idle: 0.4173102378845215s, acqrelcnt: 0, lang: None, thrd: (73062, 7965269056), cmpt: ``, lut: 1710790650.9795358
[0m16:37:31.397195 [debug] [MainThread]: Databricks adapter: Thread (73062, 7965269056) using default compute resource.
[0m16:37:31.397435 [debug] [MainThread]: Databricks adapter: conn: 6050216208: _acquire sess: None, name: master, idle: 0.4178292751312256s, acqrelcnt: 1, lang: None, thrd: (73062, 7965269056), cmpt: ``, lut: 1710790650.9795358
[0m16:37:31.397699 [debug] [MainThread]: Databricks adapter: conn: 6050216208: get_thread_connection: sess: None, name: master, idle: 0.4180910587310791s, acqrelcnt: 1, lang: None, thrd: (73062, 7965269056), cmpt: ``, lut: 1710790650.9795358
[0m16:37:31.397963 [debug] [MainThread]: Databricks adapter: conn: 6050216208: idle check connection: sess: None, name: master, idle: 0.4183363914489746s, acqrelcnt: 1, lang: None, thrd: (73062, 7965269056), cmpt: ``, lut: 1710790650.9795358
[0m16:37:31.398193 [debug] [MainThread]: On master: ROLLBACK
[0m16:37:31.398421 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:37:31.861981 [debug] [MainThread]: Databricks adapter: conn: 6050216208: session opened sess: 01eee55e-f659-11c1-956a-a3ab6e20a457, name: master, idle: 1.0967254638671875e-05s, acqrelcnt: 1, lang: None, thrd: (73062, 7965269056), cmpt: ``, lut: 1710790651.861493
[0m16:37:31.863011 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:37:31.863766 [debug] [MainThread]: Databricks adapter: conn: 6050216208: get_thread_connection: sess: 01eee55e-f659-11c1-956a-a3ab6e20a457, name: master, idle: 0.0020978450775146484s, acqrelcnt: 1, lang: None, thrd: (73062, 7965269056), cmpt: ``, lut: 1710790651.861493
[0m16:37:31.864992 [debug] [MainThread]: Databricks adapter: conn: 6050216208: idle check connection: sess: 01eee55e-f659-11c1-956a-a3ab6e20a457, name: master, idle: 0.002984762191772461s, acqrelcnt: 1, lang: None, thrd: (73062, 7965269056), cmpt: ``, lut: 1710790651.861493
[0m16:37:31.866461 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:37:31.867240 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:37:31.867851 [debug] [MainThread]: Databricks adapter: conn: 6050216208: _release sess: 01eee55e-f659-11c1-956a-a3ab6e20a457, name: master, idle: 4.0531158447265625e-06s, acqrelcnt: 0, lang: None, thrd: (73062, 7965269056), cmpt: ``, lut: 1710790651.867729
[0m16:37:31.870953 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:37:31.871584 [debug] [MainThread]: On master: ROLLBACK
[0m16:37:31.872019 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:37:31.872612 [debug] [MainThread]: On master: Close
[0m16:37:32.041048 [debug] [MainThread]: Connection 'model.default.stage_users' was properly closed.
[0m16:37:32.042324 [debug] [MainThread]: On model.default.stage_users: ROLLBACK
[0m16:37:32.043079 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:37:32.043841 [debug] [MainThread]: On model.default.stage_users: Close
[0m16:37:32.213857 [info ] [MainThread]: 
[0m16:37:32.215342 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 2.97 seconds (2.97s).
[0m16:37:32.217361 [debug] [MainThread]: Command end result
[0m16:37:32.251464 [info ] [MainThread]: 
[0m16:37:32.252032 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m16:37:32.252308 [info ] [MainThread]: 
[0m16:37:32.252676 [error] [MainThread]:   Runtime Error in model stage_users (models/stage/stage_users.sql)
  [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
[0m16:37:32.252960 [info ] [MainThread]: 
[0m16:37:32.253263 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m16:37:32.263518 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 5.143922, "process_user_time": 2.605716, "process_kernel_time": 3.416114, "process_mem_max_rss": "229474304", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:37:32.264002 [debug] [MainThread]: Command `cli run` failed at 16:37:32.263900 after 5.14 seconds
[0m16:37:32.264371 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1086c2310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1086c2590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100f4bc10>]}
[0m16:37:32.264668 [debug] [MainThread]: Flushing usage events
[0m16:39:29.683973 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109b45150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109b461d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109bc2710>]}


============================== 16:39:29.687047 | 7ec8266a-25f0-46d9-a2a8-958646caf109 ==============================
[0m16:39:29.687047 [info ] [MainThread]: Running with dbt=1.7.8
[0m16:39:29.687356 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt ', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:39:31.253115 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7ec8266a-25f0-46d9-a2a8-958646caf109', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16abb4c50>]}
[0m16:39:31.281691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7ec8266a-25f0-46d9-a2a8-958646caf109', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16abb4f90>]}
[0m16:39:31.281964 [info ] [MainThread]: Registered adapter: databricks=1.7.9
[0m16:39:31.297753 [debug] [MainThread]: checksum: 67f0013ca5f0bd43af9a0873dd50792fde83ef69de63b71cacd0b4ac656c52e5, vars: {}, profile: , target: , version: 1.7.8
[0m16:39:31.382414 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:39:31.382868 [debug] [MainThread]: Partial parsing: updated file: default://models/sources.yml
[0m16:39:31.453876 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.default.stage_users' (models/stage/stage_users.sql) depends on a source named 'raw.users' which was not found
[0m16:39:31.461675 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 1.8062736, "process_user_time": 2.045824, "process_kernel_time": 3.366109, "process_mem_max_rss": "211222528", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:39:31.461929 [debug] [MainThread]: Command `cli run` failed at 16:39:31.461868 after 1.81 seconds
[0m16:39:31.462127 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109bc1f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109bc2010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102aebcd0>]}
[0m16:39:31.462304 [debug] [MainThread]: Flushing usage events
[0m16:39:42.956689 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ad41ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ad93b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10adc2210>]}


============================== 16:39:42.958973 | 117604e1-6a2a-4fed-ae85-fe8f1d1b7c31 ==============================
[0m16:39:42.958973 [info ] [MainThread]: Running with dbt=1.7.8
[0m16:39:42.959259 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m16:39:44.087933 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '117604e1-6a2a-4fed-ae85-fe8f1d1b7c31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ade01d0>]}
[0m16:39:44.116130 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '117604e1-6a2a-4fed-ae85-fe8f1d1b7c31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16dfc6010>]}
[0m16:39:44.116387 [info ] [MainThread]: Registered adapter: databricks=1.7.9
[0m16:39:44.133485 [debug] [MainThread]: checksum: 67f0013ca5f0bd43af9a0873dd50792fde83ef69de63b71cacd0b4ac656c52e5, vars: {}, profile: , target: , version: 1.7.8
[0m16:39:44.230401 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m16:39:44.230914 [debug] [MainThread]: Partial parsing: updated file: default://models/sources.yml
[0m16:39:44.231182 [debug] [MainThread]: Partial parsing: updated file: default://models/stage/stage_users.sql
[0m16:39:44.309730 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '117604e1-6a2a-4fed-ae85-fe8f1d1b7c31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16bfa41d0>]}
[0m16:39:44.316356 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '117604e1-6a2a-4fed-ae85-fe8f1d1b7c31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16dfda910>]}
[0m16:39:44.316587 [info ] [MainThread]: Found 3 models, 3 sources, 0 exposures, 0 metrics, 538 macros, 0 groups, 0 semantic models
[0m16:39:44.316763 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '117604e1-6a2a-4fed-ae85-fe8f1d1b7c31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16bf60450>]}
[0m16:39:44.317434 [info ] [MainThread]: 
[0m16:39:44.317887 [debug] [MainThread]: Databricks adapter: conn: 6108090768: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (73785, 7965269056), cmpt: ``, lut: None
[0m16:39:44.318045 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:39:44.318177 [debug] [MainThread]: Databricks adapter: Thread (73785, 7965269056) using default compute resource.
[0m16:39:44.318331 [debug] [MainThread]: Databricks adapter: conn: 6108090768: _acquire sess: None, name: master, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (73785, 7965269056), cmpt: ``, lut: 1710790784.318284
[0m16:39:44.318877 [debug] [ThreadPool]: Databricks adapter: conn: 6140353040: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore, idle: 0s, acqrelcnt: 0, lang: None, thrd: (73785, 10932187136), cmpt: ``, lut: None
[0m16:39:44.319056 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m16:39:44.319175 [debug] [ThreadPool]: Databricks adapter: Thread (73785, 10932187136) using default compute resource.
[0m16:39:44.319293 [debug] [ThreadPool]: Databricks adapter: conn: 6140353040: _acquire sess: None, name: list_hive_metastore, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (73785, 10932187136), cmpt: ``, lut: 1710790784.319257
[0m16:39:44.319437 [debug] [ThreadPool]: Databricks adapter: conn: 6140353040: get_thread_connection: sess: None, name: list_hive_metastore, idle: 0.00014281272888183594s, acqrelcnt: 1, lang: None, thrd: (73785, 10932187136), cmpt: ``, lut: 1710790784.319257
[0m16:39:44.319570 [debug] [ThreadPool]: Databricks adapter: conn: 6140353040: idle check connection: sess: None, name: list_hive_metastore, idle: 0.0002739429473876953s, acqrelcnt: 1, lang: None, thrd: (73785, 10932187136), cmpt: ``, lut: 1710790784.319257
[0m16:39:44.319695 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m16:39:44.319818 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m16:39:44.319944 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:39:44.828328 [debug] [ThreadPool]: Databricks adapter: conn: 6140353040: session opened sess: 01eee55f-459b-177c-9dd3-62e96fa3bc31, name: list_hive_metastore, idle: 3.0994415283203125e-06s, acqrelcnt: 1, lang: None, thrd: (73785, 10932187136), cmpt: ``, lut: 1710790784.828199
[0m16:39:45.243300 [debug] [ThreadPool]: SQL status: OK in 0.9200000166893005 seconds
[0m16:39:45.254165 [debug] [ThreadPool]: Databricks adapter: conn: 6140353040: _release sess: 01eee55f-459b-177c-9dd3-62e96fa3bc31, name: list_hive_metastore, idle: 5.245208740234375e-06s, acqrelcnt: 0, lang: None, thrd: (73785, 10932187136), cmpt: ``, lut: 1710790785.254026
[0m16:39:45.255615 [debug] [ThreadPool]: Databricks adapter: conn: 6140353040: idle check connection: sess: 01eee55f-459b-177c-9dd3-62e96fa3bc31, name: list_hive_metastore, idle: 0.0014882087707519531s, acqrelcnt: 0, lang: None, thrd: (73785, 10932187136), cmpt: ``, lut: 1710790785.254026
[0m16:39:45.255912 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now list_hive_metastore_default)
[0m16:39:45.256145 [debug] [ThreadPool]: Databricks adapter: conn: 6140353040: reusing connection list_hive_metastore sess: 01eee55f-459b-177c-9dd3-62e96fa3bc31, name: list_hive_metastore_default, idle: 0.002051115036010742s, acqrelcnt: 0, lang: None, thrd: (73785, 10932187136), cmpt: ``, lut: 1710790785.254026
[0m16:39:45.256351 [debug] [ThreadPool]: Databricks adapter: Thread (73785, 10932187136) using default compute resource.
[0m16:39:45.256556 [debug] [ThreadPool]: Databricks adapter: conn: 6140353040: _acquire sess: 01eee55f-459b-177c-9dd3-62e96fa3bc31, name: list_hive_metastore_default, idle: 0.0024662017822265625s, acqrelcnt: 1, lang: None, thrd: (73785, 10932187136), cmpt: ``, lut: 1710790785.254026
[0m16:39:45.260202 [debug] [ThreadPool]: Databricks adapter: conn: 6140353040: get_thread_connection: sess: 01eee55f-459b-177c-9dd3-62e96fa3bc31, name: list_hive_metastore_default, idle: 0.006101131439208984s, acqrelcnt: 1, lang: None, thrd: (73785, 10932187136), cmpt: ``, lut: 1710790785.254026
[0m16:39:45.260425 [debug] [ThreadPool]: Databricks adapter: conn: 6140353040: idle check connection: sess: 01eee55f-459b-177c-9dd3-62e96fa3bc31, name: list_hive_metastore_default, idle: 0.006339073181152344s, acqrelcnt: 1, lang: None, thrd: (73785, 10932187136), cmpt: ``, lut: 1710790785.254026
[0m16:39:45.260611 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m16:39:45.260790 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m16:39:45.625252 [debug] [ThreadPool]: SQL status: OK in 0.36000001430511475 seconds
[0m16:39:45.638525 [debug] [ThreadPool]: Databricks adapter: conn: 6140353040: get_thread_connection: sess: 01eee55f-459b-177c-9dd3-62e96fa3bc31, name: list_hive_metastore_default, idle: 0.384357213973999s, acqrelcnt: 1, lang: None, thrd: (73785, 10932187136), cmpt: ``, lut: 1710790785.254026
[0m16:39:45.638934 [debug] [ThreadPool]: Databricks adapter: conn: 6140353040: idle check connection: sess: 01eee55f-459b-177c-9dd3-62e96fa3bc31, name: list_hive_metastore_default, idle: 0.38483309745788574s, acqrelcnt: 1, lang: None, thrd: (73785, 10932187136), cmpt: ``, lut: 1710790785.254026
[0m16:39:45.639180 [debug] [ThreadPool]: Databricks adapter: conn: 6140353040: get_thread_connection: sess: 01eee55f-459b-177c-9dd3-62e96fa3bc31, name: list_hive_metastore_default, idle: 0.3850891590118408s, acqrelcnt: 1, lang: None, thrd: (73785, 10932187136), cmpt: ``, lut: 1710790785.254026
[0m16:39:45.639402 [debug] [ThreadPool]: Databricks adapter: conn: 6140353040: idle check connection: sess: 01eee55f-459b-177c-9dd3-62e96fa3bc31, name: list_hive_metastore_default, idle: 0.3853139877319336s, acqrelcnt: 1, lang: None, thrd: (73785, 10932187136), cmpt: ``, lut: 1710790785.254026
[0m16:39:45.639615 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m16:39:45.639804 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m16:39:45.640019 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m16:39:45.948407 [debug] [ThreadPool]: SQL status: OK in 0.3100000023841858 seconds
[0m16:39:45.956468 [debug] [ThreadPool]: Databricks adapter: conn: 6140353040: get_thread_connection: sess: 01eee55f-459b-177c-9dd3-62e96fa3bc31, name: list_hive_metastore_default, idle: 0.7023050785064697s, acqrelcnt: 1, lang: None, thrd: (73785, 10932187136), cmpt: ``, lut: 1710790785.254026
[0m16:39:45.956847 [debug] [ThreadPool]: Databricks adapter: conn: 6140353040: idle check connection: sess: 01eee55f-459b-177c-9dd3-62e96fa3bc31, name: list_hive_metastore_default, idle: 0.7027380466461182s, acqrelcnt: 1, lang: None, thrd: (73785, 10932187136), cmpt: ``, lut: 1710790785.254026
[0m16:39:45.957092 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m16:39:45.957354 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m16:39:46.307465 [debug] [ThreadPool]: SQL status: OK in 0.3499999940395355 seconds
[0m16:39:46.310599 [debug] [ThreadPool]: Databricks adapter: conn: 6140353040: _release sess: 01eee55f-459b-177c-9dd3-62e96fa3bc31, name: list_hive_metastore_default, idle: 1.9073486328125e-06s, acqrelcnt: 0, lang: None, thrd: (73785, 10932187136), cmpt: ``, lut: 1710790786.310444
[0m16:39:46.313807 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '117604e1-6a2a-4fed-ae85-fe8f1d1b7c31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16c13ff10>]}
[0m16:39:46.314235 [debug] [MainThread]: Databricks adapter: conn: 6108090768: get_thread_connection: sess: None, name: master, idle: 1.995858907699585s, acqrelcnt: 1, lang: None, thrd: (73785, 7965269056), cmpt: ``, lut: 1710790784.318284
[0m16:39:46.314530 [debug] [MainThread]: Databricks adapter: conn: 6108090768: idle check connection: sess: None, name: master, idle: 1.9961509704589844s, acqrelcnt: 1, lang: None, thrd: (73785, 7965269056), cmpt: ``, lut: 1710790784.318284
[0m16:39:46.314805 [debug] [MainThread]: Databricks adapter: conn: 6108090768: get_thread_connection: sess: None, name: master, idle: 1.996445894241333s, acqrelcnt: 1, lang: None, thrd: (73785, 7965269056), cmpt: ``, lut: 1710790784.318284
[0m16:39:46.315065 [debug] [MainThread]: Databricks adapter: conn: 6108090768: idle check connection: sess: None, name: master, idle: 1.9967029094696045s, acqrelcnt: 1, lang: None, thrd: (73785, 7965269056), cmpt: ``, lut: 1710790784.318284
[0m16:39:46.315323 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:39:46.315553 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:39:46.315799 [debug] [MainThread]: Databricks adapter: conn: 6108090768: _release sess: None, name: master, idle: 9.5367431640625e-07s, acqrelcnt: 0, lang: None, thrd: (73785, 7965269056), cmpt: ``, lut: 1710790786.315721
[0m16:39:46.316390 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:39:46.316716 [info ] [MainThread]: 
[0m16:39:46.321458 [debug] [Thread-1 (]: Began running node model.default.stage_users
[0m16:39:46.321939 [info ] [Thread-1 (]: 1 of 1 START sql view model default.stage_users ................................ [RUN]
[0m16:39:46.322672 [debug] [Thread-1 (]: Databricks adapter: conn: 6140353040: idle check connection: sess: 01eee55f-459b-177c-9dd3-62e96fa3bc31, name: list_hive_metastore_default, idle: 0.012092828750610352s, acqrelcnt: 0, lang: None, thrd: (73785, 10932187136), cmpt: ``, lut: 1710790786.310444
[0m16:39:46.322920 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.default.stage_users)
[0m16:39:46.323175 [debug] [Thread-1 (]: Databricks adapter: conn: 6140353040: reusing connection list_hive_metastore_default sess: 01eee55f-459b-177c-9dd3-62e96fa3bc31, name: model.default.stage_users, idle: 0.012630939483642578s, acqrelcnt: 0, lang: None, thrd: (73785, 10932187136), cmpt: ``, lut: 1710790786.310444
[0m16:39:46.323424 [debug] [Thread-1 (]: Databricks adapter: On thread (73785, 10932187136): `hive_metastore`.`default`.`stage_users` using default compute resource.
[0m16:39:46.323669 [debug] [Thread-1 (]: Databricks adapter: conn: 6140353040: _acquire sess: 01eee55f-459b-177c-9dd3-62e96fa3bc31, name: model.default.stage_users, idle: 0.013129949569702148s, acqrelcnt: 1, lang: sql, thrd: (73785, 10932187136), cmpt: ``, lut: 1710790786.310444
[0m16:39:46.323920 [debug] [Thread-1 (]: Began compiling node model.default.stage_users
[0m16:39:46.329667 [debug] [Thread-1 (]: Writing injected SQL for node "model.default.stage_users"
[0m16:39:46.333400 [debug] [Thread-1 (]: Timing info for model.default.stage_users (compile): 16:39:46.324071 => 16:39:46.333204
[0m16:39:46.333695 [debug] [Thread-1 (]: Began executing node model.default.stage_users
[0m16:39:46.350122 [debug] [Thread-1 (]: Writing runtime sql for node "model.default.stage_users"
[0m16:39:46.353246 [debug] [Thread-1 (]: Databricks adapter: conn: 6140353040: get_thread_connection: sess: 01eee55f-459b-177c-9dd3-62e96fa3bc31, name: model.default.stage_users, idle: 0.042669057846069336s, acqrelcnt: 1, lang: sql, thrd: (73785, 10932187136), cmpt: ``, lut: 1710790786.310444
[0m16:39:46.353518 [debug] [Thread-1 (]: Databricks adapter: conn: 6140353040: idle check connection: sess: 01eee55f-459b-177c-9dd3-62e96fa3bc31, name: model.default.stage_users, idle: 0.04299283027648926s, acqrelcnt: 1, lang: sql, thrd: (73785, 10932187136), cmpt: ``, lut: 1710790786.310444
[0m16:39:46.353693 [debug] [Thread-1 (]: Using databricks connection "model.default.stage_users"
[0m16:39:46.353908 [debug] [Thread-1 (]: On model.default.stage_users: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_users"} */
create or replace view `hive_metastore`.`default`.`stage_users`
  
  
  
  as
    

select user_id AS user_id,
       name AS name,
       city AS city,
       phone_number AS phone_number,
       gender AS gender,
       nationality AS nationality,
       state AS state
from `pythiandbsql`.`raw`.`default.users`

[0m16:39:46.680328 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_users"} */
create or replace view `hive_metastore`.`default`.`stage_users`
  
  
  
  as
    

select user_id AS user_id,
       name AS name,
       city AS city,
       phone_number AS phone_number,
       gender AS gender,
       nationality AS nationality,
       state AS state
from `pythiandbsql`.`raw`.`default.users`

[0m16:39:46.682019 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
[0m16:39:46.683854 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UC_NOT_ENABLED] org.apache.spark.sql.AnalysisException: [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:663)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:540)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:389)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:353)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:401)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:152)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$2(Analyzer.scala:1690)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1689)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.$anonfun$applyOrElse$82(Analyzer.scala:1442)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$handleGlueError(Analyzer.scala:1556)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1442)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1402)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:199)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:188)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:188)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:83)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:377)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1277)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1276)
	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:85)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:377)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1306)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1303)
	at org.apache.spark.sql.catalyst.plans.logical.CreateView.mapChildren(v2Commands.scala:1408)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:377)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:203)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:184)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruningByConf(AnalysisHelper.scala:173)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruningByConf$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruningByConf(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1402)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1372)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1289)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:309)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:309)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:289)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$9(RuleExecutor.scala:382)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$9$adapted(RuleExecutor.scala:382)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:382)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:256)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:414)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:407)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:321)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:407)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:248)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:248)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:392)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:384)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:391)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:230)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:394)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:542)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1048)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:542)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:538)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1173)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:538)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:224)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:223)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:481)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:503)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:576)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:531)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:576)
	... 35 more

[0m16:39:46.685352 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01eee55f-469c-1831-afe4-ba113768bf0a
[0m16:39:46.686158 [debug] [Thread-1 (]: Timing info for model.default.stage_users (execute): 16:39:46.333864 => 16:39:46.685877
[0m16:39:46.686897 [debug] [Thread-1 (]: Databricks adapter: conn: 6140353040: _release sess: 01eee55f-459b-177c-9dd3-62e96fa3bc31, name: model.default.stage_users, idle: 7.152557373046875e-06s, acqrelcnt: 0, lang: sql, thrd: (73785, 10932187136), cmpt: ``, lut: 1710790786.6866949
[0m16:39:46.712961 [debug] [Thread-1 (]: Runtime Error in model stage_users (models/stage/stage_users.sql)
  [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
[0m16:39:46.713557 [debug] [Thread-1 (]: Databricks adapter: conn: 6140353040: _release sess: 01eee55f-459b-177c-9dd3-62e96fa3bc31, name: model.default.stage_users, idle: 3.0994415283203125e-06s, acqrelcnt: 0, lang: sql, thrd: (73785, 10932187136), cmpt: ``, lut: 1710790786.713387
[0m16:39:46.714043 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '117604e1-6a2a-4fed-ae85-fe8f1d1b7c31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16c1bbc90>]}
[0m16:39:46.714682 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model default.stage_users ....................... [[31mERROR[0m in 0.39s]
[0m16:39:46.715263 [debug] [Thread-1 (]: Finished running node model.default.stage_users
[0m16:39:46.716708 [debug] [MainThread]: Databricks adapter: conn: 6108090768: idle check connection: sess: None, name: master, idle: 0.4008769989013672s, acqrelcnt: 0, lang: None, thrd: (73785, 7965269056), cmpt: ``, lut: 1710790786.315721
[0m16:39:46.717049 [debug] [MainThread]: Databricks adapter: conn: 6108090768: reusing connection master sess: None, name: master, idle: 0.4012458324432373s, acqrelcnt: 0, lang: None, thrd: (73785, 7965269056), cmpt: ``, lut: 1710790786.315721
[0m16:39:46.717338 [debug] [MainThread]: Databricks adapter: Thread (73785, 7965269056) using default compute resource.
[0m16:39:46.717576 [debug] [MainThread]: Databricks adapter: conn: 6108090768: _acquire sess: None, name: master, idle: 0.4017829895019531s, acqrelcnt: 1, lang: None, thrd: (73785, 7965269056), cmpt: ``, lut: 1710790786.315721
[0m16:39:46.717842 [debug] [MainThread]: Databricks adapter: conn: 6108090768: get_thread_connection: sess: None, name: master, idle: 0.40204691886901855s, acqrelcnt: 1, lang: None, thrd: (73785, 7965269056), cmpt: ``, lut: 1710790786.315721
[0m16:39:46.718091 [debug] [MainThread]: Databricks adapter: conn: 6108090768: idle check connection: sess: None, name: master, idle: 0.4022939205169678s, acqrelcnt: 1, lang: None, thrd: (73785, 7965269056), cmpt: ``, lut: 1710790786.315721
[0m16:39:46.718327 [debug] [MainThread]: On master: ROLLBACK
[0m16:39:46.718551 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:39:47.194949 [debug] [MainThread]: Databricks adapter: conn: 6108090768: session opened sess: 01eee55f-4704-1ff2-a755-d45d0b28d2a9, name: master, idle: 7.867813110351562e-06s, acqrelcnt: 1, lang: None, thrd: (73785, 7965269056), cmpt: ``, lut: 1710790787.194582
[0m16:39:47.195870 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:39:47.196563 [debug] [MainThread]: Databricks adapter: conn: 6108090768: get_thread_connection: sess: 01eee55f-4704-1ff2-a755-d45d0b28d2a9, name: master, idle: 0.0017740726470947266s, acqrelcnt: 1, lang: None, thrd: (73785, 7965269056), cmpt: ``, lut: 1710790787.194582
[0m16:39:47.197084 [debug] [MainThread]: Databricks adapter: conn: 6108090768: idle check connection: sess: 01eee55f-4704-1ff2-a755-d45d0b28d2a9, name: master, idle: 0.0023980140686035156s, acqrelcnt: 1, lang: None, thrd: (73785, 7965269056), cmpt: ``, lut: 1710790787.194582
[0m16:39:47.197410 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:39:47.197693 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:39:47.198010 [debug] [MainThread]: Databricks adapter: conn: 6108090768: _release sess: 01eee55f-4704-1ff2-a755-d45d0b28d2a9, name: master, idle: 1.9073486328125e-06s, acqrelcnt: 0, lang: None, thrd: (73785, 7965269056), cmpt: ``, lut: 1710790787.197917
[0m16:39:47.198771 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:39:47.199076 [debug] [MainThread]: On master: ROLLBACK
[0m16:39:47.199359 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:39:47.199634 [debug] [MainThread]: On master: Close
[0m16:39:47.365335 [debug] [MainThread]: Connection 'model.default.stage_users' was properly closed.
[0m16:39:47.366733 [debug] [MainThread]: On model.default.stage_users: ROLLBACK
[0m16:39:47.367544 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:39:47.368225 [debug] [MainThread]: On model.default.stage_users: Close
[0m16:39:47.527562 [info ] [MainThread]: 
[0m16:39:47.528155 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.21 seconds (3.21s).
[0m16:39:47.528761 [debug] [MainThread]: Command end result
[0m16:39:47.550077 [info ] [MainThread]: 
[0m16:39:47.550459 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m16:39:47.550655 [info ] [MainThread]: 
[0m16:39:47.550844 [error] [MainThread]:   Runtime Error in model stage_users (models/stage/stage_users.sql)
  [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
[0m16:39:47.551015 [info ] [MainThread]: 
[0m16:39:47.551214 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m16:39:47.554582 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 4.625624, "process_user_time": 1.938331, "process_kernel_time": 3.419943, "process_mem_max_rss": "225918976", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:39:47.554904 [debug] [MainThread]: Command `cli run` failed at 16:39:47.554839 after 4.63 seconds
[0m16:39:47.555147 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ad9c6d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1045d7cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ad437d0>]}
[0m16:39:47.555357 [debug] [MainThread]: Flushing usage events
[0m16:44:49.610973 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108939a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1089995d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1089ab750>]}


============================== 16:44:49.613849 | 13af3635-1268-4129-a965-c99c5ab0ba74 ==============================
[0m16:44:49.613849 [info ] [MainThread]: Running with dbt=1.7.8
[0m16:44:49.614137 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt init', 'send_anonymous_usage_stats': 'True'}
[0m16:44:49.615168 [info ] [MainThread]: Setting up your profile.
[0m16:45:58.033460 [info ] [MainThread]: Profile default written to /Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/profiles.yml using target's profile_template.yml and your supplied values. Run 'dbt debug' to validate the connection.
[0m16:45:58.040243 [debug] [MainThread]: Resource report: {"command_name": "init", "command_success": true, "command_wall_clock_time": 68.45758, "process_user_time": 2.005731, "process_kernel_time": 3.452116, "process_mem_max_rss": "189480960", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:45:58.040897 [debug] [MainThread]: Command `dbt init` succeeded at 16:45:58.040752 after 68.46 seconds
[0m16:45:58.041220 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1077fa150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1011a1ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100d3b150>]}
[0m16:45:58.041553 [debug] [MainThread]: Flushing usage events
[0m16:46:14.544897 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113c4a890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113cc1790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113cc1dd0>]}


============================== 16:46:14.547867 | 25e1bf5a-0daf-49f4-9ef9-2003a4f4b2c8 ==============================
[0m16:46:14.547867 [info ] [MainThread]: Running with dbt=1.7.8
[0m16:46:14.548166 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt ', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:46:15.836556 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '25e1bf5a-0daf-49f4-9ef9-2003a4f4b2c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16e4cca90>]}
[0m16:46:15.865505 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '25e1bf5a-0daf-49f4-9ef9-2003a4f4b2c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16db35690>]}
[0m16:46:15.865802 [info ] [MainThread]: Registered adapter: databricks=1.7.9
[0m16:46:15.884043 [debug] [MainThread]: checksum: 67f0013ca5f0bd43af9a0873dd50792fde83ef69de63b71cacd0b4ac656c52e5, vars: {}, profile: , target: , version: 1.7.8
[0m16:46:15.892974 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m16:46:15.893261 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '25e1bf5a-0daf-49f4-9ef9-2003a4f4b2c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16b917d10>]}
[0m16:46:16.467315 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '25e1bf5a-0daf-49f4-9ef9-2003a4f4b2c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16e6658d0>]}
[0m16:46:16.481718 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '25e1bf5a-0daf-49f4-9ef9-2003a4f4b2c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x292b8b210>]}
[0m16:46:16.481938 [info ] [MainThread]: Found 3 models, 3 sources, 0 exposures, 0 metrics, 538 macros, 0 groups, 0 semantic models
[0m16:46:16.482112 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '25e1bf5a-0daf-49f4-9ef9-2003a4f4b2c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16e50a410>]}
[0m16:46:16.482761 [info ] [MainThread]: 
[0m16:46:16.483190 [debug] [MainThread]: Databricks adapter: conn: 6147168464: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (75985, 7965269056), cmpt: ``, lut: None
[0m16:46:16.483336 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:46:16.483471 [debug] [MainThread]: Databricks adapter: Thread (75985, 7965269056) using default compute resource.
[0m16:46:16.483592 [debug] [MainThread]: Databricks adapter: conn: 6147168464: _acquire sess: None, name: master, idle: 1.1920928955078125e-06s, acqrelcnt: 1, lang: None, thrd: (75985, 7965269056), cmpt: ``, lut: 1710791176.483553
[0m16:46:16.484068 [debug] [ThreadPool]: Databricks adapter: conn: 11051508944: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore, idle: 0s, acqrelcnt: 0, lang: None, thrd: (75985, 11069845504), cmpt: ``, lut: None
[0m16:46:16.484266 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m16:46:16.484400 [debug] [ThreadPool]: Databricks adapter: Thread (75985, 11069845504) using default compute resource.
[0m16:46:16.484536 [debug] [ThreadPool]: Databricks adapter: conn: 11051508944: _acquire sess: None, name: list_hive_metastore, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (75985, 11069845504), cmpt: ``, lut: 1710791176.484492
[0m16:46:16.484683 [debug] [ThreadPool]: Databricks adapter: conn: 11051508944: get_thread_connection: sess: None, name: list_hive_metastore, idle: 0.00015091896057128906s, acqrelcnt: 1, lang: None, thrd: (75985, 11069845504), cmpt: ``, lut: 1710791176.484492
[0m16:46:16.484808 [debug] [ThreadPool]: Databricks adapter: conn: 11051508944: idle check connection: sess: None, name: list_hive_metastore, idle: 0.0002770423889160156s, acqrelcnt: 1, lang: None, thrd: (75985, 11069845504), cmpt: ``, lut: 1710791176.484492
[0m16:46:16.484947 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m16:46:16.485061 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m16:46:16.485167 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:46:17.098949 [debug] [ThreadPool]: Databricks adapter: conn: 11051508944: session opened sess: 01eee560-2f68-1b3e-9066-330bd6a587ce, name: list_hive_metastore, idle: 1.0967254638671875e-05s, acqrelcnt: 1, lang: None, thrd: (75985, 11069845504), cmpt: ``, lut: 1710791177.0985758
[0m16:46:17.647286 [debug] [ThreadPool]: SQL status: OK in 1.159999966621399 seconds
[0m16:46:17.657868 [debug] [ThreadPool]: Databricks adapter: conn: 11051508944: _release sess: 01eee560-2f68-1b3e-9066-330bd6a587ce, name: list_hive_metastore, idle: 5.0067901611328125e-06s, acqrelcnt: 0, lang: None, thrd: (75985, 11069845504), cmpt: ``, lut: 1710791177.657724
[0m16:46:17.659120 [debug] [ThreadPool]: Databricks adapter: conn: 11051508944: idle check connection: sess: 01eee560-2f68-1b3e-9066-330bd6a587ce, name: list_hive_metastore, idle: 0.0012900829315185547s, acqrelcnt: 0, lang: None, thrd: (75985, 11069845504), cmpt: ``, lut: 1710791177.657724
[0m16:46:17.659422 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now list_hive_metastore_default)
[0m16:46:17.659727 [debug] [ThreadPool]: Databricks adapter: conn: 11051508944: reusing connection list_hive_metastore sess: 01eee560-2f68-1b3e-9066-330bd6a587ce, name: list_hive_metastore_default, idle: 0.0018990039825439453s, acqrelcnt: 0, lang: None, thrd: (75985, 11069845504), cmpt: ``, lut: 1710791177.657724
[0m16:46:17.659959 [debug] [ThreadPool]: Databricks adapter: Thread (75985, 11069845504) using default compute resource.
[0m16:46:17.660143 [debug] [ThreadPool]: Databricks adapter: conn: 11051508944: _acquire sess: 01eee560-2f68-1b3e-9066-330bd6a587ce, name: list_hive_metastore_default, idle: 0.002366304397583008s, acqrelcnt: 1, lang: None, thrd: (75985, 11069845504), cmpt: ``, lut: 1710791177.657724
[0m16:46:17.663013 [debug] [ThreadPool]: Databricks adapter: conn: 11051508944: get_thread_connection: sess: 01eee560-2f68-1b3e-9066-330bd6a587ce, name: list_hive_metastore_default, idle: 0.005225181579589844s, acqrelcnt: 1, lang: None, thrd: (75985, 11069845504), cmpt: ``, lut: 1710791177.657724
[0m16:46:17.663218 [debug] [ThreadPool]: Databricks adapter: conn: 11051508944: idle check connection: sess: 01eee560-2f68-1b3e-9066-330bd6a587ce, name: list_hive_metastore_default, idle: 0.005440950393676758s, acqrelcnt: 1, lang: None, thrd: (75985, 11069845504), cmpt: ``, lut: 1710791177.657724
[0m16:46:17.663376 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m16:46:17.663527 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m16:46:17.969492 [debug] [ThreadPool]: SQL status: OK in 0.3100000023841858 seconds
[0m16:46:17.980708 [debug] [ThreadPool]: Databricks adapter: conn: 11051508944: get_thread_connection: sess: 01eee560-2f68-1b3e-9066-330bd6a587ce, name: list_hive_metastore_default, idle: 0.3228490352630615s, acqrelcnt: 1, lang: None, thrd: (75985, 11069845504), cmpt: ``, lut: 1710791177.657724
[0m16:46:17.981104 [debug] [ThreadPool]: Databricks adapter: conn: 11051508944: idle check connection: sess: 01eee560-2f68-1b3e-9066-330bd6a587ce, name: list_hive_metastore_default, idle: 0.32331204414367676s, acqrelcnt: 1, lang: None, thrd: (75985, 11069845504), cmpt: ``, lut: 1710791177.657724
[0m16:46:17.981330 [debug] [ThreadPool]: Databricks adapter: conn: 11051508944: get_thread_connection: sess: 01eee560-2f68-1b3e-9066-330bd6a587ce, name: list_hive_metastore_default, idle: 0.3235480785369873s, acqrelcnt: 1, lang: None, thrd: (75985, 11069845504), cmpt: ``, lut: 1710791177.657724
[0m16:46:17.981530 [debug] [ThreadPool]: Databricks adapter: conn: 11051508944: idle check connection: sess: 01eee560-2f68-1b3e-9066-330bd6a587ce, name: list_hive_metastore_default, idle: 0.32375025749206543s, acqrelcnt: 1, lang: None, thrd: (75985, 11069845504), cmpt: ``, lut: 1710791177.657724
[0m16:46:17.981765 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m16:46:17.981947 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m16:46:17.982151 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m16:46:18.242542 [debug] [ThreadPool]: SQL status: OK in 0.25999999046325684 seconds
[0m16:46:18.251507 [debug] [ThreadPool]: Databricks adapter: conn: 11051508944: get_thread_connection: sess: 01eee560-2f68-1b3e-9066-330bd6a587ce, name: list_hive_metastore_default, idle: 0.5936481952667236s, acqrelcnt: 1, lang: None, thrd: (75985, 11069845504), cmpt: ``, lut: 1710791177.657724
[0m16:46:18.251942 [debug] [ThreadPool]: Databricks adapter: conn: 11051508944: idle check connection: sess: 01eee560-2f68-1b3e-9066-330bd6a587ce, name: list_hive_metastore_default, idle: 0.594135046005249s, acqrelcnt: 1, lang: None, thrd: (75985, 11069845504), cmpt: ``, lut: 1710791177.657724
[0m16:46:18.252192 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m16:46:18.252449 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m16:46:18.624454 [debug] [ThreadPool]: SQL status: OK in 0.3700000047683716 seconds
[0m16:46:18.630870 [debug] [ThreadPool]: Databricks adapter: conn: 11051508944: _release sess: 01eee560-2f68-1b3e-9066-330bd6a587ce, name: list_hive_metastore_default, idle: 6.9141387939453125e-06s, acqrelcnt: 0, lang: None, thrd: (75985, 11069845504), cmpt: ``, lut: 1710791178.630692
[0m16:46:18.635082 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '25e1bf5a-0daf-49f4-9ef9-2003a4f4b2c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16b8a9b90>]}
[0m16:46:18.635649 [debug] [MainThread]: Databricks adapter: conn: 6147168464: get_thread_connection: sess: None, name: master, idle: 2.1519970893859863s, acqrelcnt: 1, lang: None, thrd: (75985, 7965269056), cmpt: ``, lut: 1710791176.483553
[0m16:46:18.635945 [debug] [MainThread]: Databricks adapter: conn: 6147168464: idle check connection: sess: None, name: master, idle: 2.1523091793060303s, acqrelcnt: 1, lang: None, thrd: (75985, 7965269056), cmpt: ``, lut: 1710791176.483553
[0m16:46:18.636211 [debug] [MainThread]: Databricks adapter: conn: 6147168464: get_thread_connection: sess: None, name: master, idle: 2.152583122253418s, acqrelcnt: 1, lang: None, thrd: (75985, 7965269056), cmpt: ``, lut: 1710791176.483553
[0m16:46:18.636462 [debug] [MainThread]: Databricks adapter: conn: 6147168464: idle check connection: sess: None, name: master, idle: 2.152837038040161s, acqrelcnt: 1, lang: None, thrd: (75985, 7965269056), cmpt: ``, lut: 1710791176.483553
[0m16:46:18.636706 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:46:18.636932 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:46:18.637179 [debug] [MainThread]: Databricks adapter: conn: 6147168464: _release sess: None, name: master, idle: 9.5367431640625e-07s, acqrelcnt: 0, lang: None, thrd: (75985, 7965269056), cmpt: ``, lut: 1710791178.6371021
[0m16:46:18.637765 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:46:18.638066 [info ] [MainThread]: 
[0m16:46:18.642840 [debug] [Thread-1 (]: Began running node model.default.stage_users
[0m16:46:18.643271 [info ] [Thread-1 (]: 1 of 1 START sql view model default.stage_users ................................ [RUN]
[0m16:46:18.643931 [debug] [Thread-1 (]: Databricks adapter: conn: 11051508944: idle check connection: sess: 01eee560-2f68-1b3e-9066-330bd6a587ce, name: list_hive_metastore_default, idle: 0.013110876083374023s, acqrelcnt: 0, lang: None, thrd: (75985, 11069845504), cmpt: ``, lut: 1710791178.630692
[0m16:46:18.644193 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.default.stage_users)
[0m16:46:18.644486 [debug] [Thread-1 (]: Databricks adapter: conn: 11051508944: reusing connection list_hive_metastore_default sess: 01eee560-2f68-1b3e-9066-330bd6a587ce, name: model.default.stage_users, idle: 0.013679027557373047s, acqrelcnt: 0, lang: None, thrd: (75985, 11069845504), cmpt: ``, lut: 1710791178.630692
[0m16:46:18.644760 [debug] [Thread-1 (]: Databricks adapter: On thread (75985, 11069845504): `hive_metastore`.`default`.`stage_users` using default compute resource.
[0m16:46:18.645028 [debug] [Thread-1 (]: Databricks adapter: conn: 11051508944: _acquire sess: 01eee560-2f68-1b3e-9066-330bd6a587ce, name: model.default.stage_users, idle: 0.014227151870727539s, acqrelcnt: 1, lang: sql, thrd: (75985, 11069845504), cmpt: ``, lut: 1710791178.630692
[0m16:46:18.645304 [debug] [Thread-1 (]: Began compiling node model.default.stage_users
[0m16:46:18.651363 [debug] [Thread-1 (]: Writing injected SQL for node "model.default.stage_users"
[0m16:46:18.654098 [debug] [Thread-1 (]: Timing info for model.default.stage_users (compile): 16:46:18.645479 => 16:46:18.653891
[0m16:46:18.654401 [debug] [Thread-1 (]: Began executing node model.default.stage_users
[0m16:46:18.671974 [debug] [Thread-1 (]: Writing runtime sql for node "model.default.stage_users"
[0m16:46:18.674967 [debug] [Thread-1 (]: Databricks adapter: conn: 11051508944: get_thread_connection: sess: 01eee560-2f68-1b3e-9066-330bd6a587ce, name: model.default.stage_users, idle: 0.04415488243103027s, acqrelcnt: 1, lang: sql, thrd: (75985, 11069845504), cmpt: ``, lut: 1710791178.630692
[0m16:46:18.675245 [debug] [Thread-1 (]: Databricks adapter: conn: 11051508944: idle check connection: sess: 01eee560-2f68-1b3e-9066-330bd6a587ce, name: model.default.stage_users, idle: 0.0444638729095459s, acqrelcnt: 1, lang: sql, thrd: (75985, 11069845504), cmpt: ``, lut: 1710791178.630692
[0m16:46:18.675420 [debug] [Thread-1 (]: Using databricks connection "model.default.stage_users"
[0m16:46:18.675638 [debug] [Thread-1 (]: On model.default.stage_users: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_users"} */
create or replace view `hive_metastore`.`default`.`stage_users`
  
  
  
  as
    

select user_id AS user_id,
       name AS name,
       city AS city,
       phone_number AS phone_number,
       gender AS gender,
       nationality AS nationality,
       state AS state
from `pythiandbsql`.`raw`.`default.users`

[0m16:46:19.035157 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_users"} */
create or replace view `hive_metastore`.`default`.`stage_users`
  
  
  
  as
    

select user_id AS user_id,
       name AS name,
       city AS city,
       phone_number AS phone_number,
       gender AS gender,
       nationality AS nationality,
       state AS state
from `pythiandbsql`.`raw`.`default.users`

[0m16:46:19.035630 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
[0m16:46:19.036517 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UC_NOT_ENABLED] org.apache.spark.sql.AnalysisException: [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:663)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:540)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:389)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:353)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:401)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:152)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$2(Analyzer.scala:1690)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1689)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.$anonfun$applyOrElse$82(Analyzer.scala:1442)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$handleGlueError(Analyzer.scala:1556)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1442)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1402)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:199)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:188)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:188)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:83)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:377)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1277)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1276)
	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:85)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:377)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1306)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1303)
	at org.apache.spark.sql.catalyst.plans.logical.CreateView.mapChildren(v2Commands.scala:1408)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:377)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:203)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:184)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruningByConf(AnalysisHelper.scala:173)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruningByConf$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruningByConf(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1402)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1372)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1289)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:309)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:309)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:289)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$9(RuleExecutor.scala:382)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$9$adapted(RuleExecutor.scala:382)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:382)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:256)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:414)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:407)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:321)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:407)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:248)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:248)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:392)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:384)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:391)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:230)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:394)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:542)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1048)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:542)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:538)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1173)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:538)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:224)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:223)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:481)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:503)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:576)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:531)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:576)
	... 35 more

[0m16:46:19.037374 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01eee560-3078-118a-943d-bb3e4cdfaedf
[0m16:46:19.037746 [debug] [Thread-1 (]: Timing info for model.default.stage_users (execute): 16:46:18.654562 => 16:46:19.037595
[0m16:46:19.038045 [debug] [Thread-1 (]: Databricks adapter: conn: 11051508944: _release sess: 01eee560-2f68-1b3e-9066-330bd6a587ce, name: model.default.stage_users, idle: 2.86102294921875e-06s, acqrelcnt: 0, lang: sql, thrd: (75985, 11069845504), cmpt: ``, lut: 1710791179.037934
[0m16:46:19.067394 [debug] [Thread-1 (]: Runtime Error in model stage_users (models/stage/stage_users.sql)
  [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
[0m16:46:19.067769 [debug] [Thread-1 (]: Databricks adapter: conn: 11051508944: _release sess: 01eee560-2f68-1b3e-9066-330bd6a587ce, name: model.default.stage_users, idle: 2.1457672119140625e-06s, acqrelcnt: 0, lang: sql, thrd: (75985, 11069845504), cmpt: ``, lut: 1710791179.067648
[0m16:46:19.068079 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '25e1bf5a-0daf-49f4-9ef9-2003a4f4b2c8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x292b54410>]}
[0m16:46:19.068513 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model default.stage_users ....................... [[31mERROR[0m in 0.42s]
[0m16:46:19.068916 [debug] [Thread-1 (]: Finished running node model.default.stage_users
[0m16:46:19.070042 [debug] [MainThread]: Databricks adapter: conn: 6147168464: idle check connection: sess: None, name: master, idle: 0.4328329563140869s, acqrelcnt: 0, lang: None, thrd: (75985, 7965269056), cmpt: ``, lut: 1710791178.6371021
[0m16:46:19.070357 [debug] [MainThread]: Databricks adapter: conn: 6147168464: reusing connection master sess: None, name: master, idle: 0.4331777095794678s, acqrelcnt: 0, lang: None, thrd: (75985, 7965269056), cmpt: ``, lut: 1710791178.6371021
[0m16:46:19.070569 [debug] [MainThread]: Databricks adapter: Thread (75985, 7965269056) using default compute resource.
[0m16:46:19.070768 [debug] [MainThread]: Databricks adapter: conn: 6147168464: _acquire sess: None, name: master, idle: 0.43360471725463867s, acqrelcnt: 1, lang: None, thrd: (75985, 7965269056), cmpt: ``, lut: 1710791178.6371021
[0m16:46:19.070977 [debug] [MainThread]: Databricks adapter: conn: 6147168464: get_thread_connection: sess: None, name: master, idle: 0.43381786346435547s, acqrelcnt: 1, lang: None, thrd: (75985, 7965269056), cmpt: ``, lut: 1710791178.6371021
[0m16:46:19.071170 [debug] [MainThread]: Databricks adapter: conn: 6147168464: idle check connection: sess: None, name: master, idle: 0.43401193618774414s, acqrelcnt: 1, lang: None, thrd: (75985, 7965269056), cmpt: ``, lut: 1710791178.6371021
[0m16:46:19.071359 [debug] [MainThread]: On master: ROLLBACK
[0m16:46:19.071539 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:46:19.526711 [debug] [MainThread]: Databricks adapter: conn: 6147168464: session opened sess: 01eee560-30de-1568-91df-d0af05baeb72, name: master, idle: 2.86102294921875e-06s, acqrelcnt: 1, lang: None, thrd: (75985, 7965269056), cmpt: ``, lut: 1710791179.5265641
[0m16:46:19.527075 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:46:19.527309 [debug] [MainThread]: Databricks adapter: conn: 6147168464: get_thread_connection: sess: 01eee560-30de-1568-91df-d0af05baeb72, name: master, idle: 0.0006830692291259766s, acqrelcnt: 1, lang: None, thrd: (75985, 7965269056), cmpt: ``, lut: 1710791179.5265641
[0m16:46:19.527528 [debug] [MainThread]: Databricks adapter: conn: 6147168464: idle check connection: sess: 01eee560-30de-1568-91df-d0af05baeb72, name: master, idle: 0.0008938312530517578s, acqrelcnt: 1, lang: None, thrd: (75985, 7965269056), cmpt: ``, lut: 1710791179.5265641
[0m16:46:19.527724 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:46:19.527900 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:46:19.528105 [debug] [MainThread]: Databricks adapter: conn: 6147168464: _release sess: 01eee560-30de-1568-91df-d0af05baeb72, name: master, idle: 1.1920928955078125e-06s, acqrelcnt: 0, lang: None, thrd: (75985, 7965269056), cmpt: ``, lut: 1710791179.528041
[0m16:46:19.528580 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:46:19.528772 [debug] [MainThread]: On master: ROLLBACK
[0m16:46:19.528952 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:46:19.529126 [debug] [MainThread]: On master: Close
[0m16:46:19.693380 [debug] [MainThread]: Connection 'model.default.stage_users' was properly closed.
[0m16:46:19.694783 [debug] [MainThread]: On model.default.stage_users: ROLLBACK
[0m16:46:19.695539 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:46:19.696147 [debug] [MainThread]: On model.default.stage_users: Close
[0m16:46:19.873482 [info ] [MainThread]: 
[0m16:46:19.874843 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.39 seconds (3.39s).
[0m16:46:19.876556 [debug] [MainThread]: Command end result
[0m16:46:19.908911 [info ] [MainThread]: 
[0m16:46:19.909465 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m16:46:19.909724 [info ] [MainThread]: 
[0m16:46:19.909968 [error] [MainThread]:   Runtime Error in model stage_users (models/stage/stage_users.sql)
  [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
[0m16:46:19.910214 [info ] [MainThread]: 
[0m16:46:19.910498 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m16:46:19.919111 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 5.402468, "process_user_time": 2.650567, "process_kernel_time": 3.387878, "process_mem_max_rss": "227491840", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:46:19.919476 [debug] [MainThread]: Command `cli run` failed at 16:46:19.919395 after 5.40 seconds
[0m16:46:19.919764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113cc3a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113cc1ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104b2fcd0>]}
[0m16:46:19.920017 [debug] [MainThread]: Flushing usage events
[0m16:46:34.875136 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1166cfc90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1166cfb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1166fe390>]}


============================== 16:46:34.877521 | 5c8f1155-9720-4427-843c-07a4dd684916 ==============================
[0m16:46:34.877521 [info ] [MainThread]: Running with dbt=1.7.8
[0m16:46:34.877823 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m16:46:36.028574 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5c8f1155-9720-4427-843c-07a4dd684916', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13125a3d0>]}
[0m16:46:36.057408 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5c8f1155-9720-4427-843c-07a4dd684916', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11671c3d0>]}
[0m16:46:36.057718 [info ] [MainThread]: Registered adapter: databricks=1.7.9
[0m16:46:36.079210 [debug] [MainThread]: checksum: 67f0013ca5f0bd43af9a0873dd50792fde83ef69de63b71cacd0b4ac656c52e5, vars: {}, profile: , target: , version: 1.7.8
[0m16:46:36.158395 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:46:36.158839 [debug] [MainThread]: Partial parsing: updated file: default://models/stage/stage_users.sql
[0m16:46:36.209080 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.default.stage_users' (models/stage/stage_users.sql) depends on a source named 'raw.users' which was not found
[0m16:46:36.211306 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 1.3656005, "process_user_time": 2.063537, "process_kernel_time": 3.278624, "process_mem_max_rss": "209387520", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:46:36.211598 [debug] [MainThread]: Command `cli run` failed at 16:46:36.211542 after 1.37 seconds
[0m16:46:36.211807 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1166dda90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1166fd550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1110d3c10>]}
[0m16:46:36.212000 [debug] [MainThread]: Flushing usage events
[0m16:47:12.859014 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a47990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ac1ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ac2510>]}


============================== 16:47:12.862252 | 3514f88a-2ed4-4932-9cb0-dbb8a99cc024 ==============================
[0m16:47:12.862252 [info ] [MainThread]: Running with dbt=1.7.8
[0m16:47:12.862564 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:47:14.316805 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3514f88a-2ed4-4932-9cb0-dbb8a99cc024', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a5cb50>]}
[0m16:47:14.345282 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3514f88a-2ed4-4932-9cb0-dbb8a99cc024', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a5cb50>]}
[0m16:47:14.345546 [info ] [MainThread]: Registered adapter: databricks=1.7.9
[0m16:47:14.361527 [debug] [MainThread]: checksum: 67f0013ca5f0bd43af9a0873dd50792fde83ef69de63b71cacd0b4ac656c52e5, vars: {}, profile: , target: , version: 1.7.8
[0m16:47:14.439761 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m16:47:14.440161 [debug] [MainThread]: Partial parsing: updated file: default://models/sources.yml
[0m16:47:14.440311 [debug] [MainThread]: Partial parsing: updated file: default://models/stage/stage_users.sql
[0m16:47:14.516989 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3514f88a-2ed4-4932-9cb0-dbb8a99cc024', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1530d5a50>]}
[0m16:47:14.526259 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3514f88a-2ed4-4932-9cb0-dbb8a99cc024', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15318c4d0>]}
[0m16:47:14.526499 [info ] [MainThread]: Found 3 models, 3 sources, 0 exposures, 0 metrics, 538 macros, 0 groups, 0 semantic models
[0m16:47:14.526670 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3514f88a-2ed4-4932-9cb0-dbb8a99cc024', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1531a3110>]}
[0m16:47:14.527324 [info ] [MainThread]: 
[0m16:47:14.527752 [debug] [MainThread]: Databricks adapter: conn: 5688040592: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (76317, 7965269056), cmpt: ``, lut: None
[0m16:47:14.527904 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:47:14.528017 [debug] [MainThread]: Databricks adapter: Thread (76317, 7965269056) using default compute resource.
[0m16:47:14.528131 [debug] [MainThread]: Databricks adapter: conn: 5688040592: _acquire sess: None, name: master, idle: 1.1920928955078125e-06s, acqrelcnt: 1, lang: None, thrd: (76317, 7965269056), cmpt: ``, lut: 1710791234.5280929
[0m16:47:14.528610 [debug] [ThreadPool]: Databricks adapter: conn: 5686984592: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore, idle: 0s, acqrelcnt: 0, lang: None, thrd: (76317, 10854887424), cmpt: ``, lut: None
[0m16:47:14.528781 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m16:47:14.528905 [debug] [ThreadPool]: Databricks adapter: Thread (76317, 10854887424) using default compute resource.
[0m16:47:14.529021 [debug] [ThreadPool]: Databricks adapter: conn: 5686984592: _acquire sess: None, name: list_hive_metastore, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (76317, 10854887424), cmpt: ``, lut: 1710791234.528984
[0m16:47:14.529155 [debug] [ThreadPool]: Databricks adapter: conn: 5686984592: get_thread_connection: sess: None, name: list_hive_metastore, idle: 0.00013494491577148438s, acqrelcnt: 1, lang: None, thrd: (76317, 10854887424), cmpt: ``, lut: 1710791234.528984
[0m16:47:14.529275 [debug] [ThreadPool]: Databricks adapter: conn: 5686984592: idle check connection: sess: None, name: list_hive_metastore, idle: 0.0002529621124267578s, acqrelcnt: 1, lang: None, thrd: (76317, 10854887424), cmpt: ``, lut: 1710791234.528984
[0m16:47:14.529383 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m16:47:14.529498 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m16:47:14.529606 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:47:15.005119 [debug] [ThreadPool]: Databricks adapter: conn: 5686984592: session opened sess: 01eee560-51ee-1869-82dc-410426f21bac, name: list_hive_metastore, idle: 1.71661376953125e-05s, acqrelcnt: 1, lang: None, thrd: (76317, 10854887424), cmpt: ``, lut: 1710791235.004596
[0m16:47:15.284235 [debug] [ThreadPool]: SQL status: OK in 0.75 seconds
[0m16:47:15.294275 [debug] [ThreadPool]: Databricks adapter: conn: 5686984592: _release sess: 01eee560-51ee-1869-82dc-410426f21bac, name: list_hive_metastore, idle: 6.198883056640625e-06s, acqrelcnt: 0, lang: None, thrd: (76317, 10854887424), cmpt: ``, lut: 1710791235.2941608
[0m16:47:15.295687 [debug] [ThreadPool]: Databricks adapter: conn: 5686984592: idle check connection: sess: 01eee560-51ee-1869-82dc-410426f21bac, name: list_hive_metastore, idle: 0.0014450550079345703s, acqrelcnt: 0, lang: None, thrd: (76317, 10854887424), cmpt: ``, lut: 1710791235.2941608
[0m16:47:15.295961 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now list_hive_metastore_default)
[0m16:47:15.296192 [debug] [ThreadPool]: Databricks adapter: conn: 5686984592: reusing connection list_hive_metastore sess: 01eee560-51ee-1869-82dc-410426f21bac, name: list_hive_metastore_default, idle: 0.001961231231689453s, acqrelcnt: 0, lang: None, thrd: (76317, 10854887424), cmpt: ``, lut: 1710791235.2941608
[0m16:47:15.296406 [debug] [ThreadPool]: Databricks adapter: Thread (76317, 10854887424) using default compute resource.
[0m16:47:15.296616 [debug] [ThreadPool]: Databricks adapter: conn: 5686984592: _acquire sess: 01eee560-51ee-1869-82dc-410426f21bac, name: list_hive_metastore_default, idle: 0.0023910999298095703s, acqrelcnt: 1, lang: None, thrd: (76317, 10854887424), cmpt: ``, lut: 1710791235.2941608
[0m16:47:15.300224 [debug] [ThreadPool]: Databricks adapter: conn: 5686984592: get_thread_connection: sess: 01eee560-51ee-1869-82dc-410426f21bac, name: list_hive_metastore_default, idle: 0.005988121032714844s, acqrelcnt: 1, lang: None, thrd: (76317, 10854887424), cmpt: ``, lut: 1710791235.2941608
[0m16:47:15.300452 [debug] [ThreadPool]: Databricks adapter: conn: 5686984592: idle check connection: sess: 01eee560-51ee-1869-82dc-410426f21bac, name: list_hive_metastore_default, idle: 0.00623011589050293s, acqrelcnt: 1, lang: None, thrd: (76317, 10854887424), cmpt: ``, lut: 1710791235.2941608
[0m16:47:15.300631 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m16:47:15.300805 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m16:47:15.602506 [debug] [ThreadPool]: SQL status: OK in 0.30000001192092896 seconds
[0m16:47:15.614106 [debug] [ThreadPool]: Databricks adapter: conn: 5686984592: get_thread_connection: sess: 01eee560-51ee-1869-82dc-410426f21bac, name: list_hive_metastore_default, idle: 0.31978535652160645s, acqrelcnt: 1, lang: None, thrd: (76317, 10854887424), cmpt: ``, lut: 1710791235.2941608
[0m16:47:15.614556 [debug] [ThreadPool]: Databricks adapter: conn: 5686984592: idle check connection: sess: 01eee560-51ee-1869-82dc-410426f21bac, name: list_hive_metastore_default, idle: 0.320324182510376s, acqrelcnt: 1, lang: None, thrd: (76317, 10854887424), cmpt: ``, lut: 1710791235.2941608
[0m16:47:15.614802 [debug] [ThreadPool]: Databricks adapter: conn: 5686984592: get_thread_connection: sess: 01eee560-51ee-1869-82dc-410426f21bac, name: list_hive_metastore_default, idle: 0.3205721378326416s, acqrelcnt: 1, lang: None, thrd: (76317, 10854887424), cmpt: ``, lut: 1710791235.2941608
[0m16:47:15.615030 [debug] [ThreadPool]: Databricks adapter: conn: 5686984592: idle check connection: sess: 01eee560-51ee-1869-82dc-410426f21bac, name: list_hive_metastore_default, idle: 0.32080507278442383s, acqrelcnt: 1, lang: None, thrd: (76317, 10854887424), cmpt: ``, lut: 1710791235.2941608
[0m16:47:15.615240 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m16:47:15.615429 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m16:47:15.615655 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m16:47:15.874504 [debug] [ThreadPool]: SQL status: OK in 0.25999999046325684 seconds
[0m16:47:15.882233 [debug] [ThreadPool]: Databricks adapter: conn: 5686984592: get_thread_connection: sess: 01eee560-51ee-1869-82dc-410426f21bac, name: list_hive_metastore_default, idle: 0.5879580974578857s, acqrelcnt: 1, lang: None, thrd: (76317, 10854887424), cmpt: ``, lut: 1710791235.2941608
[0m16:47:15.882553 [debug] [ThreadPool]: Databricks adapter: conn: 5686984592: idle check connection: sess: 01eee560-51ee-1869-82dc-410426f21bac, name: list_hive_metastore_default, idle: 0.5883221626281738s, acqrelcnt: 1, lang: None, thrd: (76317, 10854887424), cmpt: ``, lut: 1710791235.2941608
[0m16:47:15.882768 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m16:47:15.882993 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m16:47:16.184431 [debug] [ThreadPool]: SQL status: OK in 0.30000001192092896 seconds
[0m16:47:16.189837 [debug] [ThreadPool]: Databricks adapter: conn: 5686984592: _release sess: 01eee560-51ee-1869-82dc-410426f21bac, name: list_hive_metastore_default, idle: 4.0531158447265625e-06s, acqrelcnt: 0, lang: None, thrd: (76317, 10854887424), cmpt: ``, lut: 1710791236.18964
[0m16:47:16.193711 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3514f88a-2ed4-4932-9cb0-dbb8a99cc024', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x152c68c10>]}
[0m16:47:16.194401 [debug] [MainThread]: Databricks adapter: conn: 5688040592: get_thread_connection: sess: None, name: master, idle: 1.6662001609802246s, acqrelcnt: 1, lang: None, thrd: (76317, 7965269056), cmpt: ``, lut: 1710791234.5280929
[0m16:47:16.194692 [debug] [MainThread]: Databricks adapter: conn: 5688040592: idle check connection: sess: None, name: master, idle: 1.6665160655975342s, acqrelcnt: 1, lang: None, thrd: (76317, 7965269056), cmpt: ``, lut: 1710791234.5280929
[0m16:47:16.194953 [debug] [MainThread]: Databricks adapter: conn: 5688040592: get_thread_connection: sess: None, name: master, idle: 1.666788101196289s, acqrelcnt: 1, lang: None, thrd: (76317, 7965269056), cmpt: ``, lut: 1710791234.5280929
[0m16:47:16.195202 [debug] [MainThread]: Databricks adapter: conn: 5688040592: idle check connection: sess: None, name: master, idle: 1.6670341491699219s, acqrelcnt: 1, lang: None, thrd: (76317, 7965269056), cmpt: ``, lut: 1710791234.5280929
[0m16:47:16.195434 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:47:16.195662 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:47:16.195928 [debug] [MainThread]: Databricks adapter: conn: 5688040592: _release sess: None, name: master, idle: 1.6689300537109375e-06s, acqrelcnt: 0, lang: None, thrd: (76317, 7965269056), cmpt: ``, lut: 1710791236.1958451
[0m16:47:16.196626 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:47:16.196915 [info ] [MainThread]: 
[0m16:47:16.200795 [debug] [Thread-1 (]: Began running node model.default.stage_users
[0m16:47:16.201323 [info ] [Thread-1 (]: 1 of 1 START sql view model default.stage_users ................................ [RUN]
[0m16:47:16.202083 [debug] [Thread-1 (]: Databricks adapter: conn: 5686984592: idle check connection: sess: 01eee560-51ee-1869-82dc-410426f21bac, name: list_hive_metastore_default, idle: 0.012305974960327148s, acqrelcnt: 0, lang: None, thrd: (76317, 10854887424), cmpt: ``, lut: 1710791236.18964
[0m16:47:16.202358 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.default.stage_users)
[0m16:47:16.202662 [debug] [Thread-1 (]: Databricks adapter: conn: 5686984592: reusing connection list_hive_metastore_default sess: 01eee560-51ee-1869-82dc-410426f21bac, name: model.default.stage_users, idle: 0.012904882431030273s, acqrelcnt: 0, lang: None, thrd: (76317, 10854887424), cmpt: ``, lut: 1710791236.18964
[0m16:47:16.202949 [debug] [Thread-1 (]: Databricks adapter: On thread (76317, 10854887424): `hive_metastore`.`default`.`stage_users` using default compute resource.
[0m16:47:16.203227 [debug] [Thread-1 (]: Databricks adapter: conn: 5686984592: _acquire sess: 01eee560-51ee-1869-82dc-410426f21bac, name: model.default.stage_users, idle: 0.013477087020874023s, acqrelcnt: 1, lang: sql, thrd: (76317, 10854887424), cmpt: ``, lut: 1710791236.18964
[0m16:47:16.203511 [debug] [Thread-1 (]: Began compiling node model.default.stage_users
[0m16:47:16.209436 [debug] [Thread-1 (]: Writing injected SQL for node "model.default.stage_users"
[0m16:47:16.227875 [debug] [Thread-1 (]: Timing info for model.default.stage_users (compile): 16:47:16.203688 => 16:47:16.227629
[0m16:47:16.228213 [debug] [Thread-1 (]: Began executing node model.default.stage_users
[0m16:47:16.245869 [debug] [Thread-1 (]: Writing runtime sql for node "model.default.stage_users"
[0m16:47:16.254846 [debug] [Thread-1 (]: Databricks adapter: conn: 5686984592: get_thread_connection: sess: 01eee560-51ee-1869-82dc-410426f21bac, name: model.default.stage_users, idle: 0.06508398056030273s, acqrelcnt: 1, lang: sql, thrd: (76317, 10854887424), cmpt: ``, lut: 1710791236.18964
[0m16:47:16.255101 [debug] [Thread-1 (]: Databricks adapter: conn: 5686984592: idle check connection: sess: 01eee560-51ee-1869-82dc-410426f21bac, name: model.default.stage_users, idle: 0.06537580490112305s, acqrelcnt: 1, lang: sql, thrd: (76317, 10854887424), cmpt: ``, lut: 1710791236.18964
[0m16:47:16.255285 [debug] [Thread-1 (]: Using databricks connection "model.default.stage_users"
[0m16:47:16.255527 [debug] [Thread-1 (]: On model.default.stage_users: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_users"} */
create or replace view `hive_metastore`.`default`.`stage_users`
  
  
  
  as
    

select user_id AS user_id,
       name AS name,
       city AS city,
       phone_number AS phone_number,
       gender AS gender,
       nationality AS nationality,
       state AS state
from `pythiandbsql`.`raw`.`users`

[0m16:47:16.609862 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_users"} */
create or replace view `hive_metastore`.`default`.`stage_users`
  
  
  
  as
    

select user_id AS user_id,
       name AS name,
       city AS city,
       phone_number AS phone_number,
       gender AS gender,
       nationality AS nationality,
       state AS state
from `pythiandbsql`.`raw`.`users`

[0m16:47:16.611225 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
[0m16:47:16.613031 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UC_NOT_ENABLED] org.apache.spark.sql.AnalysisException: [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:663)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:540)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:389)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:353)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:401)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.AnalysisException: [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndIdentifier$.unapply(LookupCatalog.scala:152)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveRelation$2(Analyzer.scala:1690)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveRelation(Analyzer.scala:1689)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.$anonfun$applyOrElse$82(Analyzer.scala:1442)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$handleGlueError(Analyzer.scala:1556)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1442)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply0$1.applyOrElse(Analyzer.scala:1402)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:199)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.apply(AnalysisHelper.scala:188)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anon$1.applyOrElse(AnalysisHelper.scala:188)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:83)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:377)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1277)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1276)
	at org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:85)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:377)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1306)
	at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1303)
	at org.apache.spark.sql.catalyst.plans.logical.CreateView.mapChildren(v2Commands.scala:1408)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:377)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning(AnalysisHelper.scala:203)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruning$(AnalysisHelper.scala:184)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruningByConf(AnalysisHelper.scala:173)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithSubqueriesAndPruningByConf$(AnalysisHelper.scala:169)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithSubqueriesAndPruningByConf(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply0(Analyzer.scala:1402)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1372)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1289)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:309)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:309)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:289)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$9(RuleExecutor.scala:382)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$9$adapted(RuleExecutor.scala:382)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:382)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:256)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:414)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:407)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:321)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:407)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:248)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:248)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:392)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:384)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:391)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:230)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:394)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:542)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1048)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:542)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:538)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1173)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:538)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:224)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:223)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:481)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:503)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:576)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:531)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:576)
	... 35 more

[0m16:47:16.614780 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01eee560-52ca-1d61-96ec-65e5869a11f4
[0m16:47:16.615765 [debug] [Thread-1 (]: Timing info for model.default.stage_users (execute): 16:47:16.228365 => 16:47:16.615473
[0m16:47:16.616383 [debug] [Thread-1 (]: Databricks adapter: conn: 5686984592: _release sess: 01eee560-51ee-1869-82dc-410426f21bac, name: model.default.stage_users, idle: 8.344650268554688e-06s, acqrelcnt: 0, lang: sql, thrd: (76317, 10854887424), cmpt: ``, lut: 1710791236.6162078
[0m16:47:16.648707 [debug] [Thread-1 (]: Runtime Error in model stage_users (models/stage/stage_users.sql)
  [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
[0m16:47:16.649304 [debug] [Thread-1 (]: Databricks adapter: conn: 5686984592: _release sess: 01eee560-51ee-1869-82dc-410426f21bac, name: model.default.stage_users, idle: 3.0994415283203125e-06s, acqrelcnt: 0, lang: sql, thrd: (76317, 10854887424), cmpt: ``, lut: 1710791236.649121
[0m16:47:16.649769 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3514f88a-2ed4-4932-9cb0-dbb8a99cc024', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1532fa290>]}
[0m16:47:16.650395 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model default.stage_users ....................... [[31mERROR[0m in 0.45s]
[0m16:47:16.650963 [debug] [Thread-1 (]: Finished running node model.default.stage_users
[0m16:47:16.652812 [debug] [MainThread]: Databricks adapter: conn: 5688040592: idle check connection: sess: None, name: master, idle: 0.45675086975097656s, acqrelcnt: 0, lang: None, thrd: (76317, 7965269056), cmpt: ``, lut: 1710791236.1958451
[0m16:47:16.653404 [debug] [MainThread]: Databricks adapter: conn: 5688040592: reusing connection master sess: None, name: master, idle: 0.45746779441833496s, acqrelcnt: 0, lang: None, thrd: (76317, 7965269056), cmpt: ``, lut: 1710791236.1958451
[0m16:47:16.653679 [debug] [MainThread]: Databricks adapter: Thread (76317, 7965269056) using default compute resource.
[0m16:47:16.653933 [debug] [MainThread]: Databricks adapter: conn: 5688040592: _acquire sess: None, name: master, idle: 0.45801591873168945s, acqrelcnt: 1, lang: None, thrd: (76317, 7965269056), cmpt: ``, lut: 1710791236.1958451
[0m16:47:16.654219 [debug] [MainThread]: Databricks adapter: conn: 5688040592: get_thread_connection: sess: None, name: master, idle: 0.458298921585083s, acqrelcnt: 1, lang: None, thrd: (76317, 7965269056), cmpt: ``, lut: 1710791236.1958451
[0m16:47:16.654473 [debug] [MainThread]: Databricks adapter: conn: 5688040592: idle check connection: sess: None, name: master, idle: 0.45854973793029785s, acqrelcnt: 1, lang: None, thrd: (76317, 7965269056), cmpt: ``, lut: 1710791236.1958451
[0m16:47:16.654720 [debug] [MainThread]: On master: ROLLBACK
[0m16:47:16.654953 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:47:17.128765 [debug] [MainThread]: Databricks adapter: conn: 5688040592: session opened sess: 01eee560-5331-11b5-a327-bd5e7f94f271, name: master, idle: 1.0013580322265625e-05s, acqrelcnt: 1, lang: None, thrd: (76317, 7965269056), cmpt: ``, lut: 1710791237.128354
[0m16:47:17.129932 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:47:17.130561 [debug] [MainThread]: Databricks adapter: conn: 5688040592: get_thread_connection: sess: 01eee560-5331-11b5-a327-bd5e7f94f271, name: master, idle: 0.002070903778076172s, acqrelcnt: 1, lang: None, thrd: (76317, 7965269056), cmpt: ``, lut: 1710791237.128354
[0m16:47:17.130944 [debug] [MainThread]: Databricks adapter: conn: 5688040592: idle check connection: sess: 01eee560-5331-11b5-a327-bd5e7f94f271, name: master, idle: 0.0024919509887695312s, acqrelcnt: 1, lang: None, thrd: (76317, 7965269056), cmpt: ``, lut: 1710791237.128354
[0m16:47:17.131255 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:47:17.131534 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:47:17.131838 [debug] [MainThread]: Databricks adapter: conn: 5688040592: _release sess: 01eee560-5331-11b5-a327-bd5e7f94f271, name: master, idle: 2.1457672119140625e-06s, acqrelcnt: 0, lang: None, thrd: (76317, 7965269056), cmpt: ``, lut: 1710791237.131744
[0m16:47:17.132559 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:47:17.132850 [debug] [MainThread]: On master: ROLLBACK
[0m16:47:17.133122 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:47:17.133376 [debug] [MainThread]: On master: Close
[0m16:47:17.307225 [debug] [MainThread]: Connection 'model.default.stage_users' was properly closed.
[0m16:47:17.308494 [debug] [MainThread]: On model.default.stage_users: ROLLBACK
[0m16:47:17.309240 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:47:17.309876 [debug] [MainThread]: On model.default.stage_users: Close
[0m16:47:17.475786 [info ] [MainThread]: 
[0m16:47:17.477037 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 2.95 seconds (2.95s).
[0m16:47:17.479325 [debug] [MainThread]: Command end result
[0m16:47:17.550552 [info ] [MainThread]: 
[0m16:47:17.551062 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m16:47:17.551325 [info ] [MainThread]: 
[0m16:47:17.551579 [error] [MainThread]:   Runtime Error in model stage_users (models/stage/stage_users.sql)
  [UC_NOT_ENABLED] Unity Catalog is not enabled on this cluster. SQLSTATE: 56038
[0m16:47:17.551834 [info ] [MainThread]: 
[0m16:47:17.552105 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m16:47:17.555657 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 4.7260504, "process_user_time": 2.276435, "process_kernel_time": 3.438696, "process_mem_max_rss": "229343232", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:47:17.556125 [debug] [MainThread]: Command `cli run` failed at 16:47:17.556029 after 4.73 seconds
[0m16:47:17.556496 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ac22d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a6ec90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100583cd0>]}
[0m16:47:17.556808 [debug] [MainThread]: Flushing usage events
[0m16:52:16.471628 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10af43b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10afc1a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10afc2050>]}


============================== 16:52:16.474478 | f5d7d3e5-37d9-4a6e-8e34-2fcd8bb2f841 ==============================
[0m16:52:16.474478 [info ] [MainThread]: Running with dbt=1.7.8
[0m16:52:16.474763 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m16:52:18.177393 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f5d7d3e5-37d9-4a6e-8e34-2fcd8bb2f841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15cb85f10>]}
[0m16:52:18.205933 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f5d7d3e5-37d9-4a6e-8e34-2fcd8bb2f841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15cab7f50>]}
[0m16:52:18.206184 [info ] [MainThread]: Registered adapter: databricks=1.7.9
[0m16:52:18.225758 [debug] [MainThread]: checksum: 67f0013ca5f0bd43af9a0873dd50792fde83ef69de63b71cacd0b4ac656c52e5, vars: {}, profile: , target: , version: 1.7.8
[0m16:52:18.236239 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m16:52:18.236515 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'f5d7d3e5-37d9-4a6e-8e34-2fcd8bb2f841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15cb469d0>]}
[0m16:52:18.805138 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f5d7d3e5-37d9-4a6e-8e34-2fcd8bb2f841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15ce09b90>]}
[0m16:52:18.817428 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f5d7d3e5-37d9-4a6e-8e34-2fcd8bb2f841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15cdb5f50>]}
[0m16:52:18.817662 [info ] [MainThread]: Found 3 models, 3 sources, 0 exposures, 0 metrics, 538 macros, 0 groups, 0 semantic models
[0m16:52:18.817828 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f5d7d3e5-37d9-4a6e-8e34-2fcd8bb2f841', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15d0a75d0>]}
[0m16:52:18.818479 [info ] [MainThread]: 
[0m16:52:18.818943 [debug] [MainThread]: Databricks adapter: conn: 5852585616: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (77897, 7965269056), cmpt: ``, lut: None
[0m16:52:18.819112 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:52:18.819238 [debug] [MainThread]: Databricks adapter: Thread (77897, 7965269056) using default compute resource.
[0m16:52:18.819359 [debug] [MainThread]: Databricks adapter: conn: 5852585616: _acquire sess: None, name: master, idle: 2.1457672119140625e-06s, acqrelcnt: 1, lang: None, thrd: (77897, 7965269056), cmpt: ``, lut: 1710791538.81932
[0m16:52:18.819833 [debug] [ThreadPool]: Databricks adapter: conn: 5856825808: Creating DatabricksDBTConnection sess: None, name: list_owshq, idle: 0s, acqrelcnt: 0, lang: None, thrd: (77897, 11559530496), cmpt: ``, lut: None
[0m16:52:18.820012 [debug] [ThreadPool]: Acquiring new databricks connection 'list_owshq'
[0m16:52:18.820140 [debug] [ThreadPool]: Databricks adapter: Thread (77897, 11559530496) using default compute resource.
[0m16:52:18.820270 [debug] [ThreadPool]: Databricks adapter: conn: 5856825808: _acquire sess: None, name: list_owshq, idle: 0.0s, acqrelcnt: 1, lang: None, thrd: (77897, 11559530496), cmpt: ``, lut: 1710791538.820232
[0m16:52:18.820408 [debug] [ThreadPool]: Databricks adapter: conn: 5856825808: get_thread_connection: sess: None, name: list_owshq, idle: 0.00013899803161621094s, acqrelcnt: 1, lang: None, thrd: (77897, 11559530496), cmpt: ``, lut: 1710791538.820232
[0m16:52:18.820531 [debug] [ThreadPool]: Databricks adapter: conn: 5856825808: idle check connection: sess: None, name: list_owshq, idle: 0.00026226043701171875s, acqrelcnt: 1, lang: None, thrd: (77897, 11559530496), cmpt: ``, lut: 1710791538.820232
[0m16:52:18.820647 [debug] [ThreadPool]: Using databricks connection "list_owshq"
[0m16:52:18.820757 [debug] [ThreadPool]: On list_owshq: GetSchemas(database=owshq, schema=None)
[0m16:52:18.820867 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:52:19.409193 [debug] [ThreadPool]: Databricks adapter: conn: 5856825808: session opened sess: 01eee561-075f-1736-80b7-181c18dfd5e1, name: list_owshq, idle: 1.811981201171875e-05s, acqrelcnt: 1, lang: None, thrd: (77897, 11559530496), cmpt: ``, lut: 1710791539.408603
[0m16:52:19.980657 [debug] [ThreadPool]: SQL status: OK in 1.159999966621399 seconds
[0m16:52:19.989125 [debug] [ThreadPool]: Databricks adapter: conn: 5856825808: _release sess: 01eee561-075f-1736-80b7-181c18dfd5e1, name: list_owshq, idle: 4.0531158447265625e-06s, acqrelcnt: 0, lang: None, thrd: (77897, 11559530496), cmpt: ``, lut: 1710791539.989032
[0m16:52:19.989871 [debug] [ThreadPool]: Databricks adapter: conn: 5856825808: idle check connection: sess: 01eee561-075f-1736-80b7-181c18dfd5e1, name: list_owshq, idle: 0.0007779598236083984s, acqrelcnt: 0, lang: None, thrd: (77897, 11559530496), cmpt: ``, lut: 1710791539.989032
[0m16:52:19.990052 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_owshq, now create_owshq_default)
[0m16:52:19.990216 [debug] [ThreadPool]: Databricks adapter: conn: 5856825808: reusing connection list_owshq sess: 01eee561-075f-1736-80b7-181c18dfd5e1, name: create_owshq_default, idle: 0.001135110855102539s, acqrelcnt: 0, lang: None, thrd: (77897, 11559530496), cmpt: ``, lut: 1710791539.989032
[0m16:52:19.990367 [debug] [ThreadPool]: Databricks adapter: Thread (77897, 11559530496) using default compute resource.
[0m16:52:19.990520 [debug] [ThreadPool]: Databricks adapter: conn: 5856825808: _acquire sess: 01eee561-075f-1736-80b7-181c18dfd5e1, name: create_owshq_default, idle: 0.00144195556640625s, acqrelcnt: 1, lang: None, thrd: (77897, 11559530496), cmpt: ``, lut: 1710791539.989032
[0m16:52:19.990839 [debug] [ThreadPool]: Databricks adapter: conn: 5856825808: idle check connection: sess: 01eee561-075f-1736-80b7-181c18dfd5e1, name: create_owshq_default, idle: 0.0017600059509277344s, acqrelcnt: 1, lang: None, thrd: (77897, 11559530496), cmpt: ``, lut: 1710791539.989032
[0m16:52:19.990988 [debug] [ThreadPool]: Databricks adapter: Thread (77897, 11559530496) using default compute resource.
[0m16:52:19.991132 [debug] [ThreadPool]: Databricks adapter: conn: 5856825808: _acquire sess: 01eee561-075f-1736-80b7-181c18dfd5e1, name: create_owshq_default, idle: 0.0020551681518554688s, acqrelcnt: 2, lang: None, thrd: (77897, 11559530496), cmpt: ``, lut: 1710791539.989032
[0m16:52:19.991302 [debug] [ThreadPool]: Creating schema "database: "owshq"
schema: "default"
"
[0m16:52:19.995950 [debug] [ThreadPool]: Databricks adapter: conn: 5856825808: get_thread_connection: sess: 01eee561-075f-1736-80b7-181c18dfd5e1, name: create_owshq_default, idle: 0.006847858428955078s, acqrelcnt: 2, lang: None, thrd: (77897, 11559530496), cmpt: ``, lut: 1710791539.989032
[0m16:52:19.996160 [debug] [ThreadPool]: Databricks adapter: conn: 5856825808: idle check connection: sess: 01eee561-075f-1736-80b7-181c18dfd5e1, name: create_owshq_default, idle: 0.0070819854736328125s, acqrelcnt: 2, lang: None, thrd: (77897, 11559530496), cmpt: ``, lut: 1710791539.989032
[0m16:52:19.996314 [debug] [ThreadPool]: Databricks adapter: conn: 5856825808: get_thread_connection: sess: 01eee561-075f-1736-80b7-181c18dfd5e1, name: create_owshq_default, idle: 0.007239103317260742s, acqrelcnt: 2, lang: None, thrd: (77897, 11559530496), cmpt: ``, lut: 1710791539.989032
[0m16:52:19.996469 [debug] [ThreadPool]: Databricks adapter: conn: 5856825808: idle check connection: sess: 01eee561-075f-1736-80b7-181c18dfd5e1, name: create_owshq_default, idle: 0.007389068603515625s, acqrelcnt: 2, lang: None, thrd: (77897, 11559530496), cmpt: ``, lut: 1710791539.989032
[0m16:52:19.996611 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m16:52:19.996745 [debug] [ThreadPool]: Using databricks connection "create_owshq_default"
[0m16:52:19.996889 [debug] [ThreadPool]: On create_owshq_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "create_owshq_default"} */
create schema if not exists `owshq`.`default`
  
[0m16:52:20.357075 [debug] [ThreadPool]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "create_owshq_default"} */
create schema if not exists `owshq`.`default`
  
[0m16:52:20.357785 [debug] [ThreadPool]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Catalog 'owshq' plugin class not found: spark.sql.catalog.owshq is not defined
[0m16:52:20.358872 [debug] [ThreadPool]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.connector.catalog.CatalogNotFoundException: Catalog 'owshq' plugin class not found: spark.sql.catalog.owshq is not defined
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:663)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:540)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:389)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:353)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:401)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.connector.catalog.CatalogNotFoundException: Catalog 'owshq' plugin class not found: spark.sql.catalog.owshq is not defined
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundError(QueryExecutionErrors.scala:2249)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:58)
	at org.apache.spark.sql.connector.catalog.SecureCatalogManager.assertCatalogLoadable(SecureCatalogManager.scala:44)
	at org.apache.spark.sql.connector.catalog.SecureCatalogManager.assertCatalogLoadable$(SecureCatalogManager.scala:40)
	at org.apache.spark.sql.connector.catalog.CatalogManager.assertCatalogLoadable(CatalogManager.scala:40)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:57)
	at com.databricks.sql.DatabricksCatalogManager.catalog(DatabricksCatalogManager.scala:93)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:130)
	at com.databricks.sql.DatabricksCatalogManager.currentCatalog(DatabricksCatalogManager.scala:111)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:36)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:36)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:31)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:98)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:63)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:34)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:83)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:377)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1277)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1276)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:593)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:377)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:209)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:208)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:31)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:309)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:309)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:289)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$9(RuleExecutor.scala:382)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$9$adapted(RuleExecutor.scala:382)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:382)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:256)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:414)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:407)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:321)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:407)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:248)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:248)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:392)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:384)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:391)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:230)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:394)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:542)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1048)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:542)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:538)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1173)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:538)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:224)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:223)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:481)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:503)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:576)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:531)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:576)
	... 35 more

[0m16:52:20.359954 [debug] [ThreadPool]: Databricks adapter: operation-id: 01eee561-07d2-16c8-b9ee-e6580148adcf
[0m16:52:20.360367 [debug] [ThreadPool]: Databricks adapter: Error while running:
macro create_schema
[0m16:52:20.360651 [debug] [ThreadPool]: Databricks adapter: Runtime Error
  Catalog 'owshq' plugin class not found: spark.sql.catalog.owshq is not defined
[0m16:52:20.361032 [debug] [ThreadPool]: Databricks adapter: conn: 5856825808: _release sess: 01eee561-075f-1736-80b7-181c18dfd5e1, name: create_owshq_default, idle: 0.3719048500061035s, acqrelcnt: 1, lang: None, thrd: (77897, 11559530496), cmpt: ``, lut: 1710791539.989032
[0m16:52:20.361906 [debug] [ThreadPool]: Databricks adapter: conn: 5856825808: _release sess: 01eee561-075f-1736-80b7-181c18dfd5e1, name: create_owshq_default, idle: 1.6689300537109375e-06s, acqrelcnt: 0, lang: None, thrd: (77897, 11559530496), cmpt: ``, lut: 1710791540.361807
[0m16:52:20.362756 [debug] [MainThread]: Databricks adapter: conn: 5852585616: _release sess: None, name: master, idle: 2.1457672119140625e-06s, acqrelcnt: 0, lang: None, thrd: (77897, 7965269056), cmpt: ``, lut: 1710791540.3626509
[0m16:52:20.363289 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:52:20.363524 [debug] [MainThread]: Connection 'create_owshq_default' was properly closed.
[0m16:52:20.363753 [debug] [MainThread]: On create_owshq_default: ROLLBACK
[0m16:52:20.363980 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:52:20.364214 [debug] [MainThread]: On create_owshq_default: Close
[0m16:52:20.548743 [info ] [MainThread]: 
[0m16:52:20.550323 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 1.73 seconds (1.73s).
[0m16:52:20.551915 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Catalog 'owshq' plugin class not found: spark.sql.catalog.owshq is not defined
[0m16:52:20.561247 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 4.1183715, "process_user_time": 2.567602, "process_kernel_time": 3.437651, "process_mem_max_rss": "221134848", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:52:20.562059 [debug] [MainThread]: Command `cli run` failed at 16:52:20.561915 after 4.12 seconds
[0m16:52:20.562553 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10af93010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10afa06d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10afc1d10>]}
[0m16:52:20.562980 [debug] [MainThread]: Flushing usage events
[0m16:53:50.988850 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104d5dd50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104dc1b90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104dc21d0>]}


============================== 16:53:50.991925 | ff9d0b3c-1bda-4869-a8dd-694b3fca49c9 ==============================
[0m16:53:50.991925 [info ] [MainThread]: Running with dbt=1.7.8
[0m16:53:50.992220 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt ', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:53:52.639950 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ff9d0b3c-1bda-4869-a8dd-694b3fca49c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1040f1050>]}
[0m16:53:52.668427 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ff9d0b3c-1bda-4869-a8dd-694b3fca49c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1040f1050>]}
[0m16:53:52.668683 [info ] [MainThread]: Registered adapter: databricks=1.7.9
[0m16:53:52.687103 [debug] [MainThread]: checksum: 67f0013ca5f0bd43af9a0873dd50792fde83ef69de63b71cacd0b4ac656c52e5, vars: {}, profile: , target: , version: 1.7.8
[0m16:53:52.766569 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:53:52.766843 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:53:52.769487 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ff9d0b3c-1bda-4869-a8dd-694b3fca49c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x128c44390>]}
[0m16:53:52.786378 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ff9d0b3c-1bda-4869-a8dd-694b3fca49c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x128b12ed0>]}
[0m16:53:52.786614 [info ] [MainThread]: Found 3 models, 3 sources, 0 exposures, 0 metrics, 538 macros, 0 groups, 0 semantic models
[0m16:53:52.786795 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ff9d0b3c-1bda-4869-a8dd-694b3fca49c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1288f6890>]}
[0m16:53:52.787474 [info ] [MainThread]: 
[0m16:53:52.787924 [debug] [MainThread]: Databricks adapter: conn: 4977665104: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (78393, 7965269056), cmpt: ``, lut: None
[0m16:53:52.788081 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:53:52.788196 [debug] [MainThread]: Databricks adapter: Thread (78393, 7965269056) using default compute resource.
[0m16:53:52.788314 [debug] [MainThread]: Databricks adapter: conn: 4977665104: _acquire sess: None, name: master, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (78393, 7965269056), cmpt: ``, lut: 1710791632.788276
[0m16:53:52.788856 [debug] [ThreadPool]: Databricks adapter: conn: 4978234384: Creating DatabricksDBTConnection sess: None, name: list_owshq, idle: 0s, acqrelcnt: 0, lang: None, thrd: (78393, 10924863488), cmpt: ``, lut: None
[0m16:53:52.789062 [debug] [ThreadPool]: Acquiring new databricks connection 'list_owshq'
[0m16:53:52.789189 [debug] [ThreadPool]: Databricks adapter: Thread (78393, 10924863488) using default compute resource.
[0m16:53:52.789308 [debug] [ThreadPool]: Databricks adapter: conn: 4978234384: _acquire sess: None, name: list_owshq, idle: 1.1920928955078125e-06s, acqrelcnt: 1, lang: None, thrd: (78393, 10924863488), cmpt: ``, lut: 1710791632.789273
[0m16:53:52.789439 [debug] [ThreadPool]: Databricks adapter: conn: 4978234384: get_thread_connection: sess: None, name: list_owshq, idle: 0.00012993812561035156s, acqrelcnt: 1, lang: None, thrd: (78393, 10924863488), cmpt: ``, lut: 1710791632.789273
[0m16:53:52.789563 [debug] [ThreadPool]: Databricks adapter: conn: 4978234384: idle check connection: sess: None, name: list_owshq, idle: 0.0002510547637939453s, acqrelcnt: 1, lang: None, thrd: (78393, 10924863488), cmpt: ``, lut: 1710791632.789273
[0m16:53:52.789669 [debug] [ThreadPool]: Using databricks connection "list_owshq"
[0m16:53:52.789778 [debug] [ThreadPool]: On list_owshq: GetSchemas(database=owshq, schema=None)
[0m16:53:52.789884 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:53:53.336299 [debug] [ThreadPool]: Databricks adapter: conn: 4978234384: session opened sess: 01eee561-3f5a-193d-93bd-e75a7c2617c1, name: list_owshq, idle: 1.4781951904296875e-05s, acqrelcnt: 1, lang: None, thrd: (78393, 10924863488), cmpt: ``, lut: 1710791633.335896
[0m16:53:53.618645 [debug] [ThreadPool]: SQL status: OK in 0.8299999833106995 seconds
[0m16:53:53.628790 [debug] [ThreadPool]: Databricks adapter: conn: 4978234384: _release sess: 01eee561-3f5a-193d-93bd-e75a7c2617c1, name: list_owshq, idle: 5.9604644775390625e-06s, acqrelcnt: 0, lang: None, thrd: (78393, 10924863488), cmpt: ``, lut: 1710791633.628663
[0m16:53:53.629900 [debug] [ThreadPool]: Databricks adapter: conn: 4978234384: idle check connection: sess: 01eee561-3f5a-193d-93bd-e75a7c2617c1, name: list_owshq, idle: 0.001155853271484375s, acqrelcnt: 0, lang: None, thrd: (78393, 10924863488), cmpt: ``, lut: 1710791633.628663
[0m16:53:53.630160 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_owshq, now create_owshq_default)
[0m16:53:53.630394 [debug] [ThreadPool]: Databricks adapter: conn: 4978234384: reusing connection list_owshq sess: 01eee561-3f5a-193d-93bd-e75a7c2617c1, name: create_owshq_default, idle: 0.0016598701477050781s, acqrelcnt: 0, lang: None, thrd: (78393, 10924863488), cmpt: ``, lut: 1710791633.628663
[0m16:53:53.630607 [debug] [ThreadPool]: Databricks adapter: Thread (78393, 10924863488) using default compute resource.
[0m16:53:53.630822 [debug] [ThreadPool]: Databricks adapter: conn: 4978234384: _acquire sess: 01eee561-3f5a-193d-93bd-e75a7c2617c1, name: create_owshq_default, idle: 0.002092123031616211s, acqrelcnt: 1, lang: None, thrd: (78393, 10924863488), cmpt: ``, lut: 1710791633.628663
[0m16:53:53.631257 [debug] [ThreadPool]: Databricks adapter: conn: 4978234384: idle check connection: sess: 01eee561-3f5a-193d-93bd-e75a7c2617c1, name: create_owshq_default, idle: 0.0025310516357421875s, acqrelcnt: 1, lang: None, thrd: (78393, 10924863488), cmpt: ``, lut: 1710791633.628663
[0m16:53:53.631452 [debug] [ThreadPool]: Databricks adapter: Thread (78393, 10924863488) using default compute resource.
[0m16:53:53.631638 [debug] [ThreadPool]: Databricks adapter: conn: 4978234384: _acquire sess: 01eee561-3f5a-193d-93bd-e75a7c2617c1, name: create_owshq_default, idle: 0.0029168128967285156s, acqrelcnt: 2, lang: None, thrd: (78393, 10924863488), cmpt: ``, lut: 1710791633.628663
[0m16:53:53.631860 [debug] [ThreadPool]: Creating schema "database: "owshq"
schema: "default"
"
[0m16:53:53.637824 [debug] [ThreadPool]: Databricks adapter: conn: 4978234384: get_thread_connection: sess: 01eee561-3f5a-193d-93bd-e75a7c2617c1, name: create_owshq_default, idle: 0.009088993072509766s, acqrelcnt: 2, lang: None, thrd: (78393, 10924863488), cmpt: ``, lut: 1710791633.628663
[0m16:53:53.638035 [debug] [ThreadPool]: Databricks adapter: conn: 4978234384: idle check connection: sess: 01eee561-3f5a-193d-93bd-e75a7c2617c1, name: create_owshq_default, idle: 0.009315729141235352s, acqrelcnt: 2, lang: None, thrd: (78393, 10924863488), cmpt: ``, lut: 1710791633.628663
[0m16:53:53.638232 [debug] [ThreadPool]: Databricks adapter: conn: 4978234384: get_thread_connection: sess: 01eee561-3f5a-193d-93bd-e75a7c2617c1, name: create_owshq_default, idle: 0.009516000747680664s, acqrelcnt: 2, lang: None, thrd: (78393, 10924863488), cmpt: ``, lut: 1710791633.628663
[0m16:53:53.638405 [debug] [ThreadPool]: Databricks adapter: conn: 4978234384: idle check connection: sess: 01eee561-3f5a-193d-93bd-e75a7c2617c1, name: create_owshq_default, idle: 0.009691953659057617s, acqrelcnt: 2, lang: None, thrd: (78393, 10924863488), cmpt: ``, lut: 1710791633.628663
[0m16:53:53.638577 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m16:53:53.638730 [debug] [ThreadPool]: Using databricks connection "create_owshq_default"
[0m16:53:53.638909 [debug] [ThreadPool]: On create_owshq_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "create_owshq_default"} */
create schema if not exists `owshq`.`default`
  
[0m16:53:54.063703 [debug] [ThreadPool]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "create_owshq_default"} */
create schema if not exists `owshq`.`default`
  
[0m16:53:54.064052 [debug] [ThreadPool]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Catalog 'owshq' plugin class not found: spark.sql.catalog.owshq is not defined
[0m16:53:54.064723 [debug] [ThreadPool]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.connector.catalog.CatalogNotFoundException: Catalog 'owshq' plugin class not found: spark.sql.catalog.owshq is not defined
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:663)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:124)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:540)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:389)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:183)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:178)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:64)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:367)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:353)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:401)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.connector.catalog.CatalogNotFoundException: Catalog 'owshq' plugin class not found: spark.sql.catalog.owshq is not defined
	at org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundError(QueryExecutionErrors.scala:2249)
	at org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:58)
	at org.apache.spark.sql.connector.catalog.SecureCatalogManager.assertCatalogLoadable(SecureCatalogManager.scala:44)
	at org.apache.spark.sql.connector.catalog.SecureCatalogManager.assertCatalogLoadable$(SecureCatalogManager.scala:40)
	at org.apache.spark.sql.connector.catalog.CatalogManager.assertCatalogLoadable(CatalogManager.scala:40)
	at org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:57)
	at com.databricks.sql.DatabricksCatalogManager.catalog(DatabricksCatalogManager.scala:93)
	at org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:130)
	at com.databricks.sql.DatabricksCatalogManager.currentCatalog(DatabricksCatalogManager.scala:111)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog(LookupCatalog.scala:36)
	at org.apache.spark.sql.connector.catalog.LookupCatalog.currentCatalog$(LookupCatalog.scala:36)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.currentCatalog(ResolveCatalogs.scala:31)
	at org.apache.spark.sql.connector.catalog.LookupCatalog$CatalogAndNamespace$.unapply(LookupCatalog.scala:98)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:63)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs$$anonfun$apply$1.applyOrElse(ResolveCatalogs.scala:34)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:83)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:219)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:377)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1277)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1276)
	at org.apache.spark.sql.catalyst.plans.logical.CreateNamespace.mapChildren(v2Commands.scala:593)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:224)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:377)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:217)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:209)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:208)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:34)
	at org.apache.spark.sql.catalyst.analysis.ResolveCatalogs.apply(ResolveCatalogs.scala:31)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$4(RuleExecutor.scala:309)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:309)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:306)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:289)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$9(RuleExecutor.scala:382)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$9$adapted(RuleExecutor.scala:382)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:382)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:256)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:414)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:407)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:321)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:407)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:340)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:248)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:248)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:392)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:384)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:391)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:230)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:394)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:542)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1048)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:542)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:538)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1173)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:538)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:224)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:223)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:481)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:503)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:576)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:531)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:576)
	... 35 more

[0m16:53:54.065403 [debug] [ThreadPool]: Databricks adapter: operation-id: 01eee561-3fab-1e7d-a86a-69c224ea709c
[0m16:53:54.065649 [debug] [ThreadPool]: Databricks adapter: Error while running:
macro create_schema
[0m16:53:54.065841 [debug] [ThreadPool]: Databricks adapter: Runtime Error
  Catalog 'owshq' plugin class not found: spark.sql.catalog.owshq is not defined
[0m16:53:54.066091 [debug] [ThreadPool]: Databricks adapter: conn: 4978234384: _release sess: 01eee561-3f5a-193d-93bd-e75a7c2617c1, name: create_owshq_default, idle: 0.4373629093170166s, acqrelcnt: 1, lang: None, thrd: (78393, 10924863488), cmpt: ``, lut: 1710791633.628663
[0m16:53:54.066616 [debug] [ThreadPool]: Databricks adapter: conn: 4978234384: _release sess: 01eee561-3f5a-193d-93bd-e75a7c2617c1, name: create_owshq_default, idle: 1.1920928955078125e-06s, acqrelcnt: 0, lang: None, thrd: (78393, 10924863488), cmpt: ``, lut: 1710791634.0665479
[0m16:53:54.067191 [debug] [MainThread]: Databricks adapter: conn: 4977665104: _release sess: None, name: master, idle: 1.9073486328125e-06s, acqrelcnt: 0, lang: None, thrd: (78393, 7965269056), cmpt: ``, lut: 1710791634.067084
[0m16:53:54.067686 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:53:54.067889 [debug] [MainThread]: Connection 'create_owshq_default' was properly closed.
[0m16:53:54.068072 [debug] [MainThread]: On create_owshq_default: ROLLBACK
[0m16:53:54.068255 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:53:54.068426 [debug] [MainThread]: On create_owshq_default: Close
[0m16:53:54.231058 [info ] [MainThread]: 
[0m16:53:54.231493 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 1.44 seconds (1.44s).
[0m16:53:54.231906 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    Catalog 'owshq' plugin class not found: spark.sql.catalog.owshq is not defined
[0m16:53:54.244551 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 3.2872279, "process_user_time": 2.040868, "process_kernel_time": 3.50728, "process_mem_max_rss": "218054656", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:53:54.244986 [debug] [MainThread]: Command `cli run` failed at 16:53:54.244894 after 3.29 seconds
[0m16:53:54.245273 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104dc1e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104dc20d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x128c44190>]}
[0m16:53:54.245524 [debug] [MainThread]: Flushing usage events
[0m16:55:56.217141 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b6420d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10abe7c90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b69d290>]}


============================== 16:55:56.220072 | ec561247-ce36-4ff4-92d1-f663f969a986 ==============================
[0m16:55:56.220072 [info ] [MainThread]: Running with dbt=1.7.8
[0m16:55:56.220366 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt ', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:55:57.699130 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ec561247-ce36-4ff4-92d1-f663f969a986', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1389ad490>]}
[0m16:55:57.727811 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ec561247-ce36-4ff4-92d1-f663f969a986', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1388e4b90>]}
[0m16:55:57.728099 [info ] [MainThread]: Registered adapter: databricks=1.7.9
[0m16:55:57.747703 [debug] [MainThread]: checksum: 67f0013ca5f0bd43af9a0873dd50792fde83ef69de63b71cacd0b4ac656c52e5, vars: {}, profile: , target: , version: 1.7.8
[0m16:55:57.761808 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m16:55:57.762090 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'ec561247-ce36-4ff4-92d1-f663f969a986', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x138bc6e90>]}
[0m16:55:58.338183 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ec561247-ce36-4ff4-92d1-f663f969a986', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x138e4ee50>]}
[0m16:55:58.352394 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ec561247-ce36-4ff4-92d1-f663f969a986', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x138f8b5d0>]}
[0m16:55:58.352662 [info ] [MainThread]: Found 3 models, 3 sources, 0 exposures, 0 metrics, 538 macros, 0 groups, 0 semantic models
[0m16:55:58.352830 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ec561247-ce36-4ff4-92d1-f663f969a986', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x138ac9c90>]}
[0m16:55:58.353534 [info ] [MainThread]: 
[0m16:55:58.354006 [debug] [MainThread]: Databricks adapter: conn: 5245503184: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (79046, 7965269056), cmpt: ``, lut: None
[0m16:55:58.354140 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:55:58.354258 [debug] [MainThread]: Databricks adapter: Thread (79046, 7965269056) using default compute resource.
[0m16:55:58.354386 [debug] [MainThread]: Databricks adapter: conn: 5245503184: _acquire sess: None, name: master, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (79046, 7965269056), cmpt: ``, lut: 1710791758.354343
[0m16:55:58.354888 [debug] [ThreadPool]: Databricks adapter: conn: 5250787728: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore, idle: 0s, acqrelcnt: 0, lang: None, thrd: (79046, 11559530496), cmpt: ``, lut: None
[0m16:55:58.355073 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m16:55:58.355193 [debug] [ThreadPool]: Databricks adapter: Thread (79046, 11559530496) using default compute resource.
[0m16:55:58.355314 [debug] [ThreadPool]: Databricks adapter: conn: 5250787728: _acquire sess: None, name: list_hive_metastore, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (79046, 11559530496), cmpt: ``, lut: 1710791758.355274
[0m16:55:58.355449 [debug] [ThreadPool]: Databricks adapter: conn: 5250787728: get_thread_connection: sess: None, name: list_hive_metastore, idle: 0.00013899803161621094s, acqrelcnt: 1, lang: None, thrd: (79046, 11559530496), cmpt: ``, lut: 1710791758.355274
[0m16:55:58.355567 [debug] [ThreadPool]: Databricks adapter: conn: 5250787728: idle check connection: sess: None, name: list_hive_metastore, idle: 0.0002570152282714844s, acqrelcnt: 1, lang: None, thrd: (79046, 11559530496), cmpt: ``, lut: 1710791758.355274
[0m16:55:58.355674 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m16:55:58.355783 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m16:55:58.355886 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:55:58.881346 [debug] [ThreadPool]: Databricks adapter: conn: 5250787728: session opened sess: 01eee561-8a2e-150a-b6de-4d6922283336, name: list_hive_metastore, idle: 8.106231689453125e-06s, acqrelcnt: 1, lang: None, thrd: (79046, 11559530496), cmpt: ``, lut: 1710791758.881114
[0m16:56:02.704239 [debug] [ThreadPool]: SQL status: OK in 4.349999904632568 seconds
[0m16:56:02.715159 [debug] [ThreadPool]: Databricks adapter: conn: 5250787728: _release sess: 01eee561-8a2e-150a-b6de-4d6922283336, name: list_hive_metastore, idle: 5.9604644775390625e-06s, acqrelcnt: 0, lang: None, thrd: (79046, 11559530496), cmpt: ``, lut: 1710791762.71504
[0m16:56:02.716658 [debug] [ThreadPool]: Databricks adapter: conn: 5250787728: idle check connection: sess: 01eee561-8a2e-150a-b6de-4d6922283336, name: list_hive_metastore, idle: 0.0015091896057128906s, acqrelcnt: 0, lang: None, thrd: (79046, 11559530496), cmpt: ``, lut: 1710791762.71504
[0m16:56:02.716988 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now list_hive_metastore_default)
[0m16:56:02.717250 [debug] [ThreadPool]: Databricks adapter: conn: 5250787728: reusing connection list_hive_metastore sess: 01eee561-8a2e-150a-b6de-4d6922283336, name: list_hive_metastore_default, idle: 0.0021359920501708984s, acqrelcnt: 0, lang: None, thrd: (79046, 11559530496), cmpt: ``, lut: 1710791762.71504
[0m16:56:02.717467 [debug] [ThreadPool]: Databricks adapter: Thread (79046, 11559530496) using default compute resource.
[0m16:56:02.717659 [debug] [ThreadPool]: Databricks adapter: conn: 5250787728: _acquire sess: 01eee561-8a2e-150a-b6de-4d6922283336, name: list_hive_metastore_default, idle: 0.0025610923767089844s, acqrelcnt: 1, lang: None, thrd: (79046, 11559530496), cmpt: ``, lut: 1710791762.71504
[0m16:56:02.721009 [debug] [ThreadPool]: Databricks adapter: conn: 5250787728: get_thread_connection: sess: 01eee561-8a2e-150a-b6de-4d6922283336, name: list_hive_metastore_default, idle: 0.005892038345336914s, acqrelcnt: 1, lang: None, thrd: (79046, 11559530496), cmpt: ``, lut: 1710791762.71504
[0m16:56:02.721263 [debug] [ThreadPool]: Databricks adapter: conn: 5250787728: idle check connection: sess: 01eee561-8a2e-150a-b6de-4d6922283336, name: list_hive_metastore_default, idle: 0.006140947341918945s, acqrelcnt: 1, lang: None, thrd: (79046, 11559530496), cmpt: ``, lut: 1710791762.71504
[0m16:56:02.721454 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m16:56:02.721633 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m16:56:03.180307 [debug] [ThreadPool]: SQL status: OK in 0.46000000834465027 seconds
[0m16:56:03.193338 [debug] [ThreadPool]: Databricks adapter: conn: 5250787728: get_thread_connection: sess: 01eee561-8a2e-150a-b6de-4d6922283336, name: list_hive_metastore_default, idle: 0.4781620502471924s, acqrelcnt: 1, lang: None, thrd: (79046, 11559530496), cmpt: ``, lut: 1710791762.71504
[0m16:56:03.193713 [debug] [ThreadPool]: Databricks adapter: conn: 5250787728: idle check connection: sess: 01eee561-8a2e-150a-b6de-4d6922283336, name: list_hive_metastore_default, idle: 0.4786031246185303s, acqrelcnt: 1, lang: None, thrd: (79046, 11559530496), cmpt: ``, lut: 1710791762.71504
[0m16:56:03.193957 [debug] [ThreadPool]: Databricks adapter: conn: 5250787728: get_thread_connection: sess: 01eee561-8a2e-150a-b6de-4d6922283336, name: list_hive_metastore_default, idle: 0.4788479804992676s, acqrelcnt: 1, lang: None, thrd: (79046, 11559530496), cmpt: ``, lut: 1710791762.71504
[0m16:56:03.194175 [debug] [ThreadPool]: Databricks adapter: conn: 5250787728: idle check connection: sess: 01eee561-8a2e-150a-b6de-4d6922283336, name: list_hive_metastore_default, idle: 0.47907209396362305s, acqrelcnt: 1, lang: None, thrd: (79046, 11559530496), cmpt: ``, lut: 1710791762.71504
[0m16:56:03.194378 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m16:56:03.194556 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m16:56:03.194771 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m16:56:04.306595 [debug] [ThreadPool]: SQL status: OK in 1.1100000143051147 seconds
[0m16:56:04.314210 [debug] [ThreadPool]: Databricks adapter: conn: 5250787728: get_thread_connection: sess: 01eee561-8a2e-150a-b6de-4d6922283336, name: list_hive_metastore_default, idle: 1.5990331172943115s, acqrelcnt: 1, lang: None, thrd: (79046, 11559530496), cmpt: ``, lut: 1710791762.71504
[0m16:56:04.314607 [debug] [ThreadPool]: Databricks adapter: conn: 5250787728: idle check connection: sess: 01eee561-8a2e-150a-b6de-4d6922283336, name: list_hive_metastore_default, idle: 1.5994820594787598s, acqrelcnt: 1, lang: None, thrd: (79046, 11559530496), cmpt: ``, lut: 1710791762.71504
[0m16:56:04.314867 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m16:56:04.315134 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m16:56:05.220618 [debug] [ThreadPool]: SQL status: OK in 0.8999999761581421 seconds
[0m16:56:05.228298 [debug] [ThreadPool]: Databricks adapter: conn: 5250787728: _release sess: 01eee561-8a2e-150a-b6de-4d6922283336, name: list_hive_metastore_default, idle: 7.867813110351562e-06s, acqrelcnt: 0, lang: None, thrd: (79046, 11559530496), cmpt: ``, lut: 1710791765.228086
[0m16:56:05.232280 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ec561247-ce36-4ff4-92d1-f663f969a986', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x138b35810>]}
[0m16:56:05.232881 [debug] [MainThread]: Databricks adapter: conn: 5245503184: get_thread_connection: sess: None, name: master, idle: 6.878436088562012s, acqrelcnt: 1, lang: None, thrd: (79046, 7965269056), cmpt: ``, lut: 1710791758.354343
[0m16:56:05.233196 [debug] [MainThread]: Databricks adapter: conn: 5245503184: idle check connection: sess: None, name: master, idle: 6.878775119781494s, acqrelcnt: 1, lang: None, thrd: (79046, 7965269056), cmpt: ``, lut: 1710791758.354343
[0m16:56:05.233467 [debug] [MainThread]: Databricks adapter: conn: 5245503184: get_thread_connection: sess: None, name: master, idle: 6.879046201705933s, acqrelcnt: 1, lang: None, thrd: (79046, 7965269056), cmpt: ``, lut: 1710791758.354343
[0m16:56:05.233719 [debug] [MainThread]: Databricks adapter: conn: 5245503184: idle check connection: sess: None, name: master, idle: 6.879303216934204s, acqrelcnt: 1, lang: None, thrd: (79046, 7965269056), cmpt: ``, lut: 1710791758.354343
[0m16:56:05.233957 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:56:05.234190 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:56:05.234442 [debug] [MainThread]: Databricks adapter: conn: 5245503184: _release sess: None, name: master, idle: 2.1457672119140625e-06s, acqrelcnt: 0, lang: None, thrd: (79046, 7965269056), cmpt: ``, lut: 1710791765.2343628
[0m16:56:05.235060 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:56:05.235353 [info ] [MainThread]: 
[0m16:56:05.239917 [debug] [Thread-1 (]: Began running node model.default.stage_users
[0m16:56:05.240427 [info ] [Thread-1 (]: 1 of 1 START sql view model default.stage_users ................................ [RUN]
[0m16:56:05.241059 [debug] [Thread-1 (]: Databricks adapter: conn: 5250787728: idle check connection: sess: 01eee561-8a2e-150a-b6de-4d6922283336, name: list_hive_metastore_default, idle: 0.012856006622314453s, acqrelcnt: 0, lang: None, thrd: (79046, 11559530496), cmpt: ``, lut: 1710791765.228086
[0m16:56:05.241307 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.default.stage_users)
[0m16:56:05.241574 [debug] [Thread-1 (]: Databricks adapter: conn: 5250787728: reusing connection list_hive_metastore_default sess: 01eee561-8a2e-150a-b6de-4d6922283336, name: model.default.stage_users, idle: 0.013383865356445312s, acqrelcnt: 0, lang: None, thrd: (79046, 11559530496), cmpt: ``, lut: 1710791765.228086
[0m16:56:05.241834 [debug] [Thread-1 (]: Databricks adapter: On thread (79046, 11559530496): `hive_metastore`.`default`.`stage_users` using default compute resource.
[0m16:56:05.242069 [debug] [Thread-1 (]: Databricks adapter: conn: 5250787728: _acquire sess: 01eee561-8a2e-150a-b6de-4d6922283336, name: model.default.stage_users, idle: 0.01389002799987793s, acqrelcnt: 1, lang: sql, thrd: (79046, 11559530496), cmpt: ``, lut: 1710791765.228086
[0m16:56:05.242309 [debug] [Thread-1 (]: Began compiling node model.default.stage_users
[0m16:56:05.248002 [debug] [Thread-1 (]: Writing injected SQL for node "model.default.stage_users"
[0m16:56:05.251060 [debug] [Thread-1 (]: Timing info for model.default.stage_users (compile): 16:56:05.242493 => 16:56:05.250846
[0m16:56:05.251365 [debug] [Thread-1 (]: Began executing node model.default.stage_users
[0m16:56:05.268204 [debug] [Thread-1 (]: Writing runtime sql for node "model.default.stage_users"
[0m16:56:05.270027 [debug] [Thread-1 (]: Databricks adapter: conn: 5250787728: get_thread_connection: sess: 01eee561-8a2e-150a-b6de-4d6922283336, name: model.default.stage_users, idle: 0.041738033294677734s, acqrelcnt: 1, lang: sql, thrd: (79046, 11559530496), cmpt: ``, lut: 1710791765.228086
[0m16:56:05.270368 [debug] [Thread-1 (]: Databricks adapter: conn: 5250787728: idle check connection: sess: 01eee561-8a2e-150a-b6de-4d6922283336, name: model.default.stage_users, idle: 0.0421910285949707s, acqrelcnt: 1, lang: sql, thrd: (79046, 11559530496), cmpt: ``, lut: 1710791765.228086
[0m16:56:05.270553 [debug] [Thread-1 (]: Using databricks connection "model.default.stage_users"
[0m16:56:05.270777 [debug] [Thread-1 (]: On model.default.stage_users: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_users"} */
create or replace view `hive_metastore`.`default`.`stage_users`
  
  
  
  as
    

select user_id AS user_id,
       name AS name,
       city AS city,
       phone_number AS phone_number,
       gender AS gender,
       nationality AS nationality,
       state AS state
from `pythiandbsql`.`raw`.`users`

[0m16:56:06.466485 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_users"} */
create or replace view `hive_metastore`.`default`.`stage_users`
  
  
  
  as
    

select user_id AS user_id,
       name AS name,
       city AS city,
       phone_number AS phone_number,
       gender AS gender,
       nationality AS nationality,
       state AS state
from `pythiandbsql`.`raw`.`users`

[0m16:56:06.466859 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `pythiandbsql`.`raw`.`users` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 16 pos 5
[0m16:56:06.467287 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `pythiandbsql`.`raw`.`users` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 16 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:697)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:574)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:423)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:65)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:65)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:401)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:386)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:435)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `pythiandbsql`.`raw`.`users` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 16 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:81)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:674)
	... 36 more

[0m16:56:06.467690 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01eee561-8e19-15ab-a1bb-8bf102e003d2
[0m16:56:06.467982 [debug] [Thread-1 (]: Timing info for model.default.stage_users (execute): 16:56:05.251521 => 16:56:06.467868
[0m16:56:06.468206 [debug] [Thread-1 (]: Databricks adapter: conn: 5250787728: _release sess: 01eee561-8a2e-150a-b6de-4d6922283336, name: model.default.stage_users, idle: 1.1920928955078125e-06s, acqrelcnt: 0, lang: sql, thrd: (79046, 11559530496), cmpt: ``, lut: 1710791766.468118
[0m16:56:06.478167 [debug] [Thread-1 (]: Runtime Error in model stage_users (models/stage/stage_users.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `pythiandbsql`.`raw`.`users` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 16 pos 5
[0m16:56:06.478474 [debug] [Thread-1 (]: Databricks adapter: conn: 5250787728: _release sess: 01eee561-8a2e-150a-b6de-4d6922283336, name: model.default.stage_users, idle: 9.5367431640625e-07s, acqrelcnt: 0, lang: sql, thrd: (79046, 11559530496), cmpt: ``, lut: 1710791766.4783812
[0m16:56:06.478726 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ec561247-ce36-4ff4-92d1-f663f969a986', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x138f02290>]}
[0m16:56:06.479057 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model default.stage_users ....................... [[31mERROR[0m in 1.24s]
[0m16:56:06.479374 [debug] [Thread-1 (]: Finished running node model.default.stage_users
[0m16:56:06.480226 [debug] [MainThread]: Databricks adapter: conn: 5245503184: idle check connection: sess: None, name: master, idle: 1.2457993030548096s, acqrelcnt: 0, lang: None, thrd: (79046, 7965269056), cmpt: ``, lut: 1710791765.2343628
[0m16:56:06.480428 [debug] [MainThread]: Databricks adapter: conn: 5245503184: reusing connection master sess: None, name: master, idle: 1.2460153102874756s, acqrelcnt: 0, lang: None, thrd: (79046, 7965269056), cmpt: ``, lut: 1710791765.2343628
[0m16:56:06.480584 [debug] [MainThread]: Databricks adapter: Thread (79046, 7965269056) using default compute resource.
[0m16:56:06.480739 [debug] [MainThread]: Databricks adapter: conn: 5245503184: _acquire sess: None, name: master, idle: 1.2463290691375732s, acqrelcnt: 1, lang: None, thrd: (79046, 7965269056), cmpt: ``, lut: 1710791765.2343628
[0m16:56:06.480907 [debug] [MainThread]: Databricks adapter: conn: 5245503184: get_thread_connection: sess: None, name: master, idle: 1.246495246887207s, acqrelcnt: 1, lang: None, thrd: (79046, 7965269056), cmpt: ``, lut: 1710791765.2343628
[0m16:56:06.481062 [debug] [MainThread]: Databricks adapter: conn: 5245503184: idle check connection: sess: None, name: master, idle: 1.2466552257537842s, acqrelcnt: 1, lang: None, thrd: (79046, 7965269056), cmpt: ``, lut: 1710791765.2343628
[0m16:56:06.481207 [debug] [MainThread]: On master: ROLLBACK
[0m16:56:06.481349 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:56:06.943803 [debug] [MainThread]: Databricks adapter: conn: 5245503184: session opened sess: 01eee561-8efe-13bf-b70e-a47df2f05849, name: master, idle: 1.4066696166992188e-05s, acqrelcnt: 1, lang: None, thrd: (79046, 7965269056), cmpt: ``, lut: 1710791766.943404
[0m16:56:06.944945 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:56:06.945652 [debug] [MainThread]: Databricks adapter: conn: 5245503184: get_thread_connection: sess: 01eee561-8efe-13bf-b70e-a47df2f05849, name: master, idle: 0.0020949840545654297s, acqrelcnt: 1, lang: None, thrd: (79046, 7965269056), cmpt: ``, lut: 1710791766.943404
[0m16:56:06.946231 [debug] [MainThread]: Databricks adapter: conn: 5245503184: idle check connection: sess: 01eee561-8efe-13bf-b70e-a47df2f05849, name: master, idle: 0.0027010440826416016s, acqrelcnt: 1, lang: None, thrd: (79046, 7965269056), cmpt: ``, lut: 1710791766.943404
[0m16:56:06.946741 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:56:06.947186 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:56:06.948012 [debug] [MainThread]: Databricks adapter: conn: 5245503184: _release sess: 01eee561-8efe-13bf-b70e-a47df2f05849, name: master, idle: 2.86102294921875e-06s, acqrelcnt: 0, lang: None, thrd: (79046, 7965269056), cmpt: ``, lut: 1710791766.947576
[0m16:56:06.949476 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:56:06.950537 [debug] [MainThread]: On master: ROLLBACK
[0m16:56:06.951101 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:56:06.951450 [debug] [MainThread]: On master: Close
[0m16:56:07.124215 [debug] [MainThread]: Connection 'model.default.stage_users' was properly closed.
[0m16:56:07.125210 [debug] [MainThread]: On model.default.stage_users: ROLLBACK
[0m16:56:07.126133 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:56:07.126705 [debug] [MainThread]: On model.default.stage_users: Close
[0m16:56:07.304956 [info ] [MainThread]: 
[0m16:56:07.305777 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 8.95 seconds (8.95s).
[0m16:56:07.307157 [debug] [MainThread]: Command end result
[0m16:56:07.336871 [info ] [MainThread]: 
[0m16:56:07.337324 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m16:56:07.337588 [info ] [MainThread]: 
[0m16:56:07.337862 [error] [MainThread]:   Runtime Error in model stage_users (models/stage/stage_users.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `pythiandbsql`.`raw`.`users` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 16 pos 5
[0m16:56:07.338132 [info ] [MainThread]: 
[0m16:56:07.338397 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m16:56:07.343879 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 11.156106, "process_user_time": 2.683926, "process_kernel_time": 3.522401, "process_mem_max_rss": "228524032", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:56:07.344429 [debug] [MainThread]: Command `cli run` failed at 16:56:07.344315 after 11.16 seconds
[0m16:56:07.344798 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b69cdd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b69dd90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10293bcd0>]}
[0m16:56:07.345115 [debug] [MainThread]: Flushing usage events
[0m16:57:14.682855 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121fcfc90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121fcfb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121ffe3d0>]}


============================== 16:57:14.686264 | 474721df-75d1-45e2-b0a5-56ac3419f321 ==============================
[0m16:57:14.686264 [info ] [MainThread]: Running with dbt=1.7.8
[0m16:57:14.686570 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:57:16.329047 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '474721df-75d1-45e2-b0a5-56ac3419f321', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122012c90>]}
[0m16:57:16.357320 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '474721df-75d1-45e2-b0a5-56ac3419f321', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122012c90>]}
[0m16:57:16.357564 [info ] [MainThread]: Registered adapter: databricks=1.7.9
[0m16:57:16.375181 [debug] [MainThread]: checksum: 67f0013ca5f0bd43af9a0873dd50792fde83ef69de63b71cacd0b4ac656c52e5, vars: {}, profile: , target: , version: 1.7.8
[0m16:57:16.459627 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:57:16.460048 [debug] [MainThread]: Partial parsing: updated file: default://models/sources.yml
[0m16:57:16.539844 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '474721df-75d1-45e2-b0a5-56ac3419f321', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1762d6c10>]}
[0m16:57:16.550982 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '474721df-75d1-45e2-b0a5-56ac3419f321', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16dcd9750>]}
[0m16:57:16.551217 [info ] [MainThread]: Found 3 models, 3 sources, 0 exposures, 0 metrics, 538 macros, 0 groups, 0 semantic models
[0m16:57:16.551392 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '474721df-75d1-45e2-b0a5-56ac3419f321', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16ddfa490>]}
[0m16:57:16.552068 [info ] [MainThread]: 
[0m16:57:16.552503 [debug] [MainThread]: Databricks adapter: conn: 6138340496: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (79472, 7965269056), cmpt: ``, lut: None
[0m16:57:16.552656 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:57:16.552770 [debug] [MainThread]: Databricks adapter: Thread (79472, 7965269056) using default compute resource.
[0m16:57:16.552889 [debug] [MainThread]: Databricks adapter: conn: 6138340496: _acquire sess: None, name: master, idle: 1.1920928955078125e-06s, acqrelcnt: 1, lang: None, thrd: (79472, 7965269056), cmpt: ``, lut: 1710791836.552852
[0m16:57:16.553411 [debug] [ThreadPool]: Databricks adapter: conn: 6136870480: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore, idle: 0s, acqrelcnt: 0, lang: None, thrd: (79472, 10888441856), cmpt: ``, lut: None
[0m16:57:16.553592 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m16:57:16.553715 [debug] [ThreadPool]: Databricks adapter: Thread (79472, 10888441856) using default compute resource.
[0m16:57:16.553834 [debug] [ThreadPool]: Databricks adapter: conn: 6136870480: _acquire sess: None, name: list_hive_metastore, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (79472, 10888441856), cmpt: ``, lut: 1710791836.553796
[0m16:57:16.553976 [debug] [ThreadPool]: Databricks adapter: conn: 6136870480: get_thread_connection: sess: None, name: list_hive_metastore, idle: 0.00014281272888183594s, acqrelcnt: 1, lang: None, thrd: (79472, 10888441856), cmpt: ``, lut: 1710791836.553796
[0m16:57:16.554103 [debug] [ThreadPool]: Databricks adapter: conn: 6136870480: idle check connection: sess: None, name: list_hive_metastore, idle: 0.0002651214599609375s, acqrelcnt: 1, lang: None, thrd: (79472, 10888441856), cmpt: ``, lut: 1710791836.553796
[0m16:57:16.554213 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m16:57:16.554326 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m16:57:16.554432 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:57:17.047364 [debug] [ThreadPool]: Databricks adapter: conn: 6136870480: session opened sess: 01eee561-b8c4-1de6-8cc8-7b2444cbd897, name: list_hive_metastore, idle: 1.621246337890625e-05s, acqrelcnt: 1, lang: None, thrd: (79472, 10888441856), cmpt: ``, lut: 1710791837.046898
[0m16:57:21.328121 [debug] [ThreadPool]: SQL status: OK in 4.769999980926514 seconds
[0m16:57:21.338058 [debug] [ThreadPool]: Databricks adapter: conn: 6136870480: _release sess: 01eee561-b8c4-1de6-8cc8-7b2444cbd897, name: list_hive_metastore, idle: 7.867813110351562e-06s, acqrelcnt: 0, lang: None, thrd: (79472, 10888441856), cmpt: ``, lut: 1710791841.3379211
[0m16:57:21.339684 [debug] [ThreadPool]: Databricks adapter: conn: 6136870480: idle check connection: sess: 01eee561-b8c4-1de6-8cc8-7b2444cbd897, name: list_hive_metastore, idle: 0.0016717910766601562s, acqrelcnt: 0, lang: None, thrd: (79472, 10888441856), cmpt: ``, lut: 1710791841.3379211
[0m16:57:21.339971 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now list_hive_metastore_default)
[0m16:57:21.340189 [debug] [ThreadPool]: Databricks adapter: conn: 6136870480: reusing connection list_hive_metastore sess: 01eee561-b8c4-1de6-8cc8-7b2444cbd897, name: list_hive_metastore_default, idle: 0.002203702926635742s, acqrelcnt: 0, lang: None, thrd: (79472, 10888441856), cmpt: ``, lut: 1710791841.3379211
[0m16:57:21.340384 [debug] [ThreadPool]: Databricks adapter: Thread (79472, 10888441856) using default compute resource.
[0m16:57:21.340577 [debug] [ThreadPool]: Databricks adapter: conn: 6136870480: _acquire sess: 01eee561-b8c4-1de6-8cc8-7b2444cbd897, name: list_hive_metastore_default, idle: 0.002597808837890625s, acqrelcnt: 1, lang: None, thrd: (79472, 10888441856), cmpt: ``, lut: 1710791841.3379211
[0m16:57:21.344458 [debug] [ThreadPool]: Databricks adapter: conn: 6136870480: get_thread_connection: sess: 01eee561-b8c4-1de6-8cc8-7b2444cbd897, name: list_hive_metastore_default, idle: 0.006447792053222656s, acqrelcnt: 1, lang: None, thrd: (79472, 10888441856), cmpt: ``, lut: 1710791841.3379211
[0m16:57:21.344728 [debug] [ThreadPool]: Databricks adapter: conn: 6136870480: idle check connection: sess: 01eee561-b8c4-1de6-8cc8-7b2444cbd897, name: list_hive_metastore_default, idle: 0.006746768951416016s, acqrelcnt: 1, lang: None, thrd: (79472, 10888441856), cmpt: ``, lut: 1710791841.3379211
[0m16:57:21.344926 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m16:57:21.345103 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m16:57:21.816003 [debug] [ThreadPool]: SQL status: OK in 0.4699999988079071 seconds
[0m16:57:21.830066 [debug] [ThreadPool]: Databricks adapter: conn: 6136870480: get_thread_connection: sess: 01eee561-b8c4-1de6-8cc8-7b2444cbd897, name: list_hive_metastore_default, idle: 0.492006778717041s, acqrelcnt: 1, lang: None, thrd: (79472, 10888441856), cmpt: ``, lut: 1710791841.3379211
[0m16:57:21.830456 [debug] [ThreadPool]: Databricks adapter: conn: 6136870480: idle check connection: sess: 01eee561-b8c4-1de6-8cc8-7b2444cbd897, name: list_hive_metastore_default, idle: 0.4924488067626953s, acqrelcnt: 1, lang: None, thrd: (79472, 10888441856), cmpt: ``, lut: 1710791841.3379211
[0m16:57:21.830741 [debug] [ThreadPool]: Databricks adapter: conn: 6136870480: get_thread_connection: sess: 01eee561-b8c4-1de6-8cc8-7b2444cbd897, name: list_hive_metastore_default, idle: 0.4927406311035156s, acqrelcnt: 1, lang: None, thrd: (79472, 10888441856), cmpt: ``, lut: 1710791841.3379211
[0m16:57:21.830997 [debug] [ThreadPool]: Databricks adapter: conn: 6136870480: idle check connection: sess: 01eee561-b8c4-1de6-8cc8-7b2444cbd897, name: list_hive_metastore_default, idle: 0.49300193786621094s, acqrelcnt: 1, lang: None, thrd: (79472, 10888441856), cmpt: ``, lut: 1710791841.3379211
[0m16:57:21.831228 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m16:57:21.831421 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m16:57:21.831657 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m16:57:22.920739 [debug] [ThreadPool]: SQL status: OK in 1.090000033378601 seconds
[0m16:57:22.929710 [debug] [ThreadPool]: Databricks adapter: conn: 6136870480: get_thread_connection: sess: 01eee561-b8c4-1de6-8cc8-7b2444cbd897, name: list_hive_metastore_default, idle: 1.5916368961334229s, acqrelcnt: 1, lang: None, thrd: (79472, 10888441856), cmpt: ``, lut: 1710791841.3379211
[0m16:57:22.930155 [debug] [ThreadPool]: Databricks adapter: conn: 6136870480: idle check connection: sess: 01eee561-b8c4-1de6-8cc8-7b2444cbd897, name: list_hive_metastore_default, idle: 1.5921499729156494s, acqrelcnt: 1, lang: None, thrd: (79472, 10888441856), cmpt: ``, lut: 1710791841.3379211
[0m16:57:22.930415 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m16:57:22.930677 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m16:57:23.471223 [debug] [ThreadPool]: SQL status: OK in 0.5400000214576721 seconds
[0m16:57:23.477717 [debug] [ThreadPool]: Databricks adapter: conn: 6136870480: _release sess: 01eee561-b8c4-1de6-8cc8-7b2444cbd897, name: list_hive_metastore_default, idle: 9.059906005859375e-06s, acqrelcnt: 0, lang: None, thrd: (79472, 10888441856), cmpt: ``, lut: 1710791843.477487
[0m16:57:23.481699 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '474721df-75d1-45e2-b0a5-56ac3419f321', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16e09d990>]}
[0m16:57:23.482178 [debug] [MainThread]: Databricks adapter: conn: 6138340496: get_thread_connection: sess: None, name: master, idle: 6.929229021072388s, acqrelcnt: 1, lang: None, thrd: (79472, 7965269056), cmpt: ``, lut: 1710791836.552852
[0m16:57:23.482462 [debug] [MainThread]: Databricks adapter: conn: 6138340496: idle check connection: sess: None, name: master, idle: 6.929531097412109s, acqrelcnt: 1, lang: None, thrd: (79472, 7965269056), cmpt: ``, lut: 1710791836.552852
[0m16:57:23.482729 [debug] [MainThread]: Databricks adapter: conn: 6138340496: get_thread_connection: sess: None, name: master, idle: 6.9297990798950195s, acqrelcnt: 1, lang: None, thrd: (79472, 7965269056), cmpt: ``, lut: 1710791836.552852
[0m16:57:23.482976 [debug] [MainThread]: Databricks adapter: conn: 6138340496: idle check connection: sess: None, name: master, idle: 6.930052042007446s, acqrelcnt: 1, lang: None, thrd: (79472, 7965269056), cmpt: ``, lut: 1710791836.552852
[0m16:57:23.483213 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:57:23.483439 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:57:23.483681 [debug] [MainThread]: Databricks adapter: conn: 6138340496: _release sess: None, name: master, idle: 1.9073486328125e-06s, acqrelcnt: 0, lang: None, thrd: (79472, 7965269056), cmpt: ``, lut: 1710791843.483606
[0m16:57:23.484278 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:57:23.484561 [info ] [MainThread]: 
[0m16:57:23.489938 [debug] [Thread-1 (]: Began running node model.default.stage_users
[0m16:57:23.490388 [info ] [Thread-1 (]: 1 of 1 START sql view model default.stage_users ................................ [RUN]
[0m16:57:23.491108 [debug] [Thread-1 (]: Databricks adapter: conn: 6136870480: idle check connection: sess: 01eee561-b8c4-1de6-8cc8-7b2444cbd897, name: list_hive_metastore_default, idle: 0.013485908508300781s, acqrelcnt: 0, lang: None, thrd: (79472, 10888441856), cmpt: ``, lut: 1710791843.477487
[0m16:57:23.491390 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.default.stage_users)
[0m16:57:23.491682 [debug] [Thread-1 (]: Databricks adapter: conn: 6136870480: reusing connection list_hive_metastore_default sess: 01eee561-b8c4-1de6-8cc8-7b2444cbd897, name: model.default.stage_users, idle: 0.014077901840209961s, acqrelcnt: 0, lang: None, thrd: (79472, 10888441856), cmpt: ``, lut: 1710791843.477487
[0m16:57:23.491969 [debug] [Thread-1 (]: Databricks adapter: On thread (79472, 10888441856): `hive_metastore`.`default`.`stage_users` using default compute resource.
[0m16:57:23.492243 [debug] [Thread-1 (]: Databricks adapter: conn: 6136870480: _acquire sess: 01eee561-b8c4-1de6-8cc8-7b2444cbd897, name: model.default.stage_users, idle: 0.014645099639892578s, acqrelcnt: 1, lang: sql, thrd: (79472, 10888441856), cmpt: ``, lut: 1710791843.477487
[0m16:57:23.492523 [debug] [Thread-1 (]: Began compiling node model.default.stage_users
[0m16:57:23.498460 [debug] [Thread-1 (]: Writing injected SQL for node "model.default.stage_users"
[0m16:57:23.515787 [debug] [Thread-1 (]: Timing info for model.default.stage_users (compile): 16:57:23.492703 => 16:57:23.515544
[0m16:57:23.516081 [debug] [Thread-1 (]: Began executing node model.default.stage_users
[0m16:57:23.530480 [debug] [Thread-1 (]: Writing runtime sql for node "model.default.stage_users"
[0m16:57:23.536199 [debug] [Thread-1 (]: Databricks adapter: conn: 6136870480: get_thread_connection: sess: 01eee561-b8c4-1de6-8cc8-7b2444cbd897, name: model.default.stage_users, idle: 0.05861401557922363s, acqrelcnt: 1, lang: sql, thrd: (79472, 10888441856), cmpt: ``, lut: 1710791843.477487
[0m16:57:23.536420 [debug] [Thread-1 (]: Databricks adapter: conn: 6136870480: idle check connection: sess: 01eee561-b8c4-1de6-8cc8-7b2444cbd897, name: model.default.stage_users, idle: 0.058859825134277344s, acqrelcnt: 1, lang: sql, thrd: (79472, 10888441856), cmpt: ``, lut: 1710791843.477487
[0m16:57:23.536571 [debug] [Thread-1 (]: Using databricks connection "model.default.stage_users"
[0m16:57:23.536766 [debug] [Thread-1 (]: On model.default.stage_users: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_users"} */
create or replace view `hive_metastore`.`default`.`stage_users`
  
  
  
  as
    

select user_id AS user_id,
       name AS name,
       city AS city,
       phone_number AS phone_number,
       gender AS gender,
       nationality AS nationality,
       state AS state
from `dbtdbsqldw`.`raw`.`users`

[0m16:57:24.804910 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_users"} */
create or replace view `hive_metastore`.`default`.`stage_users`
  
  
  
  as
    

select user_id AS user_id,
       name AS name,
       city AS city,
       phone_number AS phone_number,
       gender AS gender,
       nationality AS nationality,
       state AS state
from `dbtdbsqldw`.`raw`.`users`

[0m16:57:24.805414 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dbtdbsqldw`.`raw`.`users` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 16 pos 5
[0m16:57:24.805986 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dbtdbsqldw`.`raw`.`users` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 16 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:697)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:574)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:423)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:65)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:65)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:401)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:386)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:435)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dbtdbsqldw`.`raw`.`users` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 16 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:81)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:674)
	... 36 more

[0m16:57:24.806525 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01eee561-bcbe-1f55-bbae-7b0ace4e3946
[0m16:57:24.806973 [debug] [Thread-1 (]: Timing info for model.default.stage_users (execute): 16:57:23.516207 => 16:57:24.806768
[0m16:57:24.807315 [debug] [Thread-1 (]: Databricks adapter: conn: 6136870480: _release sess: 01eee561-b8c4-1de6-8cc8-7b2444cbd897, name: model.default.stage_users, idle: 2.1457672119140625e-06s, acqrelcnt: 0, lang: sql, thrd: (79472, 10888441856), cmpt: ``, lut: 1710791844.807179
[0m16:57:24.830870 [debug] [Thread-1 (]: Runtime Error in model stage_users (models/stage/stage_users.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dbtdbsqldw`.`raw`.`users` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 16 pos 5
[0m16:57:24.831355 [debug] [Thread-1 (]: Databricks adapter: conn: 6136870480: _release sess: 01eee561-b8c4-1de6-8cc8-7b2444cbd897, name: model.default.stage_users, idle: 2.86102294921875e-06s, acqrelcnt: 0, lang: sql, thrd: (79472, 10888441856), cmpt: ``, lut: 1710791844.831221
[0m16:57:24.831708 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '474721df-75d1-45e2-b0a5-56ac3419f321', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x177e07ed0>]}
[0m16:57:24.832189 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model default.stage_users ....................... [[31mERROR[0m in 1.34s]
[0m16:57:24.832627 [debug] [Thread-1 (]: Finished running node model.default.stage_users
[0m16:57:24.833724 [debug] [MainThread]: Databricks adapter: conn: 6138340496: idle check connection: sess: None, name: master, idle: 1.3500328063964844s, acqrelcnt: 0, lang: None, thrd: (79472, 7965269056), cmpt: ``, lut: 1710791843.483606
[0m16:57:24.833993 [debug] [MainThread]: Databricks adapter: conn: 6138340496: reusing connection master sess: None, name: master, idle: 1.3503179550170898s, acqrelcnt: 0, lang: None, thrd: (79472, 7965269056), cmpt: ``, lut: 1710791843.483606
[0m16:57:24.834206 [debug] [MainThread]: Databricks adapter: Thread (79472, 7965269056) using default compute resource.
[0m16:57:24.834410 [debug] [MainThread]: Databricks adapter: conn: 6138340496: _acquire sess: None, name: master, idle: 1.3507418632507324s, acqrelcnt: 1, lang: None, thrd: (79472, 7965269056), cmpt: ``, lut: 1710791843.483606
[0m16:57:24.834648 [debug] [MainThread]: Databricks adapter: conn: 6138340496: get_thread_connection: sess: None, name: master, idle: 1.3509747982025146s, acqrelcnt: 1, lang: None, thrd: (79472, 7965269056), cmpt: ``, lut: 1710791843.483606
[0m16:57:24.834861 [debug] [MainThread]: Databricks adapter: conn: 6138340496: idle check connection: sess: None, name: master, idle: 1.351194143295288s, acqrelcnt: 1, lang: None, thrd: (79472, 7965269056), cmpt: ``, lut: 1710791843.483606
[0m16:57:24.835061 [debug] [MainThread]: On master: ROLLBACK
[0m16:57:24.835275 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:57:25.309381 [debug] [MainThread]: Databricks adapter: conn: 6138340496: session opened sess: 01eee561-bdb1-13d4-92c7-4fde6f3d2a03, name: master, idle: 1.0013580322265625e-05s, acqrelcnt: 1, lang: None, thrd: (79472, 7965269056), cmpt: ``, lut: 1710791845.308922
[0m16:57:25.310565 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:57:25.311569 [debug] [MainThread]: Databricks adapter: conn: 6138340496: get_thread_connection: sess: 01eee561-bdb1-13d4-92c7-4fde6f3d2a03, name: master, idle: 0.0022869110107421875s, acqrelcnt: 1, lang: None, thrd: (79472, 7965269056), cmpt: ``, lut: 1710791845.308922
[0m16:57:25.313400 [debug] [MainThread]: Databricks adapter: conn: 6138340496: idle check connection: sess: 01eee561-bdb1-13d4-92c7-4fde6f3d2a03, name: master, idle: 0.003242015838623047s, acqrelcnt: 1, lang: None, thrd: (79472, 7965269056), cmpt: ``, lut: 1710791845.308922
[0m16:57:25.314332 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:57:25.314932 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:57:25.315900 [debug] [MainThread]: Databricks adapter: conn: 6138340496: _release sess: 01eee561-bdb1-13d4-92c7-4fde6f3d2a03, name: master, idle: 3.814697265625e-06s, acqrelcnt: 0, lang: None, thrd: (79472, 7965269056), cmpt: ``, lut: 1710791845.315767
[0m16:57:25.317514 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:57:25.317990 [debug] [MainThread]: On master: ROLLBACK
[0m16:57:25.318613 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:57:25.319086 [debug] [MainThread]: On master: Close
[0m16:57:25.486719 [debug] [MainThread]: Connection 'model.default.stage_users' was properly closed.
[0m16:57:25.488038 [debug] [MainThread]: On model.default.stage_users: ROLLBACK
[0m16:57:25.488864 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:57:25.489450 [debug] [MainThread]: On model.default.stage_users: Close
[0m16:57:25.659414 [info ] [MainThread]: 
[0m16:57:25.660658 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 9.11 seconds (9.11s).
[0m16:57:25.662157 [debug] [MainThread]: Command end result
[0m16:57:25.719888 [info ] [MainThread]: 
[0m16:57:25.720469 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m16:57:25.720757 [info ] [MainThread]: 
[0m16:57:25.721032 [error] [MainThread]:   Runtime Error in model stage_users (models/stage/stage_users.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dbtdbsqldw`.`raw`.`users` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 16 pos 5
[0m16:57:25.721319 [info ] [MainThread]: 
[0m16:57:25.721617 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m16:57:25.749528 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 11.097742, "process_user_time": 2.238032, "process_kernel_time": 3.325411, "process_mem_max_rss": "224657408", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:57:25.750112 [debug] [MainThread]: Command `cli run` failed at 16:57:25.750015 after 11.10 seconds
[0m16:57:25.750541 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107dd9dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121fd86d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121ffe3d0>]}
[0m16:57:25.750824 [debug] [MainThread]: Flushing usage events
[0m16:57:56.543922 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113e76850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113ec1990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113ec1fd0>]}


============================== 16:57:56.546882 | 8d70e50f-233a-4326-afcf-b2741635e973 ==============================
[0m16:57:56.546882 [info ] [MainThread]: Running with dbt=1.7.8
[0m16:57:56.547181 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'fail_fast': 'False', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:57:58.019881 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8d70e50f-233a-4326-afcf-b2741635e973', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16aa85e50>]}
[0m16:57:58.048413 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8d70e50f-233a-4326-afcf-b2741635e973', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16a9e6810>]}
[0m16:57:58.048710 [info ] [MainThread]: Registered adapter: databricks=1.7.9
[0m16:57:58.064505 [debug] [MainThread]: checksum: 67f0013ca5f0bd43af9a0873dd50792fde83ef69de63b71cacd0b4ac656c52e5, vars: {}, profile: , target: , version: 1.7.8
[0m16:57:58.156344 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:57:58.156758 [debug] [MainThread]: Partial parsing: updated file: default://models/sources.yml
[0m16:57:58.228117 [error] [MainThread]: Encountered an error:
Compilation Error
  Model 'model.default.stage_users' (models/stage/stage_users.sql) depends on a source named 'raw.users' which was not found
[0m16:57:58.232461 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 1.7175062, "process_user_time": 2.016704, "process_kernel_time": 3.447506, "process_mem_max_rss": "208404480", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:57:58.232701 [debug] [MainThread]: Command `cli run` failed at 16:57:58.232643 after 1.72 seconds
[0m16:57:58.232893 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113ec3e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113ec2210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102ad3c50>]}
[0m16:57:58.233064 [debug] [MainThread]: Flushing usage events
[0m16:58:21.699431 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111583a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110e685d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1115d3290>]}


============================== 16:58:21.702022 | fa036e43-54a7-44d6-844f-71631e118dae ==============================
[0m16:58:21.702022 [info ] [MainThread]: Running with dbt=1.7.8
[0m16:58:21.702320 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m16:58:23.011978 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fa036e43-54a7-44d6-844f-71631e118dae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x137334790>]}
[0m16:58:23.040355 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fa036e43-54a7-44d6-844f-71631e118dae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110c44f50>]}
[0m16:58:23.043504 [info ] [MainThread]: Registered adapter: databricks=1.7.9
[0m16:58:23.061490 [debug] [MainThread]: checksum: 67f0013ca5f0bd43af9a0873dd50792fde83ef69de63b71cacd0b4ac656c52e5, vars: {}, profile: , target: , version: 1.7.8
[0m16:58:23.141147 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m16:58:23.141547 [debug] [MainThread]: Partial parsing: updated file: default://models/sources.yml
[0m16:58:23.141701 [debug] [MainThread]: Partial parsing: updated file: default://models/stage/stage_users.sql
[0m16:58:23.225017 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fa036e43-54a7-44d6-844f-71631e118dae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1378a8a50>]}
[0m16:58:23.232844 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fa036e43-54a7-44d6-844f-71631e118dae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x137815190>]}
[0m16:58:23.233156 [info ] [MainThread]: Found 3 models, 3 sources, 0 exposures, 0 metrics, 538 macros, 0 groups, 0 semantic models
[0m16:58:23.233350 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fa036e43-54a7-44d6-844f-71631e118dae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13781fad0>]}
[0m16:58:23.234043 [info ] [MainThread]: 
[0m16:58:23.234561 [debug] [MainThread]: Databricks adapter: conn: 5226193360: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (79892, 7965269056), cmpt: ``, lut: None
[0m16:58:23.234760 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:58:23.234909 [debug] [MainThread]: Databricks adapter: Thread (79892, 7965269056) using default compute resource.
[0m16:58:23.235033 [debug] [MainThread]: Databricks adapter: conn: 5226193360: _acquire sess: None, name: master, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (79892, 7965269056), cmpt: ``, lut: 1710791903.234992
[0m16:58:23.235696 [debug] [ThreadPool]: Databricks adapter: conn: 5226235792: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore, idle: 0s, acqrelcnt: 0, lang: None, thrd: (79892, 11314704384), cmpt: ``, lut: None
[0m16:58:23.235879 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m16:58:23.236094 [debug] [ThreadPool]: Databricks adapter: Thread (79892, 11314704384) using default compute resource.
[0m16:58:23.236263 [debug] [ThreadPool]: Databricks adapter: conn: 5226235792: _acquire sess: None, name: list_hive_metastore, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (79892, 11314704384), cmpt: ``, lut: 1710791903.236216
[0m16:58:23.236442 [debug] [ThreadPool]: Databricks adapter: conn: 5226235792: get_thread_connection: sess: None, name: list_hive_metastore, idle: 0.0001819133758544922s, acqrelcnt: 1, lang: None, thrd: (79892, 11314704384), cmpt: ``, lut: 1710791903.236216
[0m16:58:23.236658 [debug] [ThreadPool]: Databricks adapter: conn: 5226235792: idle check connection: sess: None, name: list_hive_metastore, idle: 0.0003421306610107422s, acqrelcnt: 1, lang: None, thrd: (79892, 11314704384), cmpt: ``, lut: 1710791903.236216
[0m16:58:23.236862 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m16:58:23.237014 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m16:58:23.237398 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:58:23.731455 [debug] [ThreadPool]: Databricks adapter: conn: 5226235792: session opened sess: 01eee561-e086-1fc5-8130-172c898a87b9, name: list_hive_metastore, idle: 1.4781951904296875e-05s, acqrelcnt: 1, lang: None, thrd: (79892, 11314704384), cmpt: ``, lut: 1710791903.7309492
[0m16:58:24.117188 [debug] [ThreadPool]: SQL status: OK in 0.8799999952316284 seconds
[0m16:58:24.136622 [debug] [ThreadPool]: Databricks adapter: conn: 5226235792: _release sess: 01eee561-e086-1fc5-8130-172c898a87b9, name: list_hive_metastore, idle: 7.867813110351562e-06s, acqrelcnt: 0, lang: None, thrd: (79892, 11314704384), cmpt: ``, lut: 1710791904.136405
[0m16:58:24.138551 [debug] [ThreadPool]: Databricks adapter: conn: 5226235792: idle check connection: sess: 01eee561-e086-1fc5-8130-172c898a87b9, name: list_hive_metastore, idle: 0.002017974853515625s, acqrelcnt: 0, lang: None, thrd: (79892, 11314704384), cmpt: ``, lut: 1710791904.136405
[0m16:58:24.138950 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now list_hive_metastore_default)
[0m16:58:24.139255 [debug] [ThreadPool]: Databricks adapter: conn: 5226235792: reusing connection list_hive_metastore sess: 01eee561-e086-1fc5-8130-172c898a87b9, name: list_hive_metastore_default, idle: 0.0027577877044677734s, acqrelcnt: 0, lang: None, thrd: (79892, 11314704384), cmpt: ``, lut: 1710791904.136405
[0m16:58:24.139518 [debug] [ThreadPool]: Databricks adapter: Thread (79892, 11314704384) using default compute resource.
[0m16:58:24.139771 [debug] [ThreadPool]: Databricks adapter: conn: 5226235792: _acquire sess: 01eee561-e086-1fc5-8130-172c898a87b9, name: list_hive_metastore_default, idle: 0.003286123275756836s, acqrelcnt: 1, lang: None, thrd: (79892, 11314704384), cmpt: ``, lut: 1710791904.136405
[0m16:58:24.143751 [debug] [ThreadPool]: Databricks adapter: conn: 5226235792: get_thread_connection: sess: 01eee561-e086-1fc5-8130-172c898a87b9, name: list_hive_metastore_default, idle: 0.0072591304779052734s, acqrelcnt: 1, lang: None, thrd: (79892, 11314704384), cmpt: ``, lut: 1710791904.136405
[0m16:58:24.144021 [debug] [ThreadPool]: Databricks adapter: conn: 5226235792: idle check connection: sess: 01eee561-e086-1fc5-8130-172c898a87b9, name: list_hive_metastore_default, idle: 0.007543087005615234s, acqrelcnt: 1, lang: None, thrd: (79892, 11314704384), cmpt: ``, lut: 1710791904.136405
[0m16:58:24.144231 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m16:58:24.144442 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m16:58:24.596249 [debug] [ThreadPool]: SQL status: OK in 0.44999998807907104 seconds
[0m16:58:24.617693 [debug] [ThreadPool]: Databricks adapter: conn: 5226235792: get_thread_connection: sess: 01eee561-e086-1fc5-8130-172c898a87b9, name: list_hive_metastore_default, idle: 0.48101186752319336s, acqrelcnt: 1, lang: None, thrd: (79892, 11314704384), cmpt: ``, lut: 1710791904.136405
[0m16:58:24.618388 [debug] [ThreadPool]: Databricks adapter: conn: 5226235792: idle check connection: sess: 01eee561-e086-1fc5-8130-172c898a87b9, name: list_hive_metastore_default, idle: 0.48189377784729004s, acqrelcnt: 1, lang: None, thrd: (79892, 11314704384), cmpt: ``, lut: 1710791904.136405
[0m16:58:24.618708 [debug] [ThreadPool]: Databricks adapter: conn: 5226235792: get_thread_connection: sess: 01eee561-e086-1fc5-8130-172c898a87b9, name: list_hive_metastore_default, idle: 0.482219934463501s, acqrelcnt: 1, lang: None, thrd: (79892, 11314704384), cmpt: ``, lut: 1710791904.136405
[0m16:58:24.619042 [debug] [ThreadPool]: Databricks adapter: conn: 5226235792: idle check connection: sess: 01eee561-e086-1fc5-8130-172c898a87b9, name: list_hive_metastore_default, idle: 0.482496976852417s, acqrelcnt: 1, lang: None, thrd: (79892, 11314704384), cmpt: ``, lut: 1710791904.136405
[0m16:58:24.619707 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m16:58:24.620304 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m16:58:24.620638 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m16:58:24.971400 [debug] [ThreadPool]: SQL status: OK in 0.3499999940395355 seconds
[0m16:58:24.997574 [debug] [ThreadPool]: Databricks adapter: conn: 5226235792: get_thread_connection: sess: 01eee561-e086-1fc5-8130-172c898a87b9, name: list_hive_metastore_default, idle: 0.8608791828155518s, acqrelcnt: 1, lang: None, thrd: (79892, 11314704384), cmpt: ``, lut: 1710791904.136405
[0m16:58:24.998439 [debug] [ThreadPool]: Databricks adapter: conn: 5226235792: idle check connection: sess: 01eee561-e086-1fc5-8130-172c898a87b9, name: list_hive_metastore_default, idle: 0.8618700504302979s, acqrelcnt: 1, lang: None, thrd: (79892, 11314704384), cmpt: ``, lut: 1710791904.136405
[0m16:58:24.998957 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m16:58:24.999451 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m16:58:25.304841 [debug] [ThreadPool]: SQL status: OK in 0.30000001192092896 seconds
[0m16:58:25.311754 [debug] [ThreadPool]: Databricks adapter: conn: 5226235792: _release sess: 01eee561-e086-1fc5-8130-172c898a87b9, name: list_hive_metastore_default, idle: 2.002716064453125e-05s, acqrelcnt: 0, lang: None, thrd: (79892, 11314704384), cmpt: ``, lut: 1710791905.311438
[0m16:58:25.334711 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fa036e43-54a7-44d6-844f-71631e118dae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1374bd210>]}
[0m16:58:25.335712 [debug] [MainThread]: Databricks adapter: conn: 5226193360: get_thread_connection: sess: None, name: master, idle: 2.100576877593994s, acqrelcnt: 1, lang: None, thrd: (79892, 7965269056), cmpt: ``, lut: 1710791903.234992
[0m16:58:25.336077 [debug] [MainThread]: Databricks adapter: conn: 5226193360: idle check connection: sess: None, name: master, idle: 2.1010069847106934s, acqrelcnt: 1, lang: None, thrd: (79892, 7965269056), cmpt: ``, lut: 1710791903.234992
[0m16:58:25.336423 [debug] [MainThread]: Databricks adapter: conn: 5226193360: get_thread_connection: sess: None, name: master, idle: 2.1013448238372803s, acqrelcnt: 1, lang: None, thrd: (79892, 7965269056), cmpt: ``, lut: 1710791903.234992
[0m16:58:25.336659 [debug] [MainThread]: Databricks adapter: conn: 5226193360: idle check connection: sess: None, name: master, idle: 2.10160493850708s, acqrelcnt: 1, lang: None, thrd: (79892, 7965269056), cmpt: ``, lut: 1710791903.234992
[0m16:58:25.336852 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:58:25.337039 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:58:25.337240 [debug] [MainThread]: Databricks adapter: conn: 5226193360: _release sess: None, name: master, idle: 9.5367431640625e-07s, acqrelcnt: 0, lang: None, thrd: (79892, 7965269056), cmpt: ``, lut: 1710791905.337183
[0m16:58:25.337974 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:58:25.338194 [info ] [MainThread]: 
[0m16:58:25.349885 [debug] [Thread-1 (]: Began running node model.default.stage_users
[0m16:58:25.350329 [info ] [Thread-1 (]: 1 of 1 START sql view model default.stage_users ................................ [RUN]
[0m16:58:25.350984 [debug] [Thread-1 (]: Databricks adapter: conn: 5226235792: idle check connection: sess: 01eee561-e086-1fc5-8130-172c898a87b9, name: list_hive_metastore_default, idle: 0.0394289493560791s, acqrelcnt: 0, lang: None, thrd: (79892, 11314704384), cmpt: ``, lut: 1710791905.311438
[0m16:58:25.351217 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.default.stage_users)
[0m16:58:25.351479 [debug] [Thread-1 (]: Databricks adapter: conn: 5226235792: reusing connection list_hive_metastore_default sess: 01eee561-e086-1fc5-8130-172c898a87b9, name: model.default.stage_users, idle: 0.039936065673828125s, acqrelcnt: 0, lang: None, thrd: (79892, 11314704384), cmpt: ``, lut: 1710791905.311438
[0m16:58:25.351728 [debug] [Thread-1 (]: Databricks adapter: On thread (79892, 11314704384): `hive_metastore`.`default`.`stage_users` using default compute resource.
[0m16:58:25.351965 [debug] [Thread-1 (]: Databricks adapter: conn: 5226235792: _acquire sess: 01eee561-e086-1fc5-8130-172c898a87b9, name: model.default.stage_users, idle: 0.04043388366699219s, acqrelcnt: 1, lang: sql, thrd: (79892, 11314704384), cmpt: ``, lut: 1710791905.311438
[0m16:58:25.352213 [debug] [Thread-1 (]: Began compiling node model.default.stage_users
[0m16:58:25.359070 [debug] [Thread-1 (]: Writing injected SQL for node "model.default.stage_users"
[0m16:58:25.361414 [debug] [Thread-1 (]: Timing info for model.default.stage_users (compile): 16:58:25.352372 => 16:58:25.361040
[0m16:58:25.361909 [debug] [Thread-1 (]: Began executing node model.default.stage_users
[0m16:58:25.386906 [debug] [Thread-1 (]: Writing runtime sql for node "model.default.stage_users"
[0m16:58:25.396414 [debug] [Thread-1 (]: Databricks adapter: conn: 5226235792: get_thread_connection: sess: 01eee561-e086-1fc5-8130-172c898a87b9, name: model.default.stage_users, idle: 0.0847928524017334s, acqrelcnt: 1, lang: sql, thrd: (79892, 11314704384), cmpt: ``, lut: 1710791905.311438
[0m16:58:25.396768 [debug] [Thread-1 (]: Databricks adapter: conn: 5226235792: idle check connection: sess: 01eee561-e086-1fc5-8130-172c898a87b9, name: model.default.stage_users, idle: 0.08523702621459961s, acqrelcnt: 1, lang: sql, thrd: (79892, 11314704384), cmpt: ``, lut: 1710791905.311438
[0m16:58:25.396974 [debug] [Thread-1 (]: Using databricks connection "model.default.stage_users"
[0m16:58:25.397223 [debug] [Thread-1 (]: On model.default.stage_users: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_users"} */
create or replace view `hive_metastore`.`default`.`stage_users`
  
  
  
  as
    

select user_id AS user_id,
       name AS name,
       city AS city,
       phone_number AS phone_number,
       gender AS gender,
       nationality AS nationality,
       state AS state
from `dbtdbsqldw`.`default`.`users`

[0m16:58:26.001295 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_users"} */
create or replace view `hive_metastore`.`default`.`stage_users`
  
  
  
  as
    

select user_id AS user_id,
       name AS name,
       city AS city,
       phone_number AS phone_number,
       gender AS gender,
       nationality AS nationality,
       state AS state
from `dbtdbsqldw`.`default`.`users`

[0m16:58:26.002768 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dbtdbsqldw`.`default`.`users` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 16 pos 5
[0m16:58:26.005023 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dbtdbsqldw`.`default`.`users` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 16 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:697)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:574)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:423)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:65)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:65)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:401)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:386)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:435)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dbtdbsqldw`.`default`.`users` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 16 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:81)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:674)
	... 36 more

[0m16:58:26.006071 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01eee561-e19f-1bb2-a7e7-0ac13bb9b159
[0m16:58:26.006980 [debug] [Thread-1 (]: Timing info for model.default.stage_users (execute): 16:58:25.362160 => 16:58:26.006654
[0m16:58:26.007775 [debug] [Thread-1 (]: Databricks adapter: conn: 5226235792: _release sess: 01eee561-e086-1fc5-8130-172c898a87b9, name: model.default.stage_users, idle: 7.867813110351562e-06s, acqrelcnt: 0, lang: sql, thrd: (79892, 11314704384), cmpt: ``, lut: 1710791906.007582
[0m16:58:26.055687 [debug] [Thread-1 (]: Runtime Error in model stage_users (models/stage/stage_users.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dbtdbsqldw`.`default`.`users` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 16 pos 5
[0m16:58:26.060372 [debug] [Thread-1 (]: Databricks adapter: conn: 5226235792: _release sess: 01eee561-e086-1fc5-8130-172c898a87b9, name: model.default.stage_users, idle: 6.9141387939453125e-06s, acqrelcnt: 0, lang: sql, thrd: (79892, 11314704384), cmpt: ``, lut: 1710791906.0601048
[0m16:58:26.061317 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fa036e43-54a7-44d6-844f-71631e118dae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1379fa210>]}
[0m16:58:26.063502 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model default.stage_users ....................... [[31mERROR[0m in 0.71s]
[0m16:58:26.064886 [debug] [Thread-1 (]: Finished running node model.default.stage_users
[0m16:58:26.075101 [debug] [MainThread]: Databricks adapter: conn: 5226193360: idle check connection: sess: None, name: master, idle: 0.7376260757446289s, acqrelcnt: 0, lang: None, thrd: (79892, 7965269056), cmpt: ``, lut: 1710791905.337183
[0m16:58:26.076258 [debug] [MainThread]: Databricks adapter: conn: 5226193360: reusing connection master sess: None, name: master, idle: 0.7388780117034912s, acqrelcnt: 0, lang: None, thrd: (79892, 7965269056), cmpt: ``, lut: 1710791905.337183
[0m16:58:26.076856 [debug] [MainThread]: Databricks adapter: Thread (79892, 7965269056) using default compute resource.
[0m16:58:26.077516 [debug] [MainThread]: Databricks adapter: conn: 5226193360: _acquire sess: None, name: master, idle: 0.7401669025421143s, acqrelcnt: 1, lang: None, thrd: (79892, 7965269056), cmpt: ``, lut: 1710791905.337183
[0m16:58:26.078128 [debug] [MainThread]: Databricks adapter: conn: 5226193360: get_thread_connection: sess: None, name: master, idle: 0.7408080101013184s, acqrelcnt: 1, lang: None, thrd: (79892, 7965269056), cmpt: ``, lut: 1710791905.337183
[0m16:58:26.078601 [debug] [MainThread]: Databricks adapter: conn: 5226193360: idle check connection: sess: None, name: master, idle: 0.7413120269775391s, acqrelcnt: 1, lang: None, thrd: (79892, 7965269056), cmpt: ``, lut: 1710791905.337183
[0m16:58:26.079326 [debug] [MainThread]: On master: ROLLBACK
[0m16:58:26.079896 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:58:26.572750 [debug] [MainThread]: Databricks adapter: conn: 5226193360: session opened sess: 01eee561-e235-16ba-8ab5-40868b8eb5ef, name: master, idle: 4.0531158447265625e-06s, acqrelcnt: 1, lang: None, thrd: (79892, 7965269056), cmpt: ``, lut: 1710791906.572552
[0m16:58:26.573349 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:58:26.573694 [debug] [MainThread]: Databricks adapter: conn: 5226193360: get_thread_connection: sess: 01eee561-e235-16ba-8ab5-40868b8eb5ef, name: master, idle: 0.0010569095611572266s, acqrelcnt: 1, lang: None, thrd: (79892, 7965269056), cmpt: ``, lut: 1710791906.572552
[0m16:58:26.573958 [debug] [MainThread]: Databricks adapter: conn: 5226193360: idle check connection: sess: 01eee561-e235-16ba-8ab5-40868b8eb5ef, name: master, idle: 0.0013339519500732422s, acqrelcnt: 1, lang: None, thrd: (79892, 7965269056), cmpt: ``, lut: 1710791906.572552
[0m16:58:26.574184 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:58:26.574383 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:58:26.574615 [debug] [MainThread]: Databricks adapter: conn: 5226193360: _release sess: 01eee561-e235-16ba-8ab5-40868b8eb5ef, name: master, idle: 9.5367431640625e-07s, acqrelcnt: 0, lang: None, thrd: (79892, 7965269056), cmpt: ``, lut: 1710791906.574545
[0m16:58:26.575206 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:58:26.575419 [debug] [MainThread]: On master: ROLLBACK
[0m16:58:26.575624 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:58:26.575820 [debug] [MainThread]: On master: Close
[0m16:58:26.742115 [debug] [MainThread]: Connection 'model.default.stage_users' was properly closed.
[0m16:58:26.742931 [debug] [MainThread]: On model.default.stage_users: ROLLBACK
[0m16:58:26.743472 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:58:26.743790 [debug] [MainThread]: On model.default.stage_users: Close
[0m16:58:26.907727 [info ] [MainThread]: 
[0m16:58:26.908239 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.67 seconds (3.67s).
[0m16:58:26.908772 [debug] [MainThread]: Command end result
[0m16:58:26.939373 [info ] [MainThread]: 
[0m16:58:26.939798 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m16:58:26.940005 [info ] [MainThread]: 
[0m16:58:26.940516 [error] [MainThread]:   Runtime Error in model stage_users (models/stage/stage_users.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dbtdbsqldw`.`default`.`users` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 16 pos 5
[0m16:58:26.940853 [info ] [MainThread]: 
[0m16:58:26.941097 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m16:58:26.954209 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 5.282989, "process_user_time": 2.030118, "process_kernel_time": 3.4139, "process_mem_max_rss": "227475456", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:58:26.954778 [debug] [MainThread]: Command `cli run` failed at 16:58:26.954680 after 5.28 seconds
[0m16:58:26.955173 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11160a390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1115fdc50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1048d3c10>]}
[0m16:58:26.955499 [debug] [MainThread]: Flushing usage events
[0m16:58:52.333136 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123547850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1235c14d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1235c1b10>]}


============================== 16:58:52.336175 | 063d82ab-da43-4a7d-bd9c-c01ba8427d9d ==============================
[0m16:58:52.336175 [info ] [MainThread]: Running with dbt=1.7.8
[0m16:58:52.336493 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'fail_fast': 'False', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:58:53.935090 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '063d82ab-da43-4a7d-bd9c-c01ba8427d9d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12355c2d0>]}
[0m16:58:53.963905 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '063d82ab-da43-4a7d-bd9c-c01ba8427d9d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12355c2d0>]}
[0m16:58:53.964177 [info ] [MainThread]: Registered adapter: databricks=1.7.9
[0m16:58:53.979813 [debug] [MainThread]: checksum: 67f0013ca5f0bd43af9a0873dd50792fde83ef69de63b71cacd0b4ac656c52e5, vars: {}, profile: , target: , version: 1.7.8
[0m16:58:54.059664 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:58:54.060075 [debug] [MainThread]: Partial parsing: updated file: default://models/sources.yml
[0m16:58:54.136923 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '063d82ab-da43-4a7d-bd9c-c01ba8427d9d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16bb5fd90>]}
[0m16:58:54.155079 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '063d82ab-da43-4a7d-bd9c-c01ba8427d9d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16bb16350>]}
[0m16:58:54.155305 [info ] [MainThread]: Found 3 models, 3 sources, 0 exposures, 0 metrics, 538 macros, 0 groups, 0 semantic models
[0m16:58:54.155487 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '063d82ab-da43-4a7d-bd9c-c01ba8427d9d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123002190>]}
[0m16:58:54.156130 [info ] [MainThread]: 
[0m16:58:54.156569 [debug] [MainThread]: Databricks adapter: conn: 6101798032: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (80079, 7965269056), cmpt: ``, lut: None
[0m16:58:54.156722 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m16:58:54.156850 [debug] [MainThread]: Databricks adapter: Thread (80079, 7965269056) using default compute resource.
[0m16:58:54.156973 [debug] [MainThread]: Databricks adapter: conn: 6101798032: _acquire sess: None, name: master, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (80079, 7965269056), cmpt: ``, lut: 1710791934.1569312
[0m16:58:54.157504 [debug] [ThreadPool]: Databricks adapter: conn: 6097381904: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore, idle: 0s, acqrelcnt: 0, lang: None, thrd: (80079, 11066404864), cmpt: ``, lut: None
[0m16:58:54.157689 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m16:58:54.157821 [debug] [ThreadPool]: Databricks adapter: Thread (80079, 11066404864) using default compute resource.
[0m16:58:54.157946 [debug] [ThreadPool]: Databricks adapter: conn: 6097381904: _acquire sess: None, name: list_hive_metastore, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (80079, 11066404864), cmpt: ``, lut: 1710791934.157907
[0m16:58:54.158092 [debug] [ThreadPool]: Databricks adapter: conn: 6097381904: get_thread_connection: sess: None, name: list_hive_metastore, idle: 0.0001461505889892578s, acqrelcnt: 1, lang: None, thrd: (80079, 11066404864), cmpt: ``, lut: 1710791934.157907
[0m16:58:54.158222 [debug] [ThreadPool]: Databricks adapter: conn: 6097381904: idle check connection: sess: None, name: list_hive_metastore, idle: 0.00027489662170410156s, acqrelcnt: 1, lang: None, thrd: (80079, 11066404864), cmpt: ``, lut: 1710791934.157907
[0m16:58:54.158337 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m16:58:54.158455 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m16:58:54.158574 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:58:54.676058 [debug] [ThreadPool]: Databricks adapter: conn: 6097381904: session opened sess: 01eee561-f2f6-14df-a482-b4eed108dde7, name: list_hive_metastore, idle: 1.2159347534179688e-05s, acqrelcnt: 1, lang: None, thrd: (80079, 11066404864), cmpt: ``, lut: 1710791934.675618
[0m16:58:54.973607 [debug] [ThreadPool]: SQL status: OK in 0.8100000023841858 seconds
[0m16:58:54.983172 [debug] [ThreadPool]: Databricks adapter: conn: 6097381904: _release sess: 01eee561-f2f6-14df-a482-b4eed108dde7, name: list_hive_metastore, idle: 6.9141387939453125e-06s, acqrelcnt: 0, lang: None, thrd: (80079, 11066404864), cmpt: ``, lut: 1710791934.98303
[0m16:58:54.984604 [debug] [ThreadPool]: Databricks adapter: conn: 6097381904: idle check connection: sess: 01eee561-f2f6-14df-a482-b4eed108dde7, name: list_hive_metastore, idle: 0.001483917236328125s, acqrelcnt: 0, lang: None, thrd: (80079, 11066404864), cmpt: ``, lut: 1710791934.98303
[0m16:58:54.984890 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now list_hive_metastore_default)
[0m16:58:54.985143 [debug] [ThreadPool]: Databricks adapter: conn: 6097381904: reusing connection list_hive_metastore sess: 01eee561-f2f6-14df-a482-b4eed108dde7, name: list_hive_metastore_default, idle: 0.0020318031311035156s, acqrelcnt: 0, lang: None, thrd: (80079, 11066404864), cmpt: ``, lut: 1710791934.98303
[0m16:58:54.985366 [debug] [ThreadPool]: Databricks adapter: Thread (80079, 11066404864) using default compute resource.
[0m16:58:54.985581 [debug] [ThreadPool]: Databricks adapter: conn: 6097381904: _acquire sess: 01eee561-f2f6-14df-a482-b4eed108dde7, name: list_hive_metastore_default, idle: 0.002485990524291992s, acqrelcnt: 1, lang: None, thrd: (80079, 11066404864), cmpt: ``, lut: 1710791934.98303
[0m16:58:54.988818 [debug] [ThreadPool]: Databricks adapter: conn: 6097381904: get_thread_connection: sess: 01eee561-f2f6-14df-a482-b4eed108dde7, name: list_hive_metastore_default, idle: 0.005707979202270508s, acqrelcnt: 1, lang: None, thrd: (80079, 11066404864), cmpt: ``, lut: 1710791934.98303
[0m16:58:54.989056 [debug] [ThreadPool]: Databricks adapter: conn: 6097381904: idle check connection: sess: 01eee561-f2f6-14df-a482-b4eed108dde7, name: list_hive_metastore_default, idle: 0.005961894989013672s, acqrelcnt: 1, lang: None, thrd: (80079, 11066404864), cmpt: ``, lut: 1710791934.98303
[0m16:58:54.989240 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m16:58:54.989420 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m16:58:55.322307 [debug] [ThreadPool]: SQL status: OK in 0.33000001311302185 seconds
[0m16:58:55.330385 [debug] [ThreadPool]: Databricks adapter: conn: 6097381904: get_thread_connection: sess: 01eee561-f2f6-14df-a482-b4eed108dde7, name: list_hive_metastore_default, idle: 0.34726405143737793s, acqrelcnt: 1, lang: None, thrd: (80079, 11066404864), cmpt: ``, lut: 1710791934.98303
[0m16:58:55.330708 [debug] [ThreadPool]: Databricks adapter: conn: 6097381904: idle check connection: sess: 01eee561-f2f6-14df-a482-b4eed108dde7, name: list_hive_metastore_default, idle: 0.3476107120513916s, acqrelcnt: 1, lang: None, thrd: (80079, 11066404864), cmpt: ``, lut: 1710791934.98303
[0m16:58:55.330902 [debug] [ThreadPool]: Databricks adapter: conn: 6097381904: get_thread_connection: sess: 01eee561-f2f6-14df-a482-b4eed108dde7, name: list_hive_metastore_default, idle: 0.3478219509124756s, acqrelcnt: 1, lang: None, thrd: (80079, 11066404864), cmpt: ``, lut: 1710791934.98303
[0m16:58:55.331254 [debug] [ThreadPool]: Databricks adapter: conn: 6097381904: idle check connection: sess: 01eee561-f2f6-14df-a482-b4eed108dde7, name: list_hive_metastore_default, idle: 0.3481321334838867s, acqrelcnt: 1, lang: None, thrd: (80079, 11066404864), cmpt: ``, lut: 1710791934.98303
[0m16:58:55.331470 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m16:58:55.331623 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m16:58:55.331789 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m16:58:55.652155 [debug] [ThreadPool]: SQL status: OK in 0.3199999928474426 seconds
[0m16:58:55.658471 [debug] [ThreadPool]: Databricks adapter: conn: 6097381904: get_thread_connection: sess: 01eee561-f2f6-14df-a482-b4eed108dde7, name: list_hive_metastore_default, idle: 0.6753320693969727s, acqrelcnt: 1, lang: None, thrd: (80079, 11066404864), cmpt: ``, lut: 1710791934.98303
[0m16:58:55.658793 [debug] [ThreadPool]: Databricks adapter: conn: 6097381904: idle check connection: sess: 01eee561-f2f6-14df-a482-b4eed108dde7, name: list_hive_metastore_default, idle: 0.6756889820098877s, acqrelcnt: 1, lang: None, thrd: (80079, 11066404864), cmpt: ``, lut: 1710791934.98303
[0m16:58:55.659012 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m16:58:55.659235 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m16:58:55.977355 [debug] [ThreadPool]: SQL status: OK in 0.3199999928474426 seconds
[0m16:58:55.980166 [debug] [ThreadPool]: Databricks adapter: conn: 6097381904: _release sess: 01eee561-f2f6-14df-a482-b4eed108dde7, name: list_hive_metastore_default, idle: 3.0994415283203125e-06s, acqrelcnt: 0, lang: None, thrd: (80079, 11066404864), cmpt: ``, lut: 1710791935.98002
[0m16:58:55.983271 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '063d82ab-da43-4a7d-bd9c-c01ba8427d9d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16bb5fd90>]}
[0m16:58:55.983763 [debug] [MainThread]: Databricks adapter: conn: 6101798032: get_thread_connection: sess: None, name: master, idle: 1.8267347812652588s, acqrelcnt: 1, lang: None, thrd: (80079, 7965269056), cmpt: ``, lut: 1710791934.1569312
[0m16:58:55.984021 [debug] [MainThread]: Databricks adapter: conn: 6101798032: idle check connection: sess: None, name: master, idle: 1.8270189762115479s, acqrelcnt: 1, lang: None, thrd: (80079, 7965269056), cmpt: ``, lut: 1710791934.1569312
[0m16:58:55.984267 [debug] [MainThread]: Databricks adapter: conn: 6101798032: get_thread_connection: sess: None, name: master, idle: 1.8272666931152344s, acqrelcnt: 1, lang: None, thrd: (80079, 7965269056), cmpt: ``, lut: 1710791934.1569312
[0m16:58:55.984487 [debug] [MainThread]: Databricks adapter: conn: 6101798032: idle check connection: sess: None, name: master, idle: 1.8274917602539062s, acqrelcnt: 1, lang: None, thrd: (80079, 7965269056), cmpt: ``, lut: 1710791934.1569312
[0m16:58:55.984702 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:58:55.984901 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:58:55.985121 [debug] [MainThread]: Databricks adapter: conn: 6101798032: _release sess: None, name: master, idle: 9.5367431640625e-07s, acqrelcnt: 0, lang: None, thrd: (80079, 7965269056), cmpt: ``, lut: 1710791935.9850519
[0m16:58:55.985652 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m16:58:55.985911 [info ] [MainThread]: 
[0m16:58:55.988280 [debug] [Thread-1 (]: Began running node model.default.stage_users
[0m16:58:55.988627 [info ] [Thread-1 (]: 1 of 1 START sql view model default.stage_users ................................ [RUN]
[0m16:58:55.989201 [debug] [Thread-1 (]: Databricks adapter: conn: 6097381904: idle check connection: sess: 01eee561-f2f6-14df-a482-b4eed108dde7, name: list_hive_metastore_default, idle: 0.009071111679077148s, acqrelcnt: 0, lang: None, thrd: (80079, 11066404864), cmpt: ``, lut: 1710791935.98002
[0m16:58:55.989434 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.default.stage_users)
[0m16:58:55.989706 [debug] [Thread-1 (]: Databricks adapter: conn: 6097381904: reusing connection list_hive_metastore_default sess: 01eee561-f2f6-14df-a482-b4eed108dde7, name: model.default.stage_users, idle: 0.009582042694091797s, acqrelcnt: 0, lang: None, thrd: (80079, 11066404864), cmpt: ``, lut: 1710791935.98002
[0m16:58:55.989952 [debug] [Thread-1 (]: Databricks adapter: On thread (80079, 11066404864): `hive_metastore`.`default`.`stage_users` using default compute resource.
[0m16:58:55.990201 [debug] [Thread-1 (]: Databricks adapter: conn: 6097381904: _acquire sess: 01eee561-f2f6-14df-a482-b4eed108dde7, name: model.default.stage_users, idle: 0.010082006454467773s, acqrelcnt: 1, lang: sql, thrd: (80079, 11066404864), cmpt: ``, lut: 1710791935.98002
[0m16:58:55.990462 [debug] [Thread-1 (]: Began compiling node model.default.stage_users
[0m16:58:55.995697 [debug] [Thread-1 (]: Writing injected SQL for node "model.default.stage_users"
[0m16:58:55.997198 [debug] [Thread-1 (]: Timing info for model.default.stage_users (compile): 16:58:55.990611 => 16:58:55.997056
[0m16:58:55.997428 [debug] [Thread-1 (]: Began executing node model.default.stage_users
[0m16:58:56.013090 [debug] [Thread-1 (]: Writing runtime sql for node "model.default.stage_users"
[0m16:58:56.015023 [debug] [Thread-1 (]: Databricks adapter: conn: 6097381904: get_thread_connection: sess: 01eee561-f2f6-14df-a482-b4eed108dde7, name: model.default.stage_users, idle: 0.03486800193786621s, acqrelcnt: 1, lang: sql, thrd: (80079, 11066404864), cmpt: ``, lut: 1710791935.98002
[0m16:58:56.015309 [debug] [Thread-1 (]: Databricks adapter: conn: 6097381904: idle check connection: sess: 01eee561-f2f6-14df-a482-b4eed108dde7, name: model.default.stage_users, idle: 0.03520393371582031s, acqrelcnt: 1, lang: sql, thrd: (80079, 11066404864), cmpt: ``, lut: 1710791935.98002
[0m16:58:56.015479 [debug] [Thread-1 (]: Using databricks connection "model.default.stage_users"
[0m16:58:56.015682 [debug] [Thread-1 (]: On model.default.stage_users: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_users"} */
create or replace view `hive_metastore`.`default`.`stage_users`
  
  
  
  as
    

select user_id AS user_id,
       name AS name,
       city AS city,
       phone_number AS phone_number,
       gender AS gender,
       nationality AS nationality,
       state AS state
from `hive_metastore`.`default`.`users`

[0m16:58:57.358836 [debug] [Thread-1 (]: SQL status: OK in 1.340000033378601 seconds
[0m16:58:57.370004 [debug] [Thread-1 (]: Timing info for model.default.stage_users (execute): 16:58:55.997558 => 16:58:57.369802
[0m16:58:57.370495 [debug] [Thread-1 (]: Databricks adapter: conn: 6097381904: _release sess: 01eee561-f2f6-14df-a482-b4eed108dde7, name: model.default.stage_users, idle: 3.0994415283203125e-06s, acqrelcnt: 0, lang: sql, thrd: (80079, 11066404864), cmpt: ``, lut: 1710791937.370338
[0m16:58:57.371161 [debug] [Thread-1 (]: Databricks adapter: conn: 6097381904: _release sess: 01eee561-f2f6-14df-a482-b4eed108dde7, name: model.default.stage_users, idle: 9.5367431640625e-07s, acqrelcnt: 0, lang: sql, thrd: (80079, 11066404864), cmpt: ``, lut: 1710791937.371031
[0m16:58:57.371542 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '063d82ab-da43-4a7d-bd9c-c01ba8427d9d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16fd50710>]}
[0m16:58:57.372054 [info ] [Thread-1 (]: 1 of 1 OK created sql view model default.stage_users ........................... [[32mOK[0m in 1.38s]
[0m16:58:57.372530 [debug] [Thread-1 (]: Finished running node model.default.stage_users
[0m16:58:57.373773 [debug] [MainThread]: Databricks adapter: conn: 6101798032: idle check connection: sess: None, name: master, idle: 1.3886361122131348s, acqrelcnt: 0, lang: None, thrd: (80079, 7965269056), cmpt: ``, lut: 1710791935.9850519
[0m16:58:57.374049 [debug] [MainThread]: Databricks adapter: conn: 6101798032: reusing connection master sess: None, name: master, idle: 1.388923168182373s, acqrelcnt: 0, lang: None, thrd: (80079, 7965269056), cmpt: ``, lut: 1710791935.9850519
[0m16:58:57.374271 [debug] [MainThread]: Databricks adapter: Thread (80079, 7965269056) using default compute resource.
[0m16:58:57.374488 [debug] [MainThread]: Databricks adapter: conn: 6101798032: _acquire sess: None, name: master, idle: 1.3893721103668213s, acqrelcnt: 1, lang: None, thrd: (80079, 7965269056), cmpt: ``, lut: 1710791935.9850519
[0m16:58:57.374729 [debug] [MainThread]: Databricks adapter: conn: 6101798032: get_thread_connection: sess: None, name: master, idle: 1.3896081447601318s, acqrelcnt: 1, lang: None, thrd: (80079, 7965269056), cmpt: ``, lut: 1710791935.9850519
[0m16:58:57.374944 [debug] [MainThread]: Databricks adapter: conn: 6101798032: idle check connection: sess: None, name: master, idle: 1.389829158782959s, acqrelcnt: 1, lang: None, thrd: (80079, 7965269056), cmpt: ``, lut: 1710791935.9850519
[0m16:58:57.375148 [debug] [MainThread]: On master: ROLLBACK
[0m16:58:57.375346 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:58:57.851009 [debug] [MainThread]: Databricks adapter: conn: 6101798032: session opened sess: 01eee561-f4db-100d-ba96-1d4d31e8b0c2, name: master, idle: 1.0013580322265625e-05s, acqrelcnt: 1, lang: None, thrd: (80079, 7965269056), cmpt: ``, lut: 1710791937.850326
[0m16:58:57.852973 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:58:57.854080 [debug] [MainThread]: Databricks adapter: conn: 6101798032: get_thread_connection: sess: 01eee561-f4db-100d-ba96-1d4d31e8b0c2, name: master, idle: 0.0035660266876220703s, acqrelcnt: 1, lang: None, thrd: (80079, 7965269056), cmpt: ``, lut: 1710791937.850326
[0m16:58:57.854764 [debug] [MainThread]: Databricks adapter: conn: 6101798032: idle check connection: sess: 01eee561-f4db-100d-ba96-1d4d31e8b0c2, name: master, idle: 0.004312038421630859s, acqrelcnt: 1, lang: None, thrd: (80079, 7965269056), cmpt: ``, lut: 1710791937.850326
[0m16:58:57.855331 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m16:58:57.855898 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m16:58:57.856527 [debug] [MainThread]: Databricks adapter: conn: 6101798032: _release sess: 01eee561-f4db-100d-ba96-1d4d31e8b0c2, name: master, idle: 3.0994415283203125e-06s, acqrelcnt: 0, lang: None, thrd: (80079, 7965269056), cmpt: ``, lut: 1710791937.856423
[0m16:58:57.857830 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:58:57.858319 [debug] [MainThread]: On master: ROLLBACK
[0m16:58:57.858758 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:58:57.859133 [debug] [MainThread]: On master: Close
[0m16:58:58.035633 [debug] [MainThread]: Connection 'model.default.stage_users' was properly closed.
[0m16:58:58.036103 [debug] [MainThread]: On model.default.stage_users: ROLLBACK
[0m16:58:58.036415 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m16:58:58.036687 [debug] [MainThread]: On model.default.stage_users: Close
[0m16:58:58.214987 [info ] [MainThread]: 
[0m16:58:58.215591 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 4.06 seconds (4.06s).
[0m16:58:58.216241 [debug] [MainThread]: Command end result
[0m16:58:58.246918 [info ] [MainThread]: 
[0m16:58:58.247311 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:58:58.247549 [info ] [MainThread]: 
[0m16:58:58.247794 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m16:58:58.265877 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 5.9634924, "process_user_time": 2.239791, "process_kernel_time": 3.381899, "process_mem_max_rss": "222593024", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m16:58:58.266387 [debug] [MainThread]: Command `cli run` succeeded at 16:58:58.266293 after 5.96 seconds
[0m16:58:58.266713 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123592750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1235c1a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16cdb5410>]}
[0m16:58:58.266996 [debug] [MainThread]: Flushing usage events
[0m17:11:28.435286 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11197f450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1119fd8d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1119fdf10>]}


============================== 17:11:28.438121 | 31ba4e94-289f-4c58-98ca-e8b9a33b2e4b ==============================
[0m17:11:28.438121 [info ] [MainThread]: Running with dbt=1.7.8
[0m17:11:28.438408 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m17:11:29.965274 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '31ba4e94-289f-4c58-98ca-e8b9a33b2e4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x2a45f7250>]}
[0m17:11:29.994873 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '31ba4e94-289f-4c58-98ca-e8b9a33b2e4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x2a4558e90>]}
[0m17:11:29.995216 [info ] [MainThread]: Registered adapter: databricks=1.7.9
[0m17:11:30.016037 [debug] [MainThread]: checksum: 67f0013ca5f0bd43af9a0873dd50792fde83ef69de63b71cacd0b4ac656c52e5, vars: {}, profile: , target: , version: 1.7.8
[0m17:11:30.103057 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m17:11:30.103418 [debug] [MainThread]: Partial parsing: updated file: default://models/stage/stage_rides.sql
[0m17:11:30.154731 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '31ba4e94-289f-4c58-98ca-e8b9a33b2e4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16da49e90>]}
[0m17:11:30.161608 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '31ba4e94-289f-4c58-98ca-e8b9a33b2e4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16d934c90>]}
[0m17:11:30.161860 [info ] [MainThread]: Found 3 models, 3 sources, 0 exposures, 0 metrics, 538 macros, 0 groups, 0 semantic models
[0m17:11:30.162047 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '31ba4e94-289f-4c58-98ca-e8b9a33b2e4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16da3aed0>]}
[0m17:11:30.162718 [info ] [MainThread]: 
[0m17:11:30.163166 [debug] [MainThread]: Databricks adapter: conn: 6134155408: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (84082, 7965269056), cmpt: ``, lut: None
[0m17:11:30.163313 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:11:30.163455 [debug] [MainThread]: Databricks adapter: Thread (84082, 7965269056) using default compute resource.
[0m17:11:30.163584 [debug] [MainThread]: Databricks adapter: conn: 6134155408: _acquire sess: None, name: master, idle: 7.152557373046875e-07s, acqrelcnt: 1, lang: None, thrd: (84082, 7965269056), cmpt: ``, lut: 1710792690.163546
[0m17:11:30.164111 [debug] [ThreadPool]: Databricks adapter: conn: 6133282768: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore, idle: 0s, acqrelcnt: 0, lang: None, thrd: (84082, 11439599616), cmpt: ``, lut: None
[0m17:11:30.164304 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m17:11:30.164430 [debug] [ThreadPool]: Databricks adapter: Thread (84082, 11439599616) using default compute resource.
[0m17:11:30.164551 [debug] [ThreadPool]: Databricks adapter: conn: 6133282768: _acquire sess: None, name: list_hive_metastore, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (84082, 11439599616), cmpt: ``, lut: 1710792690.164513
[0m17:11:30.164688 [debug] [ThreadPool]: Databricks adapter: conn: 6133282768: get_thread_connection: sess: None, name: list_hive_metastore, idle: 0.00013685226440429688s, acqrelcnt: 1, lang: None, thrd: (84082, 11439599616), cmpt: ``, lut: 1710792690.164513
[0m17:11:30.164809 [debug] [ThreadPool]: Databricks adapter: conn: 6133282768: idle check connection: sess: None, name: list_hive_metastore, idle: 0.0002589225769042969s, acqrelcnt: 1, lang: None, thrd: (84082, 11439599616), cmpt: ``, lut: 1710792690.164513
[0m17:11:30.164916 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m17:11:30.165029 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m17:11:30.165142 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:11:30.787444 [debug] [ThreadPool]: Databricks adapter: conn: 6133282768: session opened sess: 01eee563-b598-1573-b0c5-dbe71bd6e95c, name: list_hive_metastore, idle: 2.9087066650390625e-05s, acqrelcnt: 1, lang: None, thrd: (84082, 11439599616), cmpt: ``, lut: 1710792690.786999
[0m17:11:40.621326 [debug] [ThreadPool]: SQL status: OK in 10.460000038146973 seconds
[0m17:11:40.633360 [debug] [ThreadPool]: Databricks adapter: conn: 6133282768: _release sess: 01eee563-b598-1573-b0c5-dbe71bd6e95c, name: list_hive_metastore, idle: 5.9604644775390625e-06s, acqrelcnt: 0, lang: None, thrd: (84082, 11439599616), cmpt: ``, lut: 1710792700.633203
[0m17:11:40.635110 [debug] [ThreadPool]: Databricks adapter: conn: 6133282768: idle check connection: sess: 01eee563-b598-1573-b0c5-dbe71bd6e95c, name: list_hive_metastore, idle: 0.0018079280853271484s, acqrelcnt: 0, lang: None, thrd: (84082, 11439599616), cmpt: ``, lut: 1710792700.633203
[0m17:11:40.635451 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now list_hive_metastore_default)
[0m17:11:40.635725 [debug] [ThreadPool]: Databricks adapter: conn: 6133282768: reusing connection list_hive_metastore sess: 01eee563-b598-1573-b0c5-dbe71bd6e95c, name: list_hive_metastore_default, idle: 0.002441883087158203s, acqrelcnt: 0, lang: None, thrd: (84082, 11439599616), cmpt: ``, lut: 1710792700.633203
[0m17:11:40.635945 [debug] [ThreadPool]: Databricks adapter: Thread (84082, 11439599616) using default compute resource.
[0m17:11:40.636152 [debug] [ThreadPool]: Databricks adapter: conn: 6133282768: _acquire sess: 01eee563-b598-1573-b0c5-dbe71bd6e95c, name: list_hive_metastore_default, idle: 0.002886056900024414s, acqrelcnt: 1, lang: None, thrd: (84082, 11439599616), cmpt: ``, lut: 1710792700.633203
[0m17:11:40.640132 [debug] [ThreadPool]: Databricks adapter: conn: 6133282768: get_thread_connection: sess: 01eee563-b598-1573-b0c5-dbe71bd6e95c, name: list_hive_metastore_default, idle: 0.006846904754638672s, acqrelcnt: 1, lang: None, thrd: (84082, 11439599616), cmpt: ``, lut: 1710792700.633203
[0m17:11:40.640399 [debug] [ThreadPool]: Databricks adapter: conn: 6133282768: idle check connection: sess: 01eee563-b598-1573-b0c5-dbe71bd6e95c, name: list_hive_metastore_default, idle: 0.00712275505065918s, acqrelcnt: 1, lang: None, thrd: (84082, 11439599616), cmpt: ``, lut: 1710792700.633203
[0m17:11:40.640610 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:11:40.640816 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m17:11:41.067200 [debug] [ThreadPool]: SQL status: OK in 0.4300000071525574 seconds
[0m17:11:41.091771 [debug] [ThreadPool]: Databricks adapter: conn: 6133282768: get_thread_connection: sess: 01eee563-b598-1573-b0c5-dbe71bd6e95c, name: list_hive_metastore_default, idle: 0.45835304260253906s, acqrelcnt: 1, lang: None, thrd: (84082, 11439599616), cmpt: ``, lut: 1710792700.633203
[0m17:11:41.092442 [debug] [ThreadPool]: Databricks adapter: conn: 6133282768: idle check connection: sess: 01eee563-b598-1573-b0c5-dbe71bd6e95c, name: list_hive_metastore_default, idle: 0.4591221809387207s, acqrelcnt: 1, lang: None, thrd: (84082, 11439599616), cmpt: ``, lut: 1710792700.633203
[0m17:11:41.092893 [debug] [ThreadPool]: Databricks adapter: conn: 6133282768: get_thread_connection: sess: 01eee563-b598-1573-b0c5-dbe71bd6e95c, name: list_hive_metastore_default, idle: 0.45958495140075684s, acqrelcnt: 1, lang: None, thrd: (84082, 11439599616), cmpt: ``, lut: 1710792700.633203
[0m17:11:41.093354 [debug] [ThreadPool]: Databricks adapter: conn: 6133282768: idle check connection: sess: 01eee563-b598-1573-b0c5-dbe71bd6e95c, name: list_hive_metastore_default, idle: 0.4600639343261719s, acqrelcnt: 1, lang: None, thrd: (84082, 11439599616), cmpt: ``, lut: 1710792700.633203
[0m17:11:41.094123 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:11:41.094388 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:11:41.094563 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m17:11:42.166075 [debug] [ThreadPool]: SQL status: OK in 1.0700000524520874 seconds
[0m17:11:42.185064 [debug] [ThreadPool]: Databricks adapter: conn: 6133282768: get_thread_connection: sess: 01eee563-b598-1573-b0c5-dbe71bd6e95c, name: list_hive_metastore_default, idle: 1.5516059398651123s, acqrelcnt: 1, lang: None, thrd: (84082, 11439599616), cmpt: ``, lut: 1710792700.633203
[0m17:11:42.185959 [debug] [ThreadPool]: Databricks adapter: conn: 6133282768: idle check connection: sess: 01eee563-b598-1573-b0c5-dbe71bd6e95c, name: list_hive_metastore_default, idle: 1.5526010990142822s, acqrelcnt: 1, lang: None, thrd: (84082, 11439599616), cmpt: ``, lut: 1710792700.633203
[0m17:11:42.186586 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:11:42.187113 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m17:11:42.836977 [debug] [ThreadPool]: SQL status: OK in 0.6499999761581421 seconds
[0m17:11:42.845575 [debug] [ThreadPool]: Databricks adapter: conn: 6133282768: _release sess: 01eee563-b598-1573-b0c5-dbe71bd6e95c, name: list_hive_metastore_default, idle: 8.821487426757812e-06s, acqrelcnt: 0, lang: None, thrd: (84082, 11439599616), cmpt: ``, lut: 1710792702.845226
[0m17:11:42.851187 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '31ba4e94-289f-4c58-98ca-e8b9a33b2e4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x2a465c750>]}
[0m17:11:42.852105 [debug] [MainThread]: Databricks adapter: conn: 6134155408: get_thread_connection: sess: None, name: master, idle: 12.688409805297852s, acqrelcnt: 1, lang: None, thrd: (84082, 7965269056), cmpt: ``, lut: 1710792690.163546
[0m17:11:42.852796 [debug] [MainThread]: Databricks adapter: conn: 6134155408: idle check connection: sess: None, name: master, idle: 12.689074993133545s, acqrelcnt: 1, lang: None, thrd: (84082, 7965269056), cmpt: ``, lut: 1710792690.163546
[0m17:11:42.853323 [debug] [MainThread]: Databricks adapter: conn: 6134155408: get_thread_connection: sess: None, name: master, idle: 12.68966794013977s, acqrelcnt: 1, lang: None, thrd: (84082, 7965269056), cmpt: ``, lut: 1710792690.163546
[0m17:11:42.853748 [debug] [MainThread]: Databricks adapter: conn: 6134155408: idle check connection: sess: None, name: master, idle: 12.690103769302368s, acqrelcnt: 1, lang: None, thrd: (84082, 7965269056), cmpt: ``, lut: 1710792690.163546
[0m17:11:42.854149 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:11:42.854524 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:11:42.854921 [debug] [MainThread]: Databricks adapter: conn: 6134155408: _release sess: None, name: master, idle: 1.9073486328125e-06s, acqrelcnt: 0, lang: None, thrd: (84082, 7965269056), cmpt: ``, lut: 1710792702.854822
[0m17:11:42.856009 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:11:42.856536 [info ] [MainThread]: 
[0m17:11:42.865791 [debug] [Thread-1 (]: Began running node model.default.stage_rides
[0m17:11:42.866481 [info ] [Thread-1 (]: 1 of 1 START sql view model default.stage_rides ................................ [RUN]
[0m17:11:42.867423 [debug] [Thread-1 (]: Databricks adapter: conn: 6133282768: idle check connection: sess: 01eee563-b598-1573-b0c5-dbe71bd6e95c, name: list_hive_metastore_default, idle: 0.022024869918823242s, acqrelcnt: 0, lang: None, thrd: (84082, 11439599616), cmpt: ``, lut: 1710792702.845226
[0m17:11:42.867761 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.default.stage_rides)
[0m17:11:42.868112 [debug] [Thread-1 (]: Databricks adapter: conn: 6133282768: reusing connection list_hive_metastore_default sess: 01eee563-b598-1573-b0c5-dbe71bd6e95c, name: model.default.stage_rides, idle: 0.022747039794921875s, acqrelcnt: 0, lang: None, thrd: (84082, 11439599616), cmpt: ``, lut: 1710792702.845226
[0m17:11:42.868448 [debug] [Thread-1 (]: Databricks adapter: On thread (84082, 11439599616): `hive_metastore`.`default`.`stage_rides` using default compute resource.
[0m17:11:42.868776 [debug] [Thread-1 (]: Databricks adapter: conn: 6133282768: _acquire sess: 01eee563-b598-1573-b0c5-dbe71bd6e95c, name: model.default.stage_rides, idle: 0.02341914176940918s, acqrelcnt: 1, lang: sql, thrd: (84082, 11439599616), cmpt: ``, lut: 1710792702.845226
[0m17:11:42.869097 [debug] [Thread-1 (]: Began compiling node model.default.stage_rides
[0m17:11:42.876294 [debug] [Thread-1 (]: Writing injected SQL for node "model.default.stage_rides"
[0m17:11:42.878989 [debug] [Thread-1 (]: Timing info for model.default.stage_rides (compile): 17:11:42.869302 => 17:11:42.878796
[0m17:11:42.879287 [debug] [Thread-1 (]: Began executing node model.default.stage_rides
[0m17:11:42.900663 [debug] [Thread-1 (]: Writing runtime sql for node "model.default.stage_rides"
[0m17:11:42.902671 [debug] [Thread-1 (]: Databricks adapter: conn: 6133282768: get_thread_connection: sess: 01eee563-b598-1573-b0c5-dbe71bd6e95c, name: model.default.stage_rides, idle: 0.0572967529296875s, acqrelcnt: 1, lang: sql, thrd: (84082, 11439599616), cmpt: ``, lut: 1710792702.845226
[0m17:11:42.902966 [debug] [Thread-1 (]: Databricks adapter: conn: 6133282768: idle check connection: sess: 01eee563-b598-1573-b0c5-dbe71bd6e95c, name: model.default.stage_rides, idle: 0.057640790939331055s, acqrelcnt: 1, lang: sql, thrd: (84082, 11439599616), cmpt: ``, lut: 1710792702.845226
[0m17:11:42.903173 [debug] [Thread-1 (]: Using databricks connection "model.default.stage_rides"
[0m17:11:42.903442 [debug] [Thread-1 (]: On model.default.stage_rides: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_rides"} */
create or replace view `hive_metastore`.`default`.`stage_rides`
  
  
  
  as
    

select id AS id,
       user_id AS user_id,
       payment_id AS payment_id,
       name AS type,
       cab_type AS cab_type,
       source AS source,
       destination AS destination,
       distance AS distance_in_miles,
       price AS price_in_dollars,
       surge_multiplier AS dynamic_fare
       
from `hive_metastore`.`default`.`rides`

[0m17:11:44.229573 [debug] [Thread-1 (]: SQL status: OK in 1.3300000429153442 seconds
[0m17:11:44.234045 [debug] [Thread-1 (]: Timing info for model.default.stage_rides (execute): 17:11:42.879464 => 17:11:44.233946
[0m17:11:44.234259 [debug] [Thread-1 (]: Databricks adapter: conn: 6133282768: _release sess: 01eee563-b598-1573-b0c5-dbe71bd6e95c, name: model.default.stage_rides, idle: 9.5367431640625e-07s, acqrelcnt: 0, lang: sql, thrd: (84082, 11439599616), cmpt: ``, lut: 1710792704.234184
[0m17:11:44.234572 [debug] [Thread-1 (]: Databricks adapter: conn: 6133282768: _release sess: 01eee563-b598-1573-b0c5-dbe71bd6e95c, name: model.default.stage_rides, idle: 9.5367431640625e-07s, acqrelcnt: 0, lang: sql, thrd: (84082, 11439599616), cmpt: ``, lut: 1710792704.2345119
[0m17:11:44.234753 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '31ba4e94-289f-4c58-98ca-e8b9a33b2e4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x2a4720d10>]}
[0m17:11:44.234986 [info ] [Thread-1 (]: 1 of 1 OK created sql view model default.stage_rides ........................... [[32mOK[0m in 1.37s]
[0m17:11:44.235216 [debug] [Thread-1 (]: Finished running node model.default.stage_rides
[0m17:11:44.236060 [debug] [MainThread]: Databricks adapter: conn: 6134155408: idle check connection: sess: None, name: master, idle: 1.3811519145965576s, acqrelcnt: 0, lang: None, thrd: (84082, 7965269056), cmpt: ``, lut: 1710792702.854822
[0m17:11:44.236304 [debug] [MainThread]: Databricks adapter: conn: 6134155408: reusing connection master sess: None, name: master, idle: 1.3814430236816406s, acqrelcnt: 0, lang: None, thrd: (84082, 7965269056), cmpt: ``, lut: 1710792702.854822
[0m17:11:44.236433 [debug] [MainThread]: Databricks adapter: Thread (84082, 7965269056) using default compute resource.
[0m17:11:44.236552 [debug] [MainThread]: Databricks adapter: conn: 6134155408: _acquire sess: None, name: master, idle: 1.3816959857940674s, acqrelcnt: 1, lang: None, thrd: (84082, 7965269056), cmpt: ``, lut: 1710792702.854822
[0m17:11:44.236683 [debug] [MainThread]: Databricks adapter: conn: 6134155408: get_thread_connection: sess: None, name: master, idle: 1.3818271160125732s, acqrelcnt: 1, lang: None, thrd: (84082, 7965269056), cmpt: ``, lut: 1710792702.854822
[0m17:11:44.236817 [debug] [MainThread]: Databricks adapter: conn: 6134155408: idle check connection: sess: None, name: master, idle: 1.381960153579712s, acqrelcnt: 1, lang: None, thrd: (84082, 7965269056), cmpt: ``, lut: 1710792702.854822
[0m17:11:44.236930 [debug] [MainThread]: On master: ROLLBACK
[0m17:11:44.237032 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:11:44.707874 [debug] [MainThread]: Databricks adapter: conn: 6134155408: session opened sess: 01eee563-bdef-149a-a3e8-d21c8da45cbe, name: master, idle: 1.0967254638671875e-05s, acqrelcnt: 1, lang: None, thrd: (84082, 7965269056), cmpt: ``, lut: 1710792704.707463
[0m17:11:44.709270 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:11:44.710204 [debug] [MainThread]: Databricks adapter: conn: 6134155408: get_thread_connection: sess: 01eee563-bdef-149a-a3e8-d21c8da45cbe, name: master, idle: 0.0023851394653320312s, acqrelcnt: 1, lang: None, thrd: (84082, 7965269056), cmpt: ``, lut: 1710792704.707463
[0m17:11:44.711026 [debug] [MainThread]: Databricks adapter: conn: 6134155408: idle check connection: sess: 01eee563-bdef-149a-a3e8-d21c8da45cbe, name: master, idle: 0.003407001495361328s, acqrelcnt: 1, lang: None, thrd: (84082, 7965269056), cmpt: ``, lut: 1710792704.707463
[0m17:11:44.712176 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:11:44.712821 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:11:44.713527 [debug] [MainThread]: Databricks adapter: conn: 6134155408: _release sess: 01eee563-bdef-149a-a3e8-d21c8da45cbe, name: master, idle: 3.814697265625e-06s, acqrelcnt: 0, lang: None, thrd: (84082, 7965269056), cmpt: ``, lut: 1710792704.7133062
[0m17:11:44.715239 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:11:44.716061 [debug] [MainThread]: On master: ROLLBACK
[0m17:11:44.716542 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:11:44.717006 [debug] [MainThread]: On master: Close
[0m17:11:44.883675 [debug] [MainThread]: Connection 'model.default.stage_rides' was properly closed.
[0m17:11:44.884882 [debug] [MainThread]: On model.default.stage_rides: ROLLBACK
[0m17:11:44.885692 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:11:44.886578 [debug] [MainThread]: On model.default.stage_rides: Close
[0m17:11:45.075182 [info ] [MainThread]: 
[0m17:11:45.076690 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 14.91 seconds (14.91s).
[0m17:11:45.078171 [debug] [MainThread]: Command end result
[0m17:11:45.120776 [info ] [MainThread]: 
[0m17:11:45.121159 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:11:45.121457 [info ] [MainThread]: 
[0m17:11:45.121730 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m17:11:45.127124 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 16.719831, "process_user_time": 2.246323, "process_kernel_time": 3.434912, "process_mem_max_rss": "224083968", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m17:11:45.127644 [debug] [MainThread]: Command `cli run` succeeded at 17:11:45.127549 after 16.72 seconds
[0m17:11:45.127947 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1111f9990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102b0fcd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111960910>]}
[0m17:11:45.128245 [debug] [MainThread]: Flushing usage events
[0m17:18:05.859481 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103f39160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1213f4520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1214360a0>]}


============================== 17:18:05.897004 | eb6a44fa-d445-42ed-9619-5277d75e334f ==============================
[0m17:18:05.897004 [info ] [MainThread]: Running with dbt=1.7.7
[0m17:18:05.897449 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt show --select stage_payments.sql', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m17:18:11.852783 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'eb6a44fa-d445-42ed-9619-5277d75e334f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121436eb0>]}
[0m17:18:11.895479 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'eb6a44fa-d445-42ed-9619-5277d75e334f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1369b46a0>]}
[0m17:18:11.895839 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m17:18:11.923803 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m17:18:11.932488 [info ] [MainThread]: Unable to do partial parsing because of a version mismatch
[0m17:18:11.932746 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'eb6a44fa-d445-42ed-9619-5277d75e334f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x136c1c0d0>]}
[0m17:18:13.336396 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'eb6a44fa-d445-42ed-9619-5277d75e334f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x136cfcc10>]}
[0m17:18:13.351180 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'eb6a44fa-d445-42ed-9619-5277d75e334f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x136c0ca90>]}
[0m17:18:13.351426 [info ] [MainThread]: Found 3 models, 3 sources, 0 exposures, 0 metrics, 535 macros, 0 groups, 0 semantic models
[0m17:18:13.351583 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'eb6a44fa-d445-42ed-9619-5277d75e334f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x136c6f5b0>]}
[0m17:18:13.352232 [info ] [MainThread]: 
[0m17:18:13.352632 [debug] [MainThread]: Databricks adapter: conn: 5213570672: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (85582, 7965269056), cmpt: ``, lut: None
[0m17:18:13.352786 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:18:13.352922 [debug] [MainThread]: Databricks adapter: Thread (85582, 7965269056) using default compute resource.
[0m17:18:13.353062 [debug] [MainThread]: Databricks adapter: conn: 5213570672: _acquire sess: None, name: master, idle: 1.9073486328125e-06s, acqrelcnt: 1, lang: None, thrd: (85582, 7965269056), cmpt: ``, lut: 1710793093.353017
[0m17:18:13.353628 [debug] [ThreadPool]: Databricks adapter: conn: 5215742512: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore_default, idle: 0s, acqrelcnt: 0, lang: None, thrd: (85582, 10955550720), cmpt: ``, lut: None
[0m17:18:13.353841 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_default'
[0m17:18:13.353993 [debug] [ThreadPool]: Databricks adapter: Thread (85582, 10955550720) using default compute resource.
[0m17:18:13.354137 [debug] [ThreadPool]: Databricks adapter: conn: 5215742512: _acquire sess: None, name: list_hive_metastore_default, idle: 1.1920928955078125e-06s, acqrelcnt: 1, lang: None, thrd: (85582, 10955550720), cmpt: ``, lut: 1710793093.354094
[0m17:18:13.356528 [debug] [ThreadPool]: Databricks adapter: conn: 5215742512: get_thread_connection: sess: None, name: list_hive_metastore_default, idle: 0.0023839473724365234s, acqrelcnt: 1, lang: None, thrd: (85582, 10955550720), cmpt: ``, lut: 1710793093.354094
[0m17:18:13.356687 [debug] [ThreadPool]: Databricks adapter: conn: 5215742512: idle check connection: sess: None, name: list_hive_metastore_default, idle: 0.0025517940521240234s, acqrelcnt: 1, lang: None, thrd: (85582, 10955550720), cmpt: ``, lut: 1710793093.354094
[0m17:18:13.356819 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:18:13.356951 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m17:18:13.357100 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:18:14.008596 [debug] [ThreadPool]: Databricks adapter: conn: 5215742512: session opened sess: 01eee564-a5fa-10c6-adc8-9ef8348c5825, name: list_hive_metastore_default, idle: 3.814697265625e-06s, acqrelcnt: 1, lang: None, thrd: (85582, 10955550720), cmpt: ``, lut: 1710793094.008409
[0m17:18:14.358357 [debug] [ThreadPool]: SQL status: OK in 1.0 seconds
[0m17:18:14.374001 [debug] [ThreadPool]: Databricks adapter: conn: 5215742512: get_thread_connection: sess: 01eee564-a5fa-10c6-adc8-9ef8348c5825, name: list_hive_metastore_default, idle: 0.36542606353759766s, acqrelcnt: 1, lang: None, thrd: (85582, 10955550720), cmpt: ``, lut: 1710793094.008409
[0m17:18:14.374470 [debug] [ThreadPool]: Databricks adapter: conn: 5215742512: idle check connection: sess: 01eee564-a5fa-10c6-adc8-9ef8348c5825, name: list_hive_metastore_default, idle: 0.3659830093383789s, acqrelcnt: 1, lang: None, thrd: (85582, 10955550720), cmpt: ``, lut: 1710793094.008409
[0m17:18:14.374733 [debug] [ThreadPool]: Databricks adapter: conn: 5215742512: get_thread_connection: sess: 01eee564-a5fa-10c6-adc8-9ef8348c5825, name: list_hive_metastore_default, idle: 0.36625099182128906s, acqrelcnt: 1, lang: None, thrd: (85582, 10955550720), cmpt: ``, lut: 1710793094.008409
[0m17:18:14.374972 [debug] [ThreadPool]: Databricks adapter: conn: 5215742512: idle check connection: sess: 01eee564-a5fa-10c6-adc8-9ef8348c5825, name: list_hive_metastore_default, idle: 0.3664970397949219s, acqrelcnt: 1, lang: None, thrd: (85582, 10955550720), cmpt: ``, lut: 1710793094.008409
[0m17:18:14.375201 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:18:14.375414 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:18:14.375655 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m17:18:14.655195 [debug] [ThreadPool]: SQL status: OK in 0.2800000011920929 seconds
[0m17:18:14.661816 [debug] [ThreadPool]: Databricks adapter: conn: 5215742512: get_thread_connection: sess: 01eee564-a5fa-10c6-adc8-9ef8348c5825, name: list_hive_metastore_default, idle: 0.653296947479248s, acqrelcnt: 1, lang: None, thrd: (85582, 10955550720), cmpt: ``, lut: 1710793094.008409
[0m17:18:14.662129 [debug] [ThreadPool]: Databricks adapter: conn: 5215742512: idle check connection: sess: 01eee564-a5fa-10c6-adc8-9ef8348c5825, name: list_hive_metastore_default, idle: 0.6536519527435303s, acqrelcnt: 1, lang: None, thrd: (85582, 10955550720), cmpt: ``, lut: 1710793094.008409
[0m17:18:14.662347 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:18:14.662566 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m17:18:14.969796 [debug] [ThreadPool]: SQL status: OK in 0.3100000023841858 seconds
[0m17:18:14.972596 [debug] [ThreadPool]: Databricks adapter: conn: 5215742512: _release sess: 01eee564-a5fa-10c6-adc8-9ef8348c5825, name: list_hive_metastore_default, idle: 4.0531158447265625e-06s, acqrelcnt: 0, lang: None, thrd: (85582, 10955550720), cmpt: ``, lut: 1710793094.972512
[0m17:18:14.974553 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'eb6a44fa-d445-42ed-9619-5277d75e334f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x136ceafa0>]}
[0m17:18:14.974796 [debug] [MainThread]: Databricks adapter: conn: 5213570672: _release sess: None, name: master, idle: 9.5367431640625e-07s, acqrelcnt: 0, lang: None, thrd: (85582, 7965269056), cmpt: ``, lut: 1710793094.974747
[0m17:18:14.975114 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:18:14.975274 [info ] [MainThread]: 
[0m17:18:14.982365 [debug] [Thread-1  ]: Began running node model.default.stage_payments
[0m17:18:14.982808 [debug] [Thread-1  ]: Databricks adapter: conn: 5215742512: idle check connection: sess: 01eee564-a5fa-10c6-adc8-9ef8348c5825, name: list_hive_metastore_default, idle: 0.010214805603027344s, acqrelcnt: 0, lang: None, thrd: (85582, 10955550720), cmpt: ``, lut: 1710793094.972512
[0m17:18:14.983005 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.default.stage_payments)
[0m17:18:14.983226 [debug] [Thread-1  ]: Databricks adapter: conn: 5215742512: reusing connection list_hive_metastore_default sess: 01eee564-a5fa-10c6-adc8-9ef8348c5825, name: model.default.stage_payments, idle: 0.010629892349243164s, acqrelcnt: 0, lang: None, thrd: (85582, 10955550720), cmpt: ``, lut: 1710793094.972512
[0m17:18:14.983409 [debug] [Thread-1  ]: Databricks adapter: On thread (85582, 10955550720): `hive_metastore`.`default`.`stage_payments` using default compute resource.
[0m17:18:14.983576 [debug] [Thread-1  ]: Databricks adapter: conn: 5215742512: _acquire sess: 01eee564-a5fa-10c6-adc8-9ef8348c5825, name: model.default.stage_payments, idle: 0.010995864868164062s, acqrelcnt: 1, lang: sql, thrd: (85582, 10955550720), cmpt: ``, lut: 1710793094.972512
[0m17:18:14.983903 [debug] [Thread-1  ]: Began compiling node model.default.stage_payments
[0m17:18:14.989758 [debug] [Thread-1  ]: Writing injected SQL for node "model.default.stage_payments"
[0m17:18:14.991030 [debug] [Thread-1  ]: Timing info for model.default.stage_payments (compile): 17:18:14.984010 => 17:18:14.990908
[0m17:18:14.991216 [debug] [Thread-1  ]: Began executing node model.default.stage_payments
[0m17:18:14.995458 [debug] [Thread-1  ]: Databricks adapter: conn: 5215742512: get_thread_connection: sess: 01eee564-a5fa-10c6-adc8-9ef8348c5825, name: model.default.stage_payments, idle: 0.02284097671508789s, acqrelcnt: 1, lang: sql, thrd: (85582, 10955550720), cmpt: ``, lut: 1710793094.972512
[0m17:18:14.995743 [debug] [Thread-1  ]: Databricks adapter: conn: 5215742512: idle check connection: sess: 01eee564-a5fa-10c6-adc8-9ef8348c5825, name: model.default.stage_payments, idle: 0.02315497398376465s, acqrelcnt: 1, lang: sql, thrd: (85582, 10955550720), cmpt: ``, lut: 1710793094.972512
[0m17:18:14.995901 [debug] [Thread-1  ]: Using databricks connection "model.default.stage_payments"
[0m17:18:14.996097 [debug] [Thread-1  ]: On model.default.stage_payments: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_payments"} */

  
    select *
    from (
        

select user_id AS user_id,
       name AS name,
       city AS city,
       phone_number AS phone_number,
       gender AS gender,
       nationality AS nationality,
       state AS state
from `hive_metastore`.`default`.`payments`
    ) as model_limit_subq
    limit 5


[0m17:18:15.530054 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_payments"} */

  
    select *
    from (
        

select user_id AS user_id,
       name AS name,
       city AS city,
       phone_number AS phone_number,
       gender AS gender,
       nationality AS nationality,
       state AS state
from `hive_metastore`.`default`.`payments`
    ) as model_limit_subq
    limit 5


[0m17:18:15.530695 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `name` cannot be resolved. Did you mean one of the following? [`race`, `time`, `ssn`, `city`, `datetime`].; line 9 pos 7
[0m17:18:15.531421 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `name` cannot be resolved. Did you mean one of the following? [`race`, `time`, `ssn`, `city`, `datetime`].; line 9 pos 7
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:697)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:574)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:423)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:65)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:65)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:401)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:386)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:435)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `name` cannot be resolved. Did you mean one of the following? [`race`, `time`, `ssn`, `city`, `datetime`].; line 9 pos 7
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:81)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:674)
	... 36 more

[0m17:18:15.532098 [debug] [Thread-1  ]: Databricks adapter: operation-id: 01eee564-a6ab-16f9-ac85-84d1c0e512cf
[0m17:18:15.532649 [debug] [Thread-1  ]: Timing info for model.default.stage_payments (execute): 17:18:14.991321 => 17:18:15.532400
[0m17:18:15.533110 [debug] [Thread-1  ]: Databricks adapter: conn: 5215742512: _release sess: 01eee564-a5fa-10c6-adc8-9ef8348c5825, name: model.default.stage_payments, idle: 4.0531158447265625e-06s, acqrelcnt: 0, lang: sql, thrd: (85582, 10955550720), cmpt: ``, lut: 1710793095.532939
[0m17:18:15.599174 [debug] [Thread-1  ]: Runtime Error in model stage_payments (models/stage/stage_payments.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `name` cannot be resolved. Did you mean one of the following? [`race`, `time`, `ssn`, `city`, `datetime`].; line 9 pos 7
[0m17:18:15.599576 [debug] [Thread-1  ]: Databricks adapter: conn: 5215742512: _release sess: 01eee564-a5fa-10c6-adc8-9ef8348c5825, name: model.default.stage_payments, idle: 2.1457672119140625e-06s, acqrelcnt: 0, lang: sql, thrd: (85582, 10955550720), cmpt: ``, lut: 1710793095.5994618
[0m17:18:15.599972 [debug] [Thread-1  ]: Finished running node model.default.stage_payments
[0m17:18:15.600310 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:18:15.600507 [debug] [MainThread]: Connection 'model.default.stage_payments' was properly closed.
[0m17:18:15.600709 [debug] [MainThread]: On model.default.stage_payments: ROLLBACK
[0m17:18:15.601005 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:18:15.601275 [debug] [MainThread]: On model.default.stage_payments: Close
[0m17:18:15.775735 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error in model stage_payments (models/stage/stage_payments.sql)
    [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `name` cannot be resolved. Did you mean one of the following? [`race`, `time`, `ssn`, `city`, `datetime`].; line 9 pos 7
[0m17:18:15.794267 [debug] [MainThread]: Resource report: {"command_name": "show", "command_wall_clock_time": 9.998506, "process_user_time": 4.356028, "process_kernel_time": 4.254268, "process_mem_max_rss": "209190912", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m17:18:15.794847 [debug] [MainThread]: Command `dbt show` failed at 17:18:15.794728 after 10.00 seconds
[0m17:18:15.795228 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103f39160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x136f2f250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x136cfc160>]}
[0m17:18:15.795592 [debug] [MainThread]: Flushing usage events
[0m17:18:25.691194 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103b9a160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107534520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075760a0>]}


============================== 17:18:25.694418 | 7a2a7046-3724-4190-959c-39a7f6451f70 ==============================
[0m17:18:25.694418 [info ] [MainThread]: Running with dbt=1.7.7
[0m17:18:25.694692 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt show --select stage_payments.sql', 'send_anonymous_usage_stats': 'True'}
[0m17:18:26.640255 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7a2a7046-3724-4190-959c-39a7f6451f70', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107576eb0>]}
[0m17:18:26.681994 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7a2a7046-3724-4190-959c-39a7f6451f70', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x2a0f8fe20>]}
[0m17:18:26.682310 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m17:18:26.698351 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m17:18:26.756996 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m17:18:26.757415 [debug] [MainThread]: Partial parsing: updated file: default://models/stage/stage_payments.sql
[0m17:18:26.826495 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7a2a7046-3724-4190-959c-39a7f6451f70', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x2a134bd30>]}
[0m17:18:26.841660 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7a2a7046-3724-4190-959c-39a7f6451f70', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16f29dd90>]}
[0m17:18:26.841930 [info ] [MainThread]: Found 3 models, 3 sources, 0 exposures, 0 metrics, 535 macros, 0 groups, 0 semantic models
[0m17:18:26.842118 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7a2a7046-3724-4190-959c-39a7f6451f70', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x2a103d190>]}
[0m17:18:26.842847 [info ] [MainThread]: 
[0m17:18:26.843300 [debug] [MainThread]: Databricks adapter: conn: 11291921616: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (85666, 7965269056), cmpt: ``, lut: None
[0m17:18:26.843450 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:18:26.843594 [debug] [MainThread]: Databricks adapter: Thread (85666, 7965269056) using default compute resource.
[0m17:18:26.843737 [debug] [MainThread]: Databricks adapter: conn: 11291921616: _acquire sess: None, name: master, idle: 2.1457672119140625e-06s, acqrelcnt: 1, lang: None, thrd: (85666, 7965269056), cmpt: ``, lut: 1710793106.84369
[0m17:18:26.844326 [debug] [ThreadPool]: Databricks adapter: conn: 11294521472: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore_default, idle: 0s, acqrelcnt: 0, lang: None, thrd: (85666, 11390955520), cmpt: ``, lut: None
[0m17:18:26.844518 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_default'
[0m17:18:26.844656 [debug] [ThreadPool]: Databricks adapter: Thread (85666, 11390955520) using default compute resource.
[0m17:18:26.844794 [debug] [ThreadPool]: Databricks adapter: conn: 11294521472: _acquire sess: None, name: list_hive_metastore_default, idle: 2.1457672119140625e-06s, acqrelcnt: 1, lang: None, thrd: (85666, 11390955520), cmpt: ``, lut: 1710793106.8447518
[0m17:18:26.847118 [debug] [ThreadPool]: Databricks adapter: conn: 11294521472: get_thread_connection: sess: None, name: list_hive_metastore_default, idle: 0.0023131370544433594s, acqrelcnt: 1, lang: None, thrd: (85666, 11390955520), cmpt: ``, lut: 1710793106.8447518
[0m17:18:26.847282 [debug] [ThreadPool]: Databricks adapter: conn: 11294521472: idle check connection: sess: None, name: list_hive_metastore_default, idle: 0.0024852752685546875s, acqrelcnt: 1, lang: None, thrd: (85666, 11390955520), cmpt: ``, lut: 1710793106.8447518
[0m17:18:26.847423 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:18:26.847572 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m17:18:26.847711 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:18:27.468724 [debug] [ThreadPool]: Databricks adapter: conn: 11294521472: session opened sess: 01eee564-adfe-1996-b3cf-d196bf528c9e, name: list_hive_metastore_default, idle: 6.9141387939453125e-06s, acqrelcnt: 1, lang: None, thrd: (85666, 11390955520), cmpt: ``, lut: 1710793107.4685152
[0m17:18:27.804695 [debug] [ThreadPool]: SQL status: OK in 0.9599999785423279 seconds
[0m17:18:27.812776 [debug] [ThreadPool]: Databricks adapter: conn: 11294521472: get_thread_connection: sess: 01eee564-adfe-1996-b3cf-d196bf528c9e, name: list_hive_metastore_default, idle: 0.34417271614074707s, acqrelcnt: 1, lang: None, thrd: (85666, 11390955520), cmpt: ``, lut: 1710793107.4685152
[0m17:18:27.813033 [debug] [ThreadPool]: Databricks adapter: conn: 11294521472: idle check connection: sess: 01eee564-adfe-1996-b3cf-d196bf528c9e, name: list_hive_metastore_default, idle: 0.3444688320159912s, acqrelcnt: 1, lang: None, thrd: (85666, 11390955520), cmpt: ``, lut: 1710793107.4685152
[0m17:18:27.813198 [debug] [ThreadPool]: Databricks adapter: conn: 11294521472: get_thread_connection: sess: 01eee564-adfe-1996-b3cf-d196bf528c9e, name: list_hive_metastore_default, idle: 0.3446378707885742s, acqrelcnt: 1, lang: None, thrd: (85666, 11390955520), cmpt: ``, lut: 1710793107.4685152
[0m17:18:27.813367 [debug] [ThreadPool]: Databricks adapter: conn: 11294521472: idle check connection: sess: 01eee564-adfe-1996-b3cf-d196bf528c9e, name: list_hive_metastore_default, idle: 0.3448038101196289s, acqrelcnt: 1, lang: None, thrd: (85666, 11390955520), cmpt: ``, lut: 1710793107.4685152
[0m17:18:27.813518 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:18:27.813653 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:18:27.813807 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m17:18:28.087805 [debug] [ThreadPool]: SQL status: OK in 0.27000001072883606 seconds
[0m17:18:28.096434 [debug] [ThreadPool]: Databricks adapter: conn: 11294521472: get_thread_connection: sess: 01eee564-adfe-1996-b3cf-d196bf528c9e, name: list_hive_metastore_default, idle: 0.6277878284454346s, acqrelcnt: 1, lang: None, thrd: (85666, 11390955520), cmpt: ``, lut: 1710793107.4685152
[0m17:18:28.096822 [debug] [ThreadPool]: Databricks adapter: conn: 11294521472: idle check connection: sess: 01eee564-adfe-1996-b3cf-d196bf528c9e, name: list_hive_metastore_default, idle: 0.6282198429107666s, acqrelcnt: 1, lang: None, thrd: (85666, 11390955520), cmpt: ``, lut: 1710793107.4685152
[0m17:18:28.097073 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:18:28.097334 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m17:18:28.429211 [debug] [ThreadPool]: SQL status: OK in 0.33000001311302185 seconds
[0m17:18:28.434329 [debug] [ThreadPool]: Databricks adapter: conn: 11294521472: _release sess: 01eee564-adfe-1996-b3cf-d196bf528c9e, name: list_hive_metastore_default, idle: 6.9141387939453125e-06s, acqrelcnt: 0, lang: None, thrd: (85666, 11390955520), cmpt: ``, lut: 1710793108.434117
[0m17:18:28.438800 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7a2a7046-3724-4190-959c-39a7f6451f70', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x2a0fc0940>]}
[0m17:18:28.439477 [debug] [MainThread]: Databricks adapter: conn: 11291921616: _release sess: None, name: master, idle: 4.0531158447265625e-06s, acqrelcnt: 0, lang: None, thrd: (85666, 7965269056), cmpt: ``, lut: 1710793108.439349
[0m17:18:28.440331 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:18:28.440660 [info ] [MainThread]: 
[0m17:18:28.445346 [debug] [Thread-1  ]: Began running node model.default.stage_payments
[0m17:18:28.446261 [debug] [Thread-1  ]: Databricks adapter: conn: 11294521472: idle check connection: sess: 01eee564-adfe-1996-b3cf-d196bf528c9e, name: list_hive_metastore_default, idle: 0.011986970901489258s, acqrelcnt: 0, lang: None, thrd: (85666, 11390955520), cmpt: ``, lut: 1710793108.434117
[0m17:18:28.446576 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.default.stage_payments)
[0m17:18:28.446894 [debug] [Thread-1  ]: Databricks adapter: conn: 11294521472: reusing connection list_hive_metastore_default sess: 01eee564-adfe-1996-b3cf-d196bf528c9e, name: model.default.stage_payments, idle: 0.012650012969970703s, acqrelcnt: 0, lang: None, thrd: (85666, 11390955520), cmpt: ``, lut: 1710793108.434117
[0m17:18:28.447191 [debug] [Thread-1  ]: Databricks adapter: On thread (85666, 11390955520): `hive_metastore`.`default`.`stage_payments` using default compute resource.
[0m17:18:28.447493 [debug] [Thread-1  ]: Databricks adapter: conn: 11294521472: _acquire sess: 01eee564-adfe-1996-b3cf-d196bf528c9e, name: model.default.stage_payments, idle: 0.013253927230834961s, acqrelcnt: 1, lang: sql, thrd: (85666, 11390955520), cmpt: ``, lut: 1710793108.434117
[0m17:18:28.447786 [debug] [Thread-1  ]: Began compiling node model.default.stage_payments
[0m17:18:28.455580 [debug] [Thread-1  ]: Writing injected SQL for node "model.default.stage_payments"
[0m17:18:28.457060 [debug] [Thread-1  ]: Timing info for model.default.stage_payments (compile): 17:18:28.447976 => 17:18:28.456865
[0m17:18:28.457347 [debug] [Thread-1  ]: Began executing node model.default.stage_payments
[0m17:18:28.463468 [debug] [Thread-1  ]: Databricks adapter: conn: 11294521472: get_thread_connection: sess: 01eee564-adfe-1996-b3cf-d196bf528c9e, name: model.default.stage_payments, idle: 0.029227733612060547s, acqrelcnt: 1, lang: sql, thrd: (85666, 11390955520), cmpt: ``, lut: 1710793108.434117
[0m17:18:28.463763 [debug] [Thread-1  ]: Databricks adapter: conn: 11294521472: idle check connection: sess: 01eee564-adfe-1996-b3cf-d196bf528c9e, name: model.default.stage_payments, idle: 0.02954387664794922s, acqrelcnt: 1, lang: sql, thrd: (85666, 11390955520), cmpt: ``, lut: 1710793108.434117
[0m17:18:28.463970 [debug] [Thread-1  ]: Using databricks connection "model.default.stage_payments"
[0m17:18:28.464224 [debug] [Thread-1  ]: On model.default.stage_payments: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_payments"} */

  
    select *
    from (
        

select *
from `hive_metastore`.`default`.`payments`
    ) as model_limit_subq
    limit 5


[0m17:18:29.032437 [debug] [Thread-1  ]: SQL status: OK in 0.5699999928474426 seconds
[0m17:18:29.038522 [debug] [Thread-1  ]: Timing info for model.default.stage_payments (execute): 17:18:28.457518 => 17:18:29.038135
[0m17:18:29.039111 [debug] [Thread-1  ]: Databricks adapter: conn: 11294521472: _release sess: 01eee564-adfe-1996-b3cf-d196bf528c9e, name: model.default.stage_payments, idle: 6.9141387939453125e-06s, acqrelcnt: 0, lang: sql, thrd: (85666, 11390955520), cmpt: ``, lut: 1710793109.0389209
[0m17:18:29.040027 [debug] [Thread-1  ]: Databricks adapter: conn: 11294521472: _release sess: 01eee564-adfe-1996-b3cf-d196bf528c9e, name: model.default.stage_payments, idle: 2.1457672119140625e-06s, acqrelcnt: 0, lang: sql, thrd: (85666, 11390955520), cmpt: ``, lut: 1710793109.0398688
[0m17:18:29.040645 [debug] [Thread-1  ]: Finished running node model.default.stage_payments
[0m17:18:29.041583 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:18:29.041883 [debug] [MainThread]: Connection 'model.default.stage_payments' was properly closed.
[0m17:18:29.042176 [debug] [MainThread]: On model.default.stage_payments: ROLLBACK
[0m17:18:29.042463 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:18:29.042737 [debug] [MainThread]: On model.default.stage_payments: Close
[0m17:18:29.205691 [debug] [MainThread]: Command end result
[0m17:18:29.251158 [info ] [MainThread]: Previewing node 'stage_payments':
| user_id | ssn         | gender | language | race                 | job_title            | ... |
| ------- | ----------- | ------ | -------- | -------------------- | -------------------- | --- |
|       1 | 423-55-9673 | F      | Pashto   | Chilean              | Tax Accountant       | ... |
|       2 | 223-09-3384 | F      | Polish   | Eskimo               | Internal Auditor     | ... |
|       3 | 189-50-1142 | M      | Thai     | Colombian            | Senior Cost Accou... | ... |
|       4 | 709-16-4847 | M      | Bengali  | Dominican (Domini... | Database Administ... | ... |
|       5 | 412-27-0802 | F      | Marathi  | Alaskan Athabascan   | Information Syste... | ... |

[0m17:18:29.254549 [debug] [MainThread]: Resource report: {"command_name": "show", "command_success": true, "command_wall_clock_time": 3.6026535, "process_user_time": 2.262853, "process_kernel_time": 3.306108, "process_mem_max_rss": "208322560", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m17:18:29.254978 [debug] [MainThread]: Command `dbt show` succeeded at 17:18:29.254903 after 3.60 seconds
[0m17:18:29.255245 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103b9a160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x2a10fa730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x2a10fa970>]}
[0m17:18:29.255483 [debug] [MainThread]: Flushing usage events
[0m17:23:41.944410 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10234c550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1041a4d60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1041b6760>]}


============================== 17:23:41.947755 | decca2f6-4e6a-4f2e-af40-90611264845d ==============================
[0m17:23:41.947755 [info ] [MainThread]: Running with dbt=1.7.7
[0m17:23:41.948018 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt show --select stage_payments.sql', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m17:23:43.037115 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'decca2f6-4e6a-4f2e-af40-90611264845d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1041a4d60>]}
[0m17:23:43.078496 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'decca2f6-4e6a-4f2e-af40-90611264845d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13a5740d0>]}
[0m17:23:43.078780 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m17:23:43.094884 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m17:23:43.160698 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m17:23:43.161008 [debug] [MainThread]: Partial parsing: added file: default://macros/gen_id.sql
[0m17:23:43.161196 [debug] [MainThread]: Partial parsing: updated file: default://models/stage/stage_payments.sql
[0m17:23:43.236836 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'decca2f6-4e6a-4f2e-af40-90611264845d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13b2b40d0>]}
[0m17:23:43.245741 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'decca2f6-4e6a-4f2e-af40-90611264845d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13b05b280>]}
[0m17:23:43.245981 [info ] [MainThread]: Found 3 models, 3 sources, 0 exposures, 0 metrics, 536 macros, 0 groups, 0 semantic models
[0m17:23:43.246166 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'decca2f6-4e6a-4f2e-af40-90611264845d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13b05b0d0>]}
[0m17:23:43.247063 [info ] [MainThread]: 
[0m17:23:43.247497 [debug] [MainThread]: Databricks adapter: conn: 5285195984: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (86588, 7965269056), cmpt: ``, lut: None
[0m17:23:43.247652 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:23:43.247789 [debug] [MainThread]: Databricks adapter: Thread (86588, 7965269056) using default compute resource.
[0m17:23:43.247926 [debug] [MainThread]: Databricks adapter: conn: 5285195984: _acquire sess: None, name: master, idle: 1.1920928955078125e-06s, acqrelcnt: 1, lang: None, thrd: (86588, 7965269056), cmpt: ``, lut: 1710793423.247882
[0m17:23:43.248523 [debug] [ThreadPool]: Databricks adapter: conn: 5287661520: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore_default, idle: 0s, acqrelcnt: 0, lang: None, thrd: (86588, 10850103296), cmpt: ``, lut: None
[0m17:23:43.248737 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_default'
[0m17:23:43.248890 [debug] [ThreadPool]: Databricks adapter: Thread (86588, 10850103296) using default compute resource.
[0m17:23:43.249043 [debug] [ThreadPool]: Databricks adapter: conn: 5287661520: _acquire sess: None, name: list_hive_metastore_default, idle: 1.1920928955078125e-06s, acqrelcnt: 1, lang: None, thrd: (86588, 10850103296), cmpt: ``, lut: 1710793423.248996
[0m17:23:43.251395 [debug] [ThreadPool]: Databricks adapter: conn: 5287661520: get_thread_connection: sess: None, name: list_hive_metastore_default, idle: 0.0023429393768310547s, acqrelcnt: 1, lang: None, thrd: (86588, 10850103296), cmpt: ``, lut: 1710793423.248996
[0m17:23:43.251564 [debug] [ThreadPool]: Databricks adapter: conn: 5287661520: idle check connection: sess: None, name: list_hive_metastore_default, idle: 0.0025238990783691406s, acqrelcnt: 1, lang: None, thrd: (86588, 10850103296), cmpt: ``, lut: 1710793423.248996
[0m17:23:43.251701 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:23:43.251847 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m17:23:43.251984 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:23:43.893067 [debug] [ThreadPool]: Databricks adapter: conn: 5287661520: session opened sess: 01eee565-6a9b-1aab-8493-4e73adc4b905, name: list_hive_metastore_default, idle: 1.71661376953125e-05s, acqrelcnt: 1, lang: None, thrd: (86588, 10850103296), cmpt: ``, lut: 1710793423.8925078
[0m17:23:44.465938 [debug] [ThreadPool]: SQL status: OK in 1.2100000381469727 seconds
[0m17:23:44.484845 [debug] [ThreadPool]: Databricks adapter: conn: 5287661520: get_thread_connection: sess: 01eee565-6a9b-1aab-8493-4e73adc4b905, name: list_hive_metastore_default, idle: 0.5922081470489502s, acqrelcnt: 1, lang: None, thrd: (86588, 10850103296), cmpt: ``, lut: 1710793423.8925078
[0m17:23:44.485208 [debug] [ThreadPool]: Databricks adapter: conn: 5287661520: idle check connection: sess: 01eee565-6a9b-1aab-8493-4e73adc4b905, name: list_hive_metastore_default, idle: 0.592623233795166s, acqrelcnt: 1, lang: None, thrd: (86588, 10850103296), cmpt: ``, lut: 1710793423.8925078
[0m17:23:44.485456 [debug] [ThreadPool]: Databricks adapter: conn: 5287661520: get_thread_connection: sess: 01eee565-6a9b-1aab-8493-4e73adc4b905, name: list_hive_metastore_default, idle: 0.5928840637207031s, acqrelcnt: 1, lang: None, thrd: (86588, 10850103296), cmpt: ``, lut: 1710793423.8925078
[0m17:23:44.485679 [debug] [ThreadPool]: Databricks adapter: conn: 5287661520: idle check connection: sess: 01eee565-6a9b-1aab-8493-4e73adc4b905, name: list_hive_metastore_default, idle: 0.5931072235107422s, acqrelcnt: 1, lang: None, thrd: (86588, 10850103296), cmpt: ``, lut: 1710793423.8925078
[0m17:23:44.485887 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:23:44.486080 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:23:44.486298 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m17:23:44.826700 [debug] [ThreadPool]: SQL status: OK in 0.3400000035762787 seconds
[0m17:23:44.835253 [debug] [ThreadPool]: Databricks adapter: conn: 5287661520: get_thread_connection: sess: 01eee565-6a9b-1aab-8493-4e73adc4b905, name: list_hive_metastore_default, idle: 0.9426162242889404s, acqrelcnt: 1, lang: None, thrd: (86588, 10850103296), cmpt: ``, lut: 1710793423.8925078
[0m17:23:44.835659 [debug] [ThreadPool]: Databricks adapter: conn: 5287661520: idle check connection: sess: 01eee565-6a9b-1aab-8493-4e73adc4b905, name: list_hive_metastore_default, idle: 0.9430592060089111s, acqrelcnt: 1, lang: None, thrd: (86588, 10850103296), cmpt: ``, lut: 1710793423.8925078
[0m17:23:44.835937 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:23:44.836236 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m17:23:45.150518 [debug] [ThreadPool]: SQL status: OK in 0.3100000023841858 seconds
[0m17:23:45.156512 [debug] [ThreadPool]: Databricks adapter: conn: 5287661520: _release sess: 01eee565-6a9b-1aab-8493-4e73adc4b905, name: list_hive_metastore_default, idle: 5.9604644775390625e-06s, acqrelcnt: 0, lang: None, thrd: (86588, 10850103296), cmpt: ``, lut: 1710793425.156317
[0m17:23:45.160543 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'decca2f6-4e6a-4f2e-af40-90611264845d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13a718bb0>]}
[0m17:23:45.160985 [debug] [MainThread]: Databricks adapter: conn: 5285195984: _release sess: None, name: master, idle: 1.9073486328125e-06s, acqrelcnt: 0, lang: None, thrd: (86588, 7965269056), cmpt: ``, lut: 1710793425.160886
[0m17:23:45.161662 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:23:45.161972 [info ] [MainThread]: 
[0m17:23:45.166018 [debug] [Thread-1  ]: Began running node model.default.stage_payments
[0m17:23:45.166891 [debug] [Thread-1  ]: Databricks adapter: conn: 5287661520: idle check connection: sess: 01eee565-6a9b-1aab-8493-4e73adc4b905, name: list_hive_metastore_default, idle: 0.010408163070678711s, acqrelcnt: 0, lang: None, thrd: (86588, 10850103296), cmpt: ``, lut: 1710793425.156317
[0m17:23:45.167230 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.default.stage_payments)
[0m17:23:45.167602 [debug] [Thread-1  ]: Databricks adapter: conn: 5287661520: reusing connection list_hive_metastore_default sess: 01eee565-6a9b-1aab-8493-4e73adc4b905, name: model.default.stage_payments, idle: 0.01114201545715332s, acqrelcnt: 0, lang: None, thrd: (86588, 10850103296), cmpt: ``, lut: 1710793425.156317
[0m17:23:45.167945 [debug] [Thread-1  ]: Databricks adapter: On thread (86588, 10850103296): `hive_metastore`.`default`.`stage_payments` using default compute resource.
[0m17:23:45.168293 [debug] [Thread-1  ]: Databricks adapter: conn: 5287661520: _acquire sess: 01eee565-6a9b-1aab-8493-4e73adc4b905, name: model.default.stage_payments, idle: 0.011831998825073242s, acqrelcnt: 1, lang: sql, thrd: (86588, 10850103296), cmpt: ``, lut: 1710793425.156317
[0m17:23:45.168604 [debug] [Thread-1  ]: Began compiling node model.default.stage_payments
[0m17:23:45.176300 [debug] [Thread-1  ]: Writing injected SQL for node "model.default.stage_payments"
[0m17:23:45.177970 [debug] [Thread-1  ]: Timing info for model.default.stage_payments (compile): 17:23:45.168790 => 17:23:45.177796
[0m17:23:45.178239 [debug] [Thread-1  ]: Began executing node model.default.stage_payments
[0m17:23:45.184199 [debug] [Thread-1  ]: Databricks adapter: conn: 5287661520: get_thread_connection: sess: 01eee565-6a9b-1aab-8493-4e73adc4b905, name: model.default.stage_payments, idle: 0.027753829956054688s, acqrelcnt: 1, lang: sql, thrd: (86588, 10850103296), cmpt: ``, lut: 1710793425.156317
[0m17:23:45.184497 [debug] [Thread-1  ]: Databricks adapter: conn: 5287661520: idle check connection: sess: 01eee565-6a9b-1aab-8493-4e73adc4b905, name: model.default.stage_payments, idle: 0.028071165084838867s, acqrelcnt: 1, lang: sql, thrd: (86588, 10850103296), cmpt: ``, lut: 1710793425.156317
[0m17:23:45.184712 [debug] [Thread-1  ]: Using databricks connection "model.default.stage_payments"
[0m17:23:45.185004 [debug] [Thread-1  ]: On model.default.stage_payments: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_payments"} */

  
    select *
    from (
        

select 
CAST(FLOOR(random() * (10000 - 1 + 1)) + 1 AS INTEGER)
 AS id,
       user_id AS user_id,
       case when gender = 'm' then 'male' else 'female' end as gender,
       country_code AS location,
       credit_card_type AS credit_card_type,
       datetime AS datetime
       time AS time
from `hive_metastore`.`default`.`payments`
    ) as model_limit_subq
    limit 5


[0m17:23:45.570314 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_payments"} */

  
    select *
    from (
        

select 
CAST(FLOOR(random() * (10000 - 1 + 1)) + 1 AS INTEGER)
 AS id,
       user_id AS user_id,
       case when gender = 'm' then 'male' else 'female' end as gender,
       country_code AS location,
       credit_card_type AS credit_card_type,
       datetime AS datetime
       time AS time
from `hive_metastore`.`default`.`payments`
    ) as model_limit_subq
    limit 5


[0m17:23:45.571645 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'time': missing ')'.(line 16, pos 7)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_payments"} */

  
    select *
    from (
        

select 
CAST(FLOOR(random() * (10000 - 1 + 1)) + 1 AS INTEGER)
 AS id,
       user_id AS user_id,
       case when gender = 'm' then 'male' else 'female' end as gender,
       country_code AS location,
       credit_card_type AS credit_card_type,
       datetime AS datetime
       time AS time
-------^^^
from `hive_metastore`.`default`.`payments`
    ) as model_limit_subq
    limit 5

[0m17:23:45.573086 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'time': missing ')'.(line 16, pos 7)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_payments"} */

  
    select *
    from (
        

select 
CAST(FLOOR(random() * (10000 - 1 + 1)) + 1 AS INTEGER)
 AS id,
       user_id AS user_id,
       case when gender = 'm' then 'male' else 'female' end as gender,
       country_code AS location,
       credit_card_type AS credit_card_type,
       datetime AS datetime
       time AS time
-------^^^
from `hive_metastore`.`default`.`payments`
    ) as model_limit_subq
    limit 5

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:697)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:574)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:423)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:65)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:65)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:401)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:386)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:435)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'time': missing ')'.(line 16, pos 7)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_payments"} */

  
    select *
    from (
        

select 
CAST(FLOOR(random() * (10000 - 1 + 1)) + 1 AS INTEGER)
 AS id,
       user_id AS user_id,
       case when gender = 'm' then 'male' else 'female' end as gender,
       country_code AS location,
       credit_card_type AS credit_card_type,
       datetime AS datetime
       time AS time
-------^^^
from `hive_metastore`.`default`.`payments`
    ) as model_limit_subq
    limit 5

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:267)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:101)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:111)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:87)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$2(QueryRuntimePrediction.scala:534)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:394)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:531)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1153)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:529)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:382)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:362)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:358)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:81)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:80)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:66)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:115)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)
	... 3 more

[0m17:23:45.574654 [debug] [Thread-1  ]: Databricks adapter: operation-id: 01eee565-6b7a-141c-a2eb-70aa4d6f0a13
[0m17:23:45.575533 [debug] [Thread-1  ]: Timing info for model.default.stage_payments (execute): 17:23:45.178400 => 17:23:45.575212
[0m17:23:45.576371 [debug] [Thread-1  ]: Databricks adapter: conn: 5287661520: _release sess: 01eee565-6a9b-1aab-8493-4e73adc4b905, name: model.default.stage_payments, idle: 1.4066696166992188e-05s, acqrelcnt: 0, lang: sql, thrd: (86588, 10850103296), cmpt: ``, lut: 1710793425.5761259
[0m17:23:45.614166 [debug] [Thread-1  ]: Runtime Error in model stage_payments (models/stage/stage_payments.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'time': missing ')'.(line 16, pos 7)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_payments"} */
  
    
      select *
      from (
          
  
  select 
  CAST(FLOOR(random() * (10000 - 1 + 1)) + 1 AS INTEGER)
   AS id,
         user_id AS user_id,
         case when gender = 'm' then 'male' else 'female' end as gender,
         country_code AS location,
         credit_card_type AS credit_card_type,
         datetime AS datetime
         time AS time
  -------^^^
  from `hive_metastore`.`default`.`payments`
      ) as model_limit_subq
      limit 5
  
[0m17:23:45.614599 [debug] [Thread-1  ]: Databricks adapter: conn: 5287661520: _release sess: 01eee565-6a9b-1aab-8493-4e73adc4b905, name: model.default.stage_payments, idle: 2.86102294921875e-06s, acqrelcnt: 0, lang: sql, thrd: (86588, 10850103296), cmpt: ``, lut: 1710793425.61447
[0m17:23:45.615072 [debug] [Thread-1  ]: Finished running node model.default.stage_payments
[0m17:23:45.615442 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:23:45.615706 [debug] [MainThread]: Connection 'model.default.stage_payments' was properly closed.
[0m17:23:45.615953 [debug] [MainThread]: On model.default.stage_payments: ROLLBACK
[0m17:23:45.616190 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:23:45.616396 [debug] [MainThread]: On model.default.stage_payments: Close
[0m17:23:45.781749 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error in model stage_payments (models/stage/stage_payments.sql)
    
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'time': missing ')'.(line 16, pos 7)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_payments"} */
    
      
        select *
        from (
            
    
    select 
    CAST(FLOOR(random() * (10000 - 1 + 1)) + 1 AS INTEGER)
     AS id,
           user_id AS user_id,
           case when gender = 'm' then 'male' else 'female' end as gender,
           country_code AS location,
           credit_card_type AS credit_card_type,
           datetime AS datetime
           time AS time
    -------^^^
    from `hive_metastore`.`default`.`payments`
        ) as model_limit_subq
        limit 5
    
[0m17:23:45.790003 [debug] [MainThread]: Resource report: {"command_name": "show", "command_wall_clock_time": 3.8876753, "process_user_time": 2.332667, "process_kernel_time": 3.398505, "process_mem_max_rss": "204898304", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m17:23:45.791193 [debug] [MainThread]: Command `dbt show` failed at 17:23:45.790946 after 3.89 seconds
[0m17:23:45.792485 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10234c550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13a65fa30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13b124e20>]}
[0m17:23:45.793131 [debug] [MainThread]: Flushing usage events
[0m17:23:51.417979 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104fc1160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1081744c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1081b62e0>]}


============================== 17:23:51.420467 | 0ce76ab8-9809-4f28-8762-42558169a26f ==============================
[0m17:23:51.420467 [info ] [MainThread]: Running with dbt=1.7.7
[0m17:23:51.420724 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt show --select stage_payments.sql', 'send_anonymous_usage_stats': 'True'}
[0m17:23:52.258811 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0ce76ab8-9809-4f28-8762-42558169a26f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1081b6b80>]}
[0m17:23:52.300304 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0ce76ab8-9809-4f28-8762-42558169a26f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1081b00a0>]}
[0m17:23:52.300565 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m17:23:52.314289 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m17:23:52.367460 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:23:52.367738 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:23:52.371492 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0ce76ab8-9809-4f28-8762-42558169a26f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e826f10>]}
[0m17:23:52.389782 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0ce76ab8-9809-4f28-8762-42558169a26f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12d4121c0>]}
[0m17:23:52.390053 [info ] [MainThread]: Found 3 models, 3 sources, 0 exposures, 0 metrics, 536 macros, 0 groups, 0 semantic models
[0m17:23:52.390241 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0ce76ab8-9809-4f28-8762-42558169a26f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12d43a0d0>]}
[0m17:23:52.391066 [info ] [MainThread]: 
[0m17:23:52.391548 [debug] [MainThread]: Databricks adapter: conn: 5066612352: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (86622, 7965269056), cmpt: ``, lut: None
[0m17:23:52.391714 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:23:52.391866 [debug] [MainThread]: Databricks adapter: Thread (86622, 7965269056) using default compute resource.
[0m17:23:52.392018 [debug] [MainThread]: Databricks adapter: conn: 5066612352: _acquire sess: None, name: master, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (86622, 7965269056), cmpt: ``, lut: 1710793432.3919702
[0m17:23:52.392648 [debug] [ThreadPool]: Databricks adapter: conn: 5075264704: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore_default, idle: 0s, acqrelcnt: 0, lang: None, thrd: (86622, 11458867200), cmpt: ``, lut: None
[0m17:23:52.392883 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_default'
[0m17:23:52.393045 [debug] [ThreadPool]: Databricks adapter: Thread (86622, 11458867200) using default compute resource.
[0m17:23:52.393206 [debug] [ThreadPool]: Databricks adapter: conn: 5075264704: _acquire sess: None, name: list_hive_metastore_default, idle: 1.9073486328125e-06s, acqrelcnt: 1, lang: None, thrd: (86622, 11458867200), cmpt: ``, lut: 1710793432.393156
[0m17:23:52.396068 [debug] [ThreadPool]: Databricks adapter: conn: 5075264704: get_thread_connection: sess: None, name: list_hive_metastore_default, idle: 0.002852916717529297s, acqrelcnt: 1, lang: None, thrd: (86622, 11458867200), cmpt: ``, lut: 1710793432.393156
[0m17:23:52.396249 [debug] [ThreadPool]: Databricks adapter: conn: 5075264704: idle check connection: sess: None, name: list_hive_metastore_default, idle: 0.0030448436737060547s, acqrelcnt: 1, lang: None, thrd: (86622, 11458867200), cmpt: ``, lut: 1710793432.393156
[0m17:23:52.396403 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:23:52.396555 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m17:23:52.396713 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:23:53.015642 [debug] [ThreadPool]: Databricks adapter: conn: 5075264704: session opened sess: 01eee565-700a-1160-b3ec-4645d482777f, name: list_hive_metastore_default, idle: 2.3126602172851562e-05s, acqrelcnt: 1, lang: None, thrd: (86622, 11458867200), cmpt: ``, lut: 1710793433.015127
[0m17:23:53.471124 [debug] [ThreadPool]: SQL status: OK in 1.0700000524520874 seconds
[0m17:23:53.486752 [debug] [ThreadPool]: Databricks adapter: conn: 5075264704: get_thread_connection: sess: 01eee565-700a-1160-b3ec-4645d482777f, name: list_hive_metastore_default, idle: 0.4714980125427246s, acqrelcnt: 1, lang: None, thrd: (86622, 11458867200), cmpt: ``, lut: 1710793433.015127
[0m17:23:53.487129 [debug] [ThreadPool]: Databricks adapter: conn: 5075264704: idle check connection: sess: 01eee565-700a-1160-b3ec-4645d482777f, name: list_hive_metastore_default, idle: 0.47191309928894043s, acqrelcnt: 1, lang: None, thrd: (86622, 11458867200), cmpt: ``, lut: 1710793433.015127
[0m17:23:53.487416 [debug] [ThreadPool]: Databricks adapter: conn: 5075264704: get_thread_connection: sess: 01eee565-700a-1160-b3ec-4645d482777f, name: list_hive_metastore_default, idle: 0.4722111225128174s, acqrelcnt: 1, lang: None, thrd: (86622, 11458867200), cmpt: ``, lut: 1710793433.015127
[0m17:23:53.487684 [debug] [ThreadPool]: Databricks adapter: conn: 5075264704: idle check connection: sess: 01eee565-700a-1160-b3ec-4645d482777f, name: list_hive_metastore_default, idle: 0.47248005867004395s, acqrelcnt: 1, lang: None, thrd: (86622, 11458867200), cmpt: ``, lut: 1710793433.015127
[0m17:23:53.487936 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:23:53.488162 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:23:53.488422 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m17:23:53.748301 [debug] [ThreadPool]: SQL status: OK in 0.25999999046325684 seconds
[0m17:23:53.759521 [debug] [ThreadPool]: Databricks adapter: conn: 5075264704: get_thread_connection: sess: 01eee565-700a-1160-b3ec-4645d482777f, name: list_hive_metastore_default, idle: 0.7442319393157959s, acqrelcnt: 1, lang: None, thrd: (86622, 11458867200), cmpt: ``, lut: 1710793433.015127
[0m17:23:53.760007 [debug] [ThreadPool]: Databricks adapter: conn: 5075264704: idle check connection: sess: 01eee565-700a-1160-b3ec-4645d482777f, name: list_hive_metastore_default, idle: 0.7447812557220459s, acqrelcnt: 1, lang: None, thrd: (86622, 11458867200), cmpt: ``, lut: 1710793433.015127
[0m17:23:53.760309 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:23:53.760615 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m17:23:54.095125 [debug] [ThreadPool]: SQL status: OK in 0.33000001311302185 seconds
[0m17:23:54.100948 [debug] [ThreadPool]: Databricks adapter: conn: 5075264704: _release sess: 01eee565-700a-1160-b3ec-4645d482777f, name: list_hive_metastore_default, idle: 3.0994415283203125e-06s, acqrelcnt: 0, lang: None, thrd: (86622, 11458867200), cmpt: ``, lut: 1710793434.1007628
[0m17:23:54.105744 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0ce76ab8-9809-4f28-8762-42558169a26f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12d5122e0>]}
[0m17:23:54.106526 [debug] [MainThread]: Databricks adapter: conn: 5066612352: _release sess: None, name: master, idle: 4.0531158447265625e-06s, acqrelcnt: 0, lang: None, thrd: (86622, 7965269056), cmpt: ``, lut: 1710793434.106386
[0m17:23:54.107370 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:23:54.107697 [info ] [MainThread]: 
[0m17:23:54.111329 [debug] [Thread-1  ]: Began running node model.default.stage_payments
[0m17:23:54.112111 [debug] [Thread-1  ]: Databricks adapter: conn: 5075264704: idle check connection: sess: 01eee565-700a-1160-b3ec-4645d482777f, name: list_hive_metastore_default, idle: 0.011193037033081055s, acqrelcnt: 0, lang: None, thrd: (86622, 11458867200), cmpt: ``, lut: 1710793434.1007628
[0m17:23:54.112392 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.default.stage_payments)
[0m17:23:54.112705 [debug] [Thread-1  ]: Databricks adapter: conn: 5075264704: reusing connection list_hive_metastore_default sess: 01eee565-700a-1160-b3ec-4645d482777f, name: model.default.stage_payments, idle: 0.01182103157043457s, acqrelcnt: 0, lang: None, thrd: (86622, 11458867200), cmpt: ``, lut: 1710793434.1007628
[0m17:23:54.112997 [debug] [Thread-1  ]: Databricks adapter: On thread (86622, 11458867200): `hive_metastore`.`default`.`stage_payments` using default compute resource.
[0m17:23:54.113285 [debug] [Thread-1  ]: Databricks adapter: conn: 5075264704: _acquire sess: 01eee565-700a-1160-b3ec-4645d482777f, name: model.default.stage_payments, idle: 0.012409210205078125s, acqrelcnt: 1, lang: sql, thrd: (86622, 11458867200), cmpt: ``, lut: 1710793434.1007628
[0m17:23:54.113579 [debug] [Thread-1  ]: Began compiling node model.default.stage_payments
[0m17:23:54.122300 [debug] [Thread-1  ]: Writing injected SQL for node "model.default.stage_payments"
[0m17:23:54.123618 [debug] [Thread-1  ]: Timing info for model.default.stage_payments (compile): 17:23:54.113763 => 17:23:54.123444
[0m17:23:54.123886 [debug] [Thread-1  ]: Began executing node model.default.stage_payments
[0m17:23:54.130637 [debug] [Thread-1  ]: Databricks adapter: conn: 5075264704: get_thread_connection: sess: 01eee565-700a-1160-b3ec-4645d482777f, name: model.default.stage_payments, idle: 0.02975630760192871s, acqrelcnt: 1, lang: sql, thrd: (86622, 11458867200), cmpt: ``, lut: 1710793434.1007628
[0m17:23:54.130912 [debug] [Thread-1  ]: Databricks adapter: conn: 5075264704: idle check connection: sess: 01eee565-700a-1160-b3ec-4645d482777f, name: model.default.stage_payments, idle: 0.030050992965698242s, acqrelcnt: 1, lang: sql, thrd: (86622, 11458867200), cmpt: ``, lut: 1710793434.1007628
[0m17:23:54.131113 [debug] [Thread-1  ]: Using databricks connection "model.default.stage_payments"
[0m17:23:54.131387 [debug] [Thread-1  ]: On model.default.stage_payments: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_payments"} */

  
    select *
    from (
        

select 
CAST(FLOOR(random() * (10000 - 1 + 1)) + 1 AS INTEGER)
 AS id,
       user_id AS user_id,
       case when gender = 'm' then 'male' else 'female' end as gender,
       country_code AS location,
       credit_card_type AS credit_card_type,
       datetime AS datetime
       time AS time
from `hive_metastore`.`default`.`payments`
    ) as model_limit_subq
    limit 5


[0m17:23:54.467116 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_payments"} */

  
    select *
    from (
        

select 
CAST(FLOOR(random() * (10000 - 1 + 1)) + 1 AS INTEGER)
 AS id,
       user_id AS user_id,
       case when gender = 'm' then 'male' else 'female' end as gender,
       country_code AS location,
       credit_card_type AS credit_card_type,
       datetime AS datetime
       time AS time
from `hive_metastore`.`default`.`payments`
    ) as model_limit_subq
    limit 5


[0m17:23:54.469561 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'time': missing ')'.(line 16, pos 7)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_payments"} */

  
    select *
    from (
        

select 
CAST(FLOOR(random() * (10000 - 1 + 1)) + 1 AS INTEGER)
 AS id,
       user_id AS user_id,
       case when gender = 'm' then 'male' else 'female' end as gender,
       country_code AS location,
       credit_card_type AS credit_card_type,
       datetime AS datetime
       time AS time
-------^^^
from `hive_metastore`.`default`.`payments`
    ) as model_limit_subq
    limit 5

[0m17:23:54.470988 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'time': missing ')'.(line 16, pos 7)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_payments"} */

  
    select *
    from (
        

select 
CAST(FLOOR(random() * (10000 - 1 + 1)) + 1 AS INTEGER)
 AS id,
       user_id AS user_id,
       case when gender = 'm' then 'male' else 'female' end as gender,
       country_code AS location,
       credit_card_type AS credit_card_type,
       datetime AS datetime
       time AS time
-------^^^
from `hive_metastore`.`default`.`payments`
    ) as model_limit_subq
    limit 5

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:697)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:574)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:423)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:65)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:65)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:401)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:386)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:435)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'time': missing ')'.(line 16, pos 7)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_payments"} */

  
    select *
    from (
        

select 
CAST(FLOOR(random() * (10000 - 1 + 1)) + 1 AS INTEGER)
 AS id,
       user_id AS user_id,
       case when gender = 'm' then 'male' else 'female' end as gender,
       country_code AS location,
       credit_card_type AS credit_card_type,
       datetime AS datetime
       time AS time
-------^^^
from `hive_metastore`.`default`.`payments`
    ) as model_limit_subq
    limit 5

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:267)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:101)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:111)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:87)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:77)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:98)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:74)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$2(QueryRuntimePrediction.scala:534)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:394)
	at com.databricks.sql.QueryRuntimePrediction.$anonfun$getQueryExecutionWithParsedPlan$1(QueryRuntimePrediction.scala:531)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1153)
	at com.databricks.sql.QueryRuntimePrediction.getQueryExecutionWithParsedPlan(QueryRuntimePrediction.scala:529)
	at com.databricks.sql.QueryRuntimePrediction.getRuntimeCategory(QueryRuntimePrediction.scala:382)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$3(ClusterLoadMonitor.scala:362)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:104)
	at scala.util.Using$.resource(Using.scala:269)
	at com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:103)
	at com.databricks.sql.ClusterLoadMonitor.$anonfun$getRuntimeCategory$2(ClusterLoadMonitor.scala:358)
	at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:81)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:80)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:66)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:115)
	at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)
	... 3 more

[0m17:23:54.472260 [debug] [Thread-1  ]: Databricks adapter: operation-id: 01eee565-70cf-178e-b1a6-3ab8db995b0c
[0m17:23:54.473112 [debug] [Thread-1  ]: Timing info for model.default.stage_payments (execute): 17:23:54.124048 => 17:23:54.472807
[0m17:23:54.473769 [debug] [Thread-1  ]: Databricks adapter: conn: 5075264704: _release sess: 01eee565-700a-1160-b3ec-4645d482777f, name: model.default.stage_payments, idle: 1.2159347534179688e-05s, acqrelcnt: 0, lang: sql, thrd: (86622, 11458867200), cmpt: ``, lut: 1710793434.473572
[0m17:23:54.513146 [debug] [Thread-1  ]: Runtime Error in model stage_payments (models/stage/stage_payments.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'time': missing ')'.(line 16, pos 7)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_payments"} */
  
    
      select *
      from (
          
  
  select 
  CAST(FLOOR(random() * (10000 - 1 + 1)) + 1 AS INTEGER)
   AS id,
         user_id AS user_id,
         case when gender = 'm' then 'male' else 'female' end as gender,
         country_code AS location,
         credit_card_type AS credit_card_type,
         datetime AS datetime
         time AS time
  -------^^^
  from `hive_metastore`.`default`.`payments`
      ) as model_limit_subq
      limit 5
  
[0m17:23:54.513711 [debug] [Thread-1  ]: Databricks adapter: conn: 5075264704: _release sess: 01eee565-700a-1160-b3ec-4645d482777f, name: model.default.stage_payments, idle: 4.0531158447265625e-06s, acqrelcnt: 0, lang: sql, thrd: (86622, 11458867200), cmpt: ``, lut: 1710793434.513563
[0m17:23:54.514267 [debug] [Thread-1  ]: Finished running node model.default.stage_payments
[0m17:23:54.514755 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:23:54.515175 [debug] [MainThread]: Connection 'model.default.stage_payments' was properly closed.
[0m17:23:54.515550 [debug] [MainThread]: On model.default.stage_payments: ROLLBACK
[0m17:23:54.515818 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:23:54.516057 [debug] [MainThread]: On model.default.stage_payments: Close
[0m17:23:54.686454 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error in model stage_payments (models/stage/stage_payments.sql)
    
    [PARSE_SYNTAX_ERROR] Syntax error at or near 'time': missing ')'.(line 16, pos 7)
    
    == SQL ==
    /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_payments"} */
    
      
        select *
        from (
            
    
    select 
    CAST(FLOOR(random() * (10000 - 1 + 1)) + 1 AS INTEGER)
     AS id,
           user_id AS user_id,
           case when gender = 'm' then 'male' else 'female' end as gender,
           country_code AS location,
           credit_card_type AS credit_card_type,
           datetime AS datetime
           time AS time
    -------^^^
    from `hive_metastore`.`default`.`payments`
        ) as model_limit_subq
        limit 5
    
[0m17:23:54.694652 [debug] [MainThread]: Resource report: {"command_name": "show", "command_wall_clock_time": 3.3127291, "process_user_time": 2.234791, "process_kernel_time": 3.375096, "process_mem_max_rss": "205422592", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m17:23:54.696112 [debug] [MainThread]: Command `dbt show` failed at 17:23:54.695897 after 3.32 seconds
[0m17:23:54.696868 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104fc1160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12dfe5fa0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12e835e20>]}
[0m17:23:54.697591 [debug] [MainThread]: Flushing usage events
[0m17:24:02.470485 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10595a160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107bf8520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107c3a0a0>]}


============================== 17:24:02.472832 | 66b13a87-46c2-4da9-9676-851ff246150b ==============================
[0m17:24:02.472832 [info ] [MainThread]: Running with dbt=1.7.7
[0m17:24:02.473084 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt show --select stage_payments.sql', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m17:24:03.347059 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '66b13a87-46c2-4da9-9676-851ff246150b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107c3aeb0>]}
[0m17:24:03.388375 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '66b13a87-46c2-4da9-9676-851ff246150b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1687746a0>]}
[0m17:24:03.388639 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m17:24:03.401223 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m17:24:03.452252 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m17:24:03.452749 [debug] [MainThread]: Partial parsing: updated file: default://models/stage/stage_payments.sql
[0m17:24:03.523869 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '66b13a87-46c2-4da9-9676-851ff246150b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x169db20d0>]}
[0m17:24:03.531226 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '66b13a87-46c2-4da9-9676-851ff246150b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x169a5b0d0>]}
[0m17:24:03.531444 [info ] [MainThread]: Found 3 models, 3 sources, 0 exposures, 0 metrics, 536 macros, 0 groups, 0 semantic models
[0m17:24:03.531608 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '66b13a87-46c2-4da9-9676-851ff246150b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x169b25ee0>]}
[0m17:24:03.532290 [info ] [MainThread]: 
[0m17:24:03.532697 [debug] [MainThread]: Databricks adapter: conn: 6068264816: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (86653, 7965269056), cmpt: ``, lut: None
[0m17:24:03.532856 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:24:03.532992 [debug] [MainThread]: Databricks adapter: Thread (86653, 7965269056) using default compute resource.
[0m17:24:03.533130 [debug] [MainThread]: Databricks adapter: conn: 6068264816: _acquire sess: None, name: master, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (86653, 7965269056), cmpt: ``, lut: 1710793443.533086
[0m17:24:03.533698 [debug] [ThreadPool]: Databricks adapter: conn: 6070939408: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore_default, idle: 0s, acqrelcnt: 0, lang: None, thrd: (86653, 10797936640), cmpt: ``, lut: None
[0m17:24:03.533895 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_default'
[0m17:24:03.534047 [debug] [ThreadPool]: Databricks adapter: Thread (86653, 10797936640) using default compute resource.
[0m17:24:03.534198 [debug] [ThreadPool]: Databricks adapter: conn: 6070939408: _acquire sess: None, name: list_hive_metastore_default, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (86653, 10797936640), cmpt: ``, lut: 1710793443.53415
[0m17:24:03.536416 [debug] [ThreadPool]: Databricks adapter: conn: 6070939408: get_thread_connection: sess: None, name: list_hive_metastore_default, idle: 0.0022172927856445312s, acqrelcnt: 1, lang: None, thrd: (86653, 10797936640), cmpt: ``, lut: 1710793443.53415
[0m17:24:03.536581 [debug] [ThreadPool]: Databricks adapter: conn: 6070939408: idle check connection: sess: None, name: list_hive_metastore_default, idle: 0.002382040023803711s, acqrelcnt: 1, lang: None, thrd: (86653, 10797936640), cmpt: ``, lut: 1710793443.53415
[0m17:24:03.536732 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:24:03.536872 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m17:24:03.537002 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:24:04.120638 [debug] [ThreadPool]: Databricks adapter: conn: 6070939408: session opened sess: 01eee565-76aa-177d-b9be-59b1e17fa662, name: list_hive_metastore_default, idle: 1.71661376953125e-05s, acqrelcnt: 1, lang: None, thrd: (86653, 10797936640), cmpt: ``, lut: 1710793444.120144
[0m17:24:04.436137 [debug] [ThreadPool]: SQL status: OK in 0.8999999761581421 seconds
[0m17:24:04.452612 [debug] [ThreadPool]: Databricks adapter: conn: 6070939408: get_thread_connection: sess: 01eee565-76aa-177d-b9be-59b1e17fa662, name: list_hive_metastore_default, idle: 0.33232617378234863s, acqrelcnt: 1, lang: None, thrd: (86653, 10797936640), cmpt: ``, lut: 1710793444.120144
[0m17:24:04.453039 [debug] [ThreadPool]: Databricks adapter: conn: 6070939408: idle check connection: sess: 01eee565-76aa-177d-b9be-59b1e17fa662, name: list_hive_metastore_default, idle: 0.3328101634979248s, acqrelcnt: 1, lang: None, thrd: (86653, 10797936640), cmpt: ``, lut: 1710793444.120144
[0m17:24:04.453321 [debug] [ThreadPool]: Databricks adapter: conn: 6070939408: get_thread_connection: sess: 01eee565-76aa-177d-b9be-59b1e17fa662, name: list_hive_metastore_default, idle: 0.3330988883972168s, acqrelcnt: 1, lang: None, thrd: (86653, 10797936640), cmpt: ``, lut: 1710793444.120144
[0m17:24:04.453589 [debug] [ThreadPool]: Databricks adapter: conn: 6070939408: idle check connection: sess: 01eee565-76aa-177d-b9be-59b1e17fa662, name: list_hive_metastore_default, idle: 0.33336520195007324s, acqrelcnt: 1, lang: None, thrd: (86653, 10797936640), cmpt: ``, lut: 1710793444.120144
[0m17:24:04.453840 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:24:04.454066 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:24:04.454317 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m17:24:04.721872 [debug] [ThreadPool]: SQL status: OK in 0.27000001072883606 seconds
[0m17:24:04.733967 [debug] [ThreadPool]: Databricks adapter: conn: 6070939408: get_thread_connection: sess: 01eee565-76aa-177d-b9be-59b1e17fa662, name: list_hive_metastore_default, idle: 0.6136980056762695s, acqrelcnt: 1, lang: None, thrd: (86653, 10797936640), cmpt: ``, lut: 1710793444.120144
[0m17:24:04.734352 [debug] [ThreadPool]: Databricks adapter: conn: 6070939408: idle check connection: sess: 01eee565-76aa-177d-b9be-59b1e17fa662, name: list_hive_metastore_default, idle: 0.6141149997711182s, acqrelcnt: 1, lang: None, thrd: (86653, 10797936640), cmpt: ``, lut: 1710793444.120144
[0m17:24:04.734636 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:24:04.734929 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m17:24:05.064712 [debug] [ThreadPool]: SQL status: OK in 0.33000001311302185 seconds
[0m17:24:05.070182 [debug] [ThreadPool]: Databricks adapter: conn: 6070939408: _release sess: 01eee565-76aa-177d-b9be-59b1e17fa662, name: list_hive_metastore_default, idle: 2.1457672119140625e-06s, acqrelcnt: 0, lang: None, thrd: (86653, 10797936640), cmpt: ``, lut: 1710793445.070032
[0m17:24:05.074062 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '66b13a87-46c2-4da9-9676-851ff246150b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1698cc670>]}
[0m17:24:05.074527 [debug] [MainThread]: Databricks adapter: conn: 6068264816: _release sess: None, name: master, idle: 1.6689300537109375e-06s, acqrelcnt: 0, lang: None, thrd: (86653, 7965269056), cmpt: ``, lut: 1710793445.0744262
[0m17:24:05.075224 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:24:05.075533 [info ] [MainThread]: 
[0m17:24:05.078990 [debug] [Thread-1  ]: Began running node model.default.stage_payments
[0m17:24:05.079848 [debug] [Thread-1  ]: Databricks adapter: conn: 6070939408: idle check connection: sess: 01eee565-76aa-177d-b9be-59b1e17fa662, name: list_hive_metastore_default, idle: 0.009650230407714844s, acqrelcnt: 0, lang: None, thrd: (86653, 10797936640), cmpt: ``, lut: 1710793445.070032
[0m17:24:05.080182 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.default.stage_payments)
[0m17:24:05.080551 [debug] [Thread-1  ]: Databricks adapter: conn: 6070939408: reusing connection list_hive_metastore_default sess: 01eee565-76aa-177d-b9be-59b1e17fa662, name: model.default.stage_payments, idle: 0.010372161865234375s, acqrelcnt: 0, lang: None, thrd: (86653, 10797936640), cmpt: ``, lut: 1710793445.070032
[0m17:24:05.080897 [debug] [Thread-1  ]: Databricks adapter: On thread (86653, 10797936640): `hive_metastore`.`default`.`stage_payments` using default compute resource.
[0m17:24:05.081227 [debug] [Thread-1  ]: Databricks adapter: conn: 6070939408: _acquire sess: 01eee565-76aa-177d-b9be-59b1e17fa662, name: model.default.stage_payments, idle: 0.011069059371948242s, acqrelcnt: 1, lang: sql, thrd: (86653, 10797936640), cmpt: ``, lut: 1710793445.070032
[0m17:24:05.081520 [debug] [Thread-1  ]: Began compiling node model.default.stage_payments
[0m17:24:05.089193 [debug] [Thread-1  ]: Writing injected SQL for node "model.default.stage_payments"
[0m17:24:05.090511 [debug] [Thread-1  ]: Timing info for model.default.stage_payments (compile): 17:24:05.081704 => 17:24:05.090339
[0m17:24:05.090776 [debug] [Thread-1  ]: Began executing node model.default.stage_payments
[0m17:24:05.096672 [debug] [Thread-1  ]: Databricks adapter: conn: 6070939408: get_thread_connection: sess: 01eee565-76aa-177d-b9be-59b1e17fa662, name: model.default.stage_payments, idle: 0.02651214599609375s, acqrelcnt: 1, lang: sql, thrd: (86653, 10797936640), cmpt: ``, lut: 1710793445.070032
[0m17:24:05.096966 [debug] [Thread-1  ]: Databricks adapter: conn: 6070939408: idle check connection: sess: 01eee565-76aa-177d-b9be-59b1e17fa662, name: model.default.stage_payments, idle: 0.0268251895904541s, acqrelcnt: 1, lang: sql, thrd: (86653, 10797936640), cmpt: ``, lut: 1710793445.070032
[0m17:24:05.097180 [debug] [Thread-1  ]: Using databricks connection "model.default.stage_payments"
[0m17:24:05.097467 [debug] [Thread-1  ]: On model.default.stage_payments: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_payments"} */

  
    select *
    from (
        

select 
CAST(FLOOR(random() * (10000 - 1 + 1)) + 1 AS INTEGER)
 AS id,
       user_id AS user_id,
       case when gender = 'm' then 'male' else 'female' end as gender,
       country_code AS location,
       credit_card_type AS credit_card_type,
       datetime AS datetime,
       time AS time
from `hive_metastore`.`default`.`payments`
    ) as model_limit_subq
    limit 5


[0m17:24:05.752925 [debug] [Thread-1  ]: SQL status: OK in 0.6499999761581421 seconds
[0m17:24:05.758864 [debug] [Thread-1  ]: Timing info for model.default.stage_payments (execute): 17:24:05.090932 => 17:24:05.758485
[0m17:24:05.759427 [debug] [Thread-1  ]: Databricks adapter: conn: 6070939408: _release sess: 01eee565-76aa-177d-b9be-59b1e17fa662, name: model.default.stage_payments, idle: 5.9604644775390625e-06s, acqrelcnt: 0, lang: sql, thrd: (86653, 10797936640), cmpt: ``, lut: 1710793445.759246
[0m17:24:05.760305 [debug] [Thread-1  ]: Databricks adapter: conn: 6070939408: _release sess: 01eee565-76aa-177d-b9be-59b1e17fa662, name: model.default.stage_payments, idle: 1.9073486328125e-06s, acqrelcnt: 0, lang: sql, thrd: (86653, 10797936640), cmpt: ``, lut: 1710793445.760148
[0m17:24:05.760892 [debug] [Thread-1  ]: Finished running node model.default.stage_payments
[0m17:24:05.761848 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:24:05.762223 [debug] [MainThread]: Connection 'model.default.stage_payments' was properly closed.
[0m17:24:05.762531 [debug] [MainThread]: On model.default.stage_payments: ROLLBACK
[0m17:24:05.762849 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:24:05.763126 [debug] [MainThread]: On model.default.stage_payments: Close
[0m17:24:05.930257 [debug] [MainThread]: Command end result
[0m17:24:05.955053 [info ] [MainThread]: Previewing node 'stage_payments':
|   id | user_id | gender | location | credit_card_type     |   datetime | ... |
| ---- | ------- | ------ | -------- | -------------------- | ---------- | --- |
| 5414 |       1 | female | BO       | mastercard           | 2019-07-01 | ... |
| 7723 |       2 | female | AZ       | jcb                  | 2019-08-11 | ... |
| 5441 |       3 | female | PT       | bankcard             | 2019-07-21 | ... |
|  876 |       4 | female | ID       | diners-club-carte... | 2020-05-06 | ... |
| 4187 |       5 | female | ID       | jcb                  | 2020-03-25 | ... |

[0m17:24:05.957888 [debug] [MainThread]: Resource report: {"command_name": "show", "command_success": true, "command_wall_clock_time": 3.5222263, "process_user_time": 2.257762, "process_kernel_time": 3.37636, "process_mem_max_rss": "206749696", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m17:24:05.958185 [debug] [MainThread]: Command `dbt show` succeeded at 17:24:05.958118 after 3.52 seconds
[0m17:24:05.958402 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10595a160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1687746a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107c3aeb0>]}
[0m17:24:05.958607 [debug] [MainThread]: Flushing usage events
[0m17:24:33.091456 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f59dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1129b44f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1129f6cd0>]}


============================== 17:24:33.095038 | 2a18b487-6d96-44a1-9bc6-4862ec737835 ==============================
[0m17:24:33.095038 [info ] [MainThread]: Running with dbt=1.7.7
[0m17:24:33.095300 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt show --select stage_payments.sql', 'send_anonymous_usage_stats': 'True'}
[0m17:24:34.261168 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2a18b487-6d96-44a1-9bc6-4862ec737835', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1129f6e20>]}
[0m17:24:34.303396 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2a18b487-6d96-44a1-9bc6-4862ec737835', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1563b0070>]}
[0m17:24:34.303711 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m17:24:34.318856 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m17:24:34.391268 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m17:24:34.391620 [debug] [MainThread]: Partial parsing: updated file: default://models/stage/stage_payments.sql
[0m17:24:34.462877 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2a18b487-6d96-44a1-9bc6-4862ec737835', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1569f00d0>]}
[0m17:24:34.484500 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2a18b487-6d96-44a1-9bc6-4862ec737835', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15679b040>]}
[0m17:24:34.484751 [info ] [MainThread]: Found 3 models, 3 sources, 0 exposures, 0 metrics, 536 macros, 0 groups, 0 semantic models
[0m17:24:34.484932 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2a18b487-6d96-44a1-9bc6-4862ec737835', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x156866eb0>]}
[0m17:24:34.485630 [info ] [MainThread]: 
[0m17:24:34.486074 [debug] [MainThread]: Databricks adapter: conn: 5746618176: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (86742, 7965269056), cmpt: ``, lut: None
[0m17:24:34.486238 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:24:34.486373 [debug] [MainThread]: Databricks adapter: Thread (86742, 7965269056) using default compute resource.
[0m17:24:34.486510 [debug] [MainThread]: Databricks adapter: conn: 5746618176: _acquire sess: None, name: master, idle: 2.1457672119140625e-06s, acqrelcnt: 1, lang: None, thrd: (86742, 7965269056), cmpt: ``, lut: 1710793474.486465
[0m17:24:34.487083 [debug] [ThreadPool]: Databricks adapter: conn: 5748228928: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore_default, idle: 0s, acqrelcnt: 0, lang: None, thrd: (86742, 11559530496), cmpt: ``, lut: None
[0m17:24:34.487296 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_default'
[0m17:24:34.487454 [debug] [ThreadPool]: Databricks adapter: Thread (86742, 11559530496) using default compute resource.
[0m17:24:34.487605 [debug] [ThreadPool]: Databricks adapter: conn: 5748228928: _acquire sess: None, name: list_hive_metastore_default, idle: 2.1457672119140625e-06s, acqrelcnt: 1, lang: None, thrd: (86742, 11559530496), cmpt: ``, lut: 1710793474.487558
[0m17:24:34.489995 [debug] [ThreadPool]: Databricks adapter: conn: 5748228928: get_thread_connection: sess: None, name: list_hive_metastore_default, idle: 0.0023851394653320312s, acqrelcnt: 1, lang: None, thrd: (86742, 11559530496), cmpt: ``, lut: 1710793474.487558
[0m17:24:34.490164 [debug] [ThreadPool]: Databricks adapter: conn: 5748228928: idle check connection: sess: None, name: list_hive_metastore_default, idle: 0.0025610923767089844s, acqrelcnt: 1, lang: None, thrd: (86742, 11559530496), cmpt: ``, lut: 1710793474.487558
[0m17:24:34.490308 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:24:34.490448 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m17:24:34.490586 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:24:35.130290 [debug] [ThreadPool]: Databricks adapter: conn: 5748228928: session opened sess: 01eee565-8924-1a4b-be8b-a19438247358, name: list_hive_metastore_default, idle: 1.3828277587890625e-05s, acqrelcnt: 1, lang: None, thrd: (86742, 11559530496), cmpt: ``, lut: 1710793475.129816
[0m17:24:35.461687 [debug] [ThreadPool]: SQL status: OK in 0.9700000286102295 seconds
[0m17:24:35.481930 [debug] [ThreadPool]: Databricks adapter: conn: 5748228928: get_thread_connection: sess: 01eee565-8924-1a4b-be8b-a19438247358, name: list_hive_metastore_default, idle: 0.35199499130249023s, acqrelcnt: 1, lang: None, thrd: (86742, 11559530496), cmpt: ``, lut: 1710793475.129816
[0m17:24:35.482290 [debug] [ThreadPool]: Databricks adapter: conn: 5748228928: idle check connection: sess: 01eee565-8924-1a4b-be8b-a19438247358, name: list_hive_metastore_default, idle: 0.3524010181427002s, acqrelcnt: 1, lang: None, thrd: (86742, 11559530496), cmpt: ``, lut: 1710793475.129816
[0m17:24:35.482527 [debug] [ThreadPool]: Databricks adapter: conn: 5748228928: get_thread_connection: sess: 01eee565-8924-1a4b-be8b-a19438247358, name: list_hive_metastore_default, idle: 0.3526439666748047s, acqrelcnt: 1, lang: None, thrd: (86742, 11559530496), cmpt: ``, lut: 1710793475.129816
[0m17:24:35.482746 [debug] [ThreadPool]: Databricks adapter: conn: 5748228928: idle check connection: sess: 01eee565-8924-1a4b-be8b-a19438247358, name: list_hive_metastore_default, idle: 0.35286784172058105s, acqrelcnt: 1, lang: None, thrd: (86742, 11559530496), cmpt: ``, lut: 1710793475.129816
[0m17:24:35.482950 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:24:35.483137 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:24:35.483348 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m17:24:35.782639 [debug] [ThreadPool]: SQL status: OK in 0.30000001192092896 seconds
[0m17:24:35.791522 [debug] [ThreadPool]: Databricks adapter: conn: 5748228928: get_thread_connection: sess: 01eee565-8924-1a4b-be8b-a19438247358, name: list_hive_metastore_default, idle: 0.661564826965332s, acqrelcnt: 1, lang: None, thrd: (86742, 11559530496), cmpt: ``, lut: 1710793475.129816
[0m17:24:35.791955 [debug] [ThreadPool]: Databricks adapter: conn: 5748228928: idle check connection: sess: 01eee565-8924-1a4b-be8b-a19438247358, name: list_hive_metastore_default, idle: 0.6620380878448486s, acqrelcnt: 1, lang: None, thrd: (86742, 11559530496), cmpt: ``, lut: 1710793475.129816
[0m17:24:35.792270 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:24:35.792571 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m17:24:36.105595 [debug] [ThreadPool]: SQL status: OK in 0.3100000023841858 seconds
[0m17:24:36.113263 [debug] [ThreadPool]: Databricks adapter: conn: 5748228928: _release sess: 01eee565-8924-1a4b-be8b-a19438247358, name: list_hive_metastore_default, idle: 5.9604644775390625e-06s, acqrelcnt: 0, lang: None, thrd: (86742, 11559530496), cmpt: ``, lut: 1710793476.113049
[0m17:24:36.117623 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2a18b487-6d96-44a1-9bc6-4862ec737835', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x156592d00>]}
[0m17:24:36.118324 [debug] [MainThread]: Databricks adapter: conn: 5746618176: _release sess: None, name: master, idle: 4.0531158447265625e-06s, acqrelcnt: 0, lang: None, thrd: (86742, 7965269056), cmpt: ``, lut: 1710793476.118208
[0m17:24:36.119121 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:24:36.119426 [info ] [MainThread]: 
[0m17:24:36.123720 [debug] [Thread-1  ]: Began running node model.default.stage_payments
[0m17:24:36.124507 [debug] [Thread-1  ]: Databricks adapter: conn: 5748228928: idle check connection: sess: 01eee565-8924-1a4b-be8b-a19438247358, name: list_hive_metastore_default, idle: 0.011312007904052734s, acqrelcnt: 0, lang: None, thrd: (86742, 11559530496), cmpt: ``, lut: 1710793476.113049
[0m17:24:36.124795 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.default.stage_payments)
[0m17:24:36.125112 [debug] [Thread-1  ]: Databricks adapter: conn: 5748228928: reusing connection list_hive_metastore_default sess: 01eee565-8924-1a4b-be8b-a19438247358, name: model.default.stage_payments, idle: 0.011937856674194336s, acqrelcnt: 0, lang: None, thrd: (86742, 11559530496), cmpt: ``, lut: 1710793476.113049
[0m17:24:36.125408 [debug] [Thread-1  ]: Databricks adapter: On thread (86742, 11559530496): `hive_metastore`.`default`.`stage_payments` using default compute resource.
[0m17:24:36.125709 [debug] [Thread-1  ]: Databricks adapter: conn: 5748228928: _acquire sess: 01eee565-8924-1a4b-be8b-a19438247358, name: model.default.stage_payments, idle: 0.012534856796264648s, acqrelcnt: 1, lang: sql, thrd: (86742, 11559530496), cmpt: ``, lut: 1710793476.113049
[0m17:24:36.125993 [debug] [Thread-1  ]: Began compiling node model.default.stage_payments
[0m17:24:36.133878 [debug] [Thread-1  ]: Writing injected SQL for node "model.default.stage_payments"
[0m17:24:36.135472 [debug] [Thread-1  ]: Timing info for model.default.stage_payments (compile): 17:24:36.126180 => 17:24:36.135278
[0m17:24:36.135754 [debug] [Thread-1  ]: Began executing node model.default.stage_payments
[0m17:24:36.141713 [debug] [Thread-1  ]: Databricks adapter: conn: 5748228928: get_thread_connection: sess: 01eee565-8924-1a4b-be8b-a19438247358, name: model.default.stage_payments, idle: 0.028535842895507812s, acqrelcnt: 1, lang: sql, thrd: (86742, 11559530496), cmpt: ``, lut: 1710793476.113049
[0m17:24:36.142009 [debug] [Thread-1  ]: Databricks adapter: conn: 5748228928: idle check connection: sess: 01eee565-8924-1a4b-be8b-a19438247358, name: model.default.stage_payments, idle: 0.028848886489868164s, acqrelcnt: 1, lang: sql, thrd: (86742, 11559530496), cmpt: ``, lut: 1710793476.113049
[0m17:24:36.142224 [debug] [Thread-1  ]: Using databricks connection "model.default.stage_payments"
[0m17:24:36.142507 [debug] [Thread-1  ]: On model.default.stage_payments: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_payments"} */

  
    select *
    from (
        

select 
CAST(FLOOR(random() * (10000 - 1 + 1)) + 1 AS INTEGER)
 AS id,
       user_id AS user_id,
       case when gender = 'M' then 'male' else 'female' end as gender,
       country_code AS location,
       credit_card_type AS credit_card_type,
       datetime AS datetime,
       time AS time
from `hive_metastore`.`default`.`payments`
    ) as model_limit_subq
    limit 5


[0m17:24:36.540811 [debug] [Thread-1  ]: SQL status: OK in 0.4000000059604645 seconds
[0m17:24:36.548150 [debug] [Thread-1  ]: Timing info for model.default.stage_payments (execute): 17:24:36.135912 => 17:24:36.547767
[0m17:24:36.548753 [debug] [Thread-1  ]: Databricks adapter: conn: 5748228928: _release sess: 01eee565-8924-1a4b-be8b-a19438247358, name: model.default.stage_payments, idle: 4.76837158203125e-06s, acqrelcnt: 0, lang: sql, thrd: (86742, 11559530496), cmpt: ``, lut: 1710793476.5485592
[0m17:24:36.549630 [debug] [Thread-1  ]: Databricks adapter: conn: 5748228928: _release sess: 01eee565-8924-1a4b-be8b-a19438247358, name: model.default.stage_payments, idle: 2.1457672119140625e-06s, acqrelcnt: 0, lang: sql, thrd: (86742, 11559530496), cmpt: ``, lut: 1710793476.549474
[0m17:24:36.550232 [debug] [Thread-1  ]: Finished running node model.default.stage_payments
[0m17:24:36.551185 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:24:36.551547 [debug] [MainThread]: Connection 'model.default.stage_payments' was properly closed.
[0m17:24:36.551852 [debug] [MainThread]: On model.default.stage_payments: ROLLBACK
[0m17:24:36.552154 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:24:36.552426 [debug] [MainThread]: On model.default.stage_payments: Close
[0m17:24:36.720933 [debug] [MainThread]: Command end result
[0m17:24:36.772971 [info ] [MainThread]: Previewing node 'stage_payments':
|   id | user_id | gender | location | credit_card_type     |   datetime | ... |
| ---- | ------- | ------ | -------- | -------------------- | ---------- | --- |
| 3009 |       1 | female | BO       | mastercard           | 2019-07-01 | ... |
| 9670 |       2 | female | AZ       | jcb                  | 2019-08-11 | ... |
| 8092 |       3 | male   | PT       | bankcard             | 2019-07-21 | ... |
|  138 |       4 | male   | ID       | diners-club-carte... | 2020-05-06 | ... |
| 1783 |       5 | female | ID       | jcb                  | 2020-03-25 | ... |

[0m17:24:36.778715 [debug] [MainThread]: Resource report: {"command_name": "show", "command_success": true, "command_wall_clock_time": 3.7321777, "process_user_time": 2.521573, "process_kernel_time": 3.27819, "process_mem_max_rss": "211238912", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m17:24:36.779401 [debug] [MainThread]: Command `dbt show` succeeded at 17:24:36.779262 after 3.73 seconds
[0m17:24:36.779863 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f59dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15693c640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1563b0070>]}
[0m17:24:36.780324 [debug] [MainThread]: Flushing usage events
[0m17:26:32.067401 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a25df50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a2c1910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a2c1f10>]}


============================== 17:26:32.069731 | 97f5e603-bb8a-4f70-a68c-3a2a3a01a16d ==============================
[0m17:26:32.069731 [info ] [MainThread]: Running with dbt=1.7.8
[0m17:26:32.070021 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m17:26:33.094579 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '97f5e603-bb8a-4f70-a68c-3a2a3a01a16d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x14f591690>]}
[0m17:26:33.123266 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '97f5e603-bb8a-4f70-a68c-3a2a3a01a16d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a262350>]}
[0m17:26:33.123541 [info ] [MainThread]: Registered adapter: databricks=1.7.9
[0m17:26:33.141200 [debug] [MainThread]: checksum: 67f0013ca5f0bd43af9a0873dd50792fde83ef69de63b71cacd0b4ac656c52e5, vars: {}, profile: , target: , version: 1.7.8
[0m17:26:33.222141 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:26:33.222403 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:26:33.225367 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '97f5e603-bb8a-4f70-a68c-3a2a3a01a16d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x14f591690>]}
[0m17:26:33.233331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '97f5e603-bb8a-4f70-a68c-3a2a3a01a16d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x14f69c790>]}
[0m17:26:33.233592 [info ] [MainThread]: Found 3 models, 3 sources, 0 exposures, 0 metrics, 539 macros, 0 groups, 0 semantic models
[0m17:26:33.233760 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '97f5e603-bb8a-4f70-a68c-3a2a3a01a16d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x14f3a7cd0>]}
[0m17:26:33.234488 [info ] [MainThread]: 
[0m17:26:33.234962 [debug] [MainThread]: Databricks adapter: conn: 5627301584: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (87258, 7965269056), cmpt: ``, lut: None
[0m17:26:33.235109 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:26:33.235220 [debug] [MainThread]: Databricks adapter: Thread (87258, 7965269056) using default compute resource.
[0m17:26:33.235335 [debug] [MainThread]: Databricks adapter: conn: 5627301584: _acquire sess: None, name: master, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (87258, 7965269056), cmpt: ``, lut: 1710793593.235296
[0m17:26:33.235892 [debug] [ThreadPool]: Databricks adapter: conn: 5627307024: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore, idle: 0s, acqrelcnt: 0, lang: None, thrd: (87258, 6112702464), cmpt: ``, lut: None
[0m17:26:33.236079 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m17:26:33.236209 [debug] [ThreadPool]: Databricks adapter: Thread (87258, 6112702464) using default compute resource.
[0m17:26:33.236329 [debug] [ThreadPool]: Databricks adapter: conn: 5627307024: _acquire sess: None, name: list_hive_metastore, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (87258, 6112702464), cmpt: ``, lut: 1710793593.2362921
[0m17:26:33.236461 [debug] [ThreadPool]: Databricks adapter: conn: 5627307024: get_thread_connection: sess: None, name: list_hive_metastore, idle: 0.0001327991485595703s, acqrelcnt: 1, lang: None, thrd: (87258, 6112702464), cmpt: ``, lut: 1710793593.2362921
[0m17:26:33.236584 [debug] [ThreadPool]: Databricks adapter: conn: 5627307024: idle check connection: sess: None, name: list_hive_metastore, idle: 0.00025177001953125s, acqrelcnt: 1, lang: None, thrd: (87258, 6112702464), cmpt: ``, lut: 1710793593.2362921
[0m17:26:33.236688 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m17:26:33.236798 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m17:26:33.236902 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:26:33.750143 [debug] [ThreadPool]: Databricks adapter: conn: 5627307024: session opened sess: 01eee565-cfd7-170a-ade1-9119bea498dc, name: list_hive_metastore, idle: 2.86102294921875e-06s, acqrelcnt: 1, lang: None, thrd: (87258, 6112702464), cmpt: ``, lut: 1710793593.750051
[0m17:26:34.074751 [debug] [ThreadPool]: SQL status: OK in 0.8399999737739563 seconds
[0m17:26:34.086985 [debug] [ThreadPool]: Databricks adapter: conn: 5627307024: _release sess: 01eee565-cfd7-170a-ade1-9119bea498dc, name: list_hive_metastore, idle: 4.291534423828125e-06s, acqrelcnt: 0, lang: None, thrd: (87258, 6112702464), cmpt: ``, lut: 1710793594.0868669
[0m17:26:34.088287 [debug] [ThreadPool]: Databricks adapter: conn: 5627307024: idle check connection: sess: 01eee565-cfd7-170a-ade1-9119bea498dc, name: list_hive_metastore, idle: 0.0013401508331298828s, acqrelcnt: 0, lang: None, thrd: (87258, 6112702464), cmpt: ``, lut: 1710793594.0868669
[0m17:26:34.088540 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now list_hive_metastore_default)
[0m17:26:34.088737 [debug] [ThreadPool]: Databricks adapter: conn: 5627307024: reusing connection list_hive_metastore sess: 01eee565-cfd7-170a-ade1-9119bea498dc, name: list_hive_metastore_default, idle: 0.001810312271118164s, acqrelcnt: 0, lang: None, thrd: (87258, 6112702464), cmpt: ``, lut: 1710793594.0868669
[0m17:26:34.088915 [debug] [ThreadPool]: Databricks adapter: Thread (87258, 6112702464) using default compute resource.
[0m17:26:34.089094 [debug] [ThreadPool]: Databricks adapter: conn: 5627307024: _acquire sess: 01eee565-cfd7-170a-ade1-9119bea498dc, name: list_hive_metastore_default, idle: 0.0021712779998779297s, acqrelcnt: 1, lang: None, thrd: (87258, 6112702464), cmpt: ``, lut: 1710793594.0868669
[0m17:26:34.092208 [debug] [ThreadPool]: Databricks adapter: conn: 5627307024: get_thread_connection: sess: 01eee565-cfd7-170a-ade1-9119bea498dc, name: list_hive_metastore_default, idle: 0.005273103713989258s, acqrelcnt: 1, lang: None, thrd: (87258, 6112702464), cmpt: ``, lut: 1710793594.0868669
[0m17:26:34.092413 [debug] [ThreadPool]: Databricks adapter: conn: 5627307024: idle check connection: sess: 01eee565-cfd7-170a-ade1-9119bea498dc, name: list_hive_metastore_default, idle: 0.005494117736816406s, acqrelcnt: 1, lang: None, thrd: (87258, 6112702464), cmpt: ``, lut: 1710793594.0868669
[0m17:26:34.092577 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:26:34.092733 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m17:26:34.434562 [debug] [ThreadPool]: SQL status: OK in 0.3400000035762787 seconds
[0m17:26:34.446077 [debug] [ThreadPool]: Databricks adapter: conn: 5627307024: get_thread_connection: sess: 01eee565-cfd7-170a-ade1-9119bea498dc, name: list_hive_metastore_default, idle: 0.35907936096191406s, acqrelcnt: 1, lang: None, thrd: (87258, 6112702464), cmpt: ``, lut: 1710793594.0868669
[0m17:26:34.446487 [debug] [ThreadPool]: Databricks adapter: conn: 5627307024: idle check connection: sess: 01eee565-cfd7-170a-ade1-9119bea498dc, name: list_hive_metastore_default, idle: 0.35953211784362793s, acqrelcnt: 1, lang: None, thrd: (87258, 6112702464), cmpt: ``, lut: 1710793594.0868669
[0m17:26:34.446770 [debug] [ThreadPool]: Databricks adapter: conn: 5627307024: get_thread_connection: sess: 01eee565-cfd7-170a-ade1-9119bea498dc, name: list_hive_metastore_default, idle: 0.35982704162597656s, acqrelcnt: 1, lang: None, thrd: (87258, 6112702464), cmpt: ``, lut: 1710793594.0868669
[0m17:26:34.447026 [debug] [ThreadPool]: Databricks adapter: conn: 5627307024: idle check connection: sess: 01eee565-cfd7-170a-ade1-9119bea498dc, name: list_hive_metastore_default, idle: 0.36008501052856445s, acqrelcnt: 1, lang: None, thrd: (87258, 6112702464), cmpt: ``, lut: 1710793594.0868669
[0m17:26:34.447277 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:26:34.447490 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:26:34.447732 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m17:26:34.712368 [debug] [ThreadPool]: SQL status: OK in 0.25999999046325684 seconds
[0m17:26:34.718969 [debug] [ThreadPool]: Databricks adapter: conn: 5627307024: get_thread_connection: sess: 01eee565-cfd7-170a-ade1-9119bea498dc, name: list_hive_metastore_default, idle: 0.6319842338562012s, acqrelcnt: 1, lang: None, thrd: (87258, 6112702464), cmpt: ``, lut: 1710793594.0868669
[0m17:26:34.719321 [debug] [ThreadPool]: Databricks adapter: conn: 5627307024: idle check connection: sess: 01eee565-cfd7-170a-ade1-9119bea498dc, name: list_hive_metastore_default, idle: 0.6323721408843994s, acqrelcnt: 1, lang: None, thrd: (87258, 6112702464), cmpt: ``, lut: 1710793594.0868669
[0m17:26:34.719570 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:26:34.719825 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m17:26:35.016433 [debug] [ThreadPool]: SQL status: OK in 0.30000001192092896 seconds
[0m17:26:35.021367 [debug] [ThreadPool]: Databricks adapter: conn: 5627307024: _release sess: 01eee565-cfd7-170a-ade1-9119bea498dc, name: list_hive_metastore_default, idle: 3.814697265625e-06s, acqrelcnt: 0, lang: None, thrd: (87258, 6112702464), cmpt: ``, lut: 1710793595.021209
[0m17:26:35.025269 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '97f5e603-bb8a-4f70-a68c-3a2a3a01a16d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x14f3a7cd0>]}
[0m17:26:35.025766 [debug] [MainThread]: Databricks adapter: conn: 5627301584: get_thread_connection: sess: None, name: master, idle: 1.7903680801391602s, acqrelcnt: 1, lang: None, thrd: (87258, 7965269056), cmpt: ``, lut: 1710793593.235296
[0m17:26:35.026092 [debug] [MainThread]: Databricks adapter: conn: 5627301584: idle check connection: sess: None, name: master, idle: 1.790701150894165s, acqrelcnt: 1, lang: None, thrd: (87258, 7965269056), cmpt: ``, lut: 1710793593.235296
[0m17:26:35.026397 [debug] [MainThread]: Databricks adapter: conn: 5627301584: get_thread_connection: sess: None, name: master, idle: 1.7910149097442627s, acqrelcnt: 1, lang: None, thrd: (87258, 7965269056), cmpt: ``, lut: 1710793593.235296
[0m17:26:35.026695 [debug] [MainThread]: Databricks adapter: conn: 5627301584: idle check connection: sess: None, name: master, idle: 1.7913129329681396s, acqrelcnt: 1, lang: None, thrd: (87258, 7965269056), cmpt: ``, lut: 1710793593.235296
[0m17:26:35.026982 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:26:35.027238 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:26:35.027522 [debug] [MainThread]: Databricks adapter: conn: 5627301584: _release sess: None, name: master, idle: 1.9073486328125e-06s, acqrelcnt: 0, lang: None, thrd: (87258, 7965269056), cmpt: ``, lut: 1710793595.027431
[0m17:26:35.028117 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:26:35.028389 [info ] [MainThread]: 
[0m17:26:35.032296 [debug] [Thread-1 (]: Began running node model.default.stage_payments
[0m17:26:35.032713 [info ] [Thread-1 (]: 1 of 1 START sql view model default.stage_payments ............................. [RUN]
[0m17:26:35.033351 [debug] [Thread-1 (]: Databricks adapter: conn: 5627307024: idle check connection: sess: 01eee565-cfd7-170a-ade1-9119bea498dc, name: list_hive_metastore_default, idle: 0.01201486587524414s, acqrelcnt: 0, lang: None, thrd: (87258, 6112702464), cmpt: ``, lut: 1710793595.021209
[0m17:26:35.033611 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.default.stage_payments)
[0m17:26:35.033904 [debug] [Thread-1 (]: Databricks adapter: conn: 5627307024: reusing connection list_hive_metastore_default sess: 01eee565-cfd7-170a-ade1-9119bea498dc, name: model.default.stage_payments, idle: 0.01258087158203125s, acqrelcnt: 0, lang: None, thrd: (87258, 6112702464), cmpt: ``, lut: 1710793595.021209
[0m17:26:35.034178 [debug] [Thread-1 (]: Databricks adapter: On thread (87258, 6112702464): `hive_metastore`.`default`.`stage_payments` using default compute resource.
[0m17:26:35.034450 [debug] [Thread-1 (]: Databricks adapter: conn: 5627307024: _acquire sess: 01eee565-cfd7-170a-ade1-9119bea498dc, name: model.default.stage_payments, idle: 0.013135910034179688s, acqrelcnt: 1, lang: sql, thrd: (87258, 6112702464), cmpt: ``, lut: 1710793595.021209
[0m17:26:35.034712 [debug] [Thread-1 (]: Began compiling node model.default.stage_payments
[0m17:26:35.041306 [debug] [Thread-1 (]: Writing injected SQL for node "model.default.stage_payments"
[0m17:26:35.044010 [debug] [Thread-1 (]: Timing info for model.default.stage_payments (compile): 17:26:35.034879 => 17:26:35.043807
[0m17:26:35.044310 [debug] [Thread-1 (]: Began executing node model.default.stage_payments
[0m17:26:35.063675 [debug] [Thread-1 (]: Writing runtime sql for node "model.default.stage_payments"
[0m17:26:35.065697 [debug] [Thread-1 (]: Databricks adapter: conn: 5627307024: get_thread_connection: sess: 01eee565-cfd7-170a-ade1-9119bea498dc, name: model.default.stage_payments, idle: 0.04438519477844238s, acqrelcnt: 1, lang: sql, thrd: (87258, 6112702464), cmpt: ``, lut: 1710793595.021209
[0m17:26:35.065920 [debug] [Thread-1 (]: Databricks adapter: conn: 5627307024: idle check connection: sess: 01eee565-cfd7-170a-ade1-9119bea498dc, name: model.default.stage_payments, idle: 0.04463386535644531s, acqrelcnt: 1, lang: sql, thrd: (87258, 6112702464), cmpt: ``, lut: 1710793595.021209
[0m17:26:35.066080 [debug] [Thread-1 (]: Using databricks connection "model.default.stage_payments"
[0m17:26:35.066303 [debug] [Thread-1 (]: On model.default.stage_payments: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_payments"} */
create or replace view `hive_metastore`.`default`.`stage_payments`
  
  
  
  as
    

select 
CAST(FLOOR(random() * (10000 - 1 + 1)) + 1 AS INTEGER)
 AS id,
       user_id AS user_id,
       case when gender = 'M' then 'male' else 'female' end as gender,
       country_code AS location,
       credit_card_type AS credit_card_type,
       datetime AS datetime,
       time AS time
from `hive_metastore`.`default`.`payments`

[0m17:26:35.735361 [debug] [Thread-1 (]: SQL status: OK in 0.6700000166893005 seconds
[0m17:26:35.741603 [debug] [Thread-1 (]: Timing info for model.default.stage_payments (execute): 17:26:35.044474 => 17:26:35.741475
[0m17:26:35.741881 [debug] [Thread-1 (]: Databricks adapter: conn: 5627307024: _release sess: 01eee565-cfd7-170a-ade1-9119bea498dc, name: model.default.stage_payments, idle: 9.5367431640625e-07s, acqrelcnt: 0, lang: sql, thrd: (87258, 6112702464), cmpt: ``, lut: 1710793595.7417831
[0m17:26:35.742315 [debug] [Thread-1 (]: Databricks adapter: conn: 5627307024: _release sess: 01eee565-cfd7-170a-ade1-9119bea498dc, name: model.default.stage_payments, idle: 1.1920928955078125e-06s, acqrelcnt: 0, lang: sql, thrd: (87258, 6112702464), cmpt: ``, lut: 1710793595.74223
[0m17:26:35.742553 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '97f5e603-bb8a-4f70-a68c-3a2a3a01a16d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x14f5d6510>]}
[0m17:26:35.742879 [info ] [Thread-1 (]: 1 of 1 OK created sql view model default.stage_payments ........................ [[32mOK[0m in 0.71s]
[0m17:26:35.743175 [debug] [Thread-1 (]: Finished running node model.default.stage_payments
[0m17:26:35.744028 [debug] [MainThread]: Databricks adapter: conn: 5627301584: idle check connection: sess: None, name: master, idle: 0.7165317535400391s, acqrelcnt: 0, lang: None, thrd: (87258, 7965269056), cmpt: ``, lut: 1710793595.027431
[0m17:26:35.744253 [debug] [MainThread]: Databricks adapter: conn: 5627301584: reusing connection master sess: None, name: master, idle: 0.7167699337005615s, acqrelcnt: 0, lang: None, thrd: (87258, 7965269056), cmpt: ``, lut: 1710793595.027431
[0m17:26:35.744420 [debug] [MainThread]: Databricks adapter: Thread (87258, 7965269056) using default compute resource.
[0m17:26:35.744582 [debug] [MainThread]: Databricks adapter: conn: 5627301584: _acquire sess: None, name: master, idle: 0.7171030044555664s, acqrelcnt: 1, lang: None, thrd: (87258, 7965269056), cmpt: ``, lut: 1710793595.027431
[0m17:26:35.744757 [debug] [MainThread]: Databricks adapter: conn: 5627301584: get_thread_connection: sess: None, name: master, idle: 0.7172751426696777s, acqrelcnt: 1, lang: None, thrd: (87258, 7965269056), cmpt: ``, lut: 1710793595.027431
[0m17:26:35.744918 [debug] [MainThread]: Databricks adapter: conn: 5627301584: idle check connection: sess: None, name: master, idle: 0.7174382209777832s, acqrelcnt: 1, lang: None, thrd: (87258, 7965269056), cmpt: ``, lut: 1710793595.027431
[0m17:26:35.745069 [debug] [MainThread]: On master: ROLLBACK
[0m17:26:35.745217 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:26:36.195151 [debug] [MainThread]: Databricks adapter: conn: 5627301584: session opened sess: 01eee565-d14f-1d9c-a473-3871921cc259, name: master, idle: 3.814697265625e-06s, acqrelcnt: 1, lang: None, thrd: (87258, 7965269056), cmpt: ``, lut: 1710793596.194947
[0m17:26:36.195679 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:26:36.196000 [debug] [MainThread]: Databricks adapter: conn: 5627301584: get_thread_connection: sess: 01eee565-d14f-1d9c-a473-3871921cc259, name: master, idle: 0.0009670257568359375s, acqrelcnt: 1, lang: None, thrd: (87258, 7965269056), cmpt: ``, lut: 1710793596.194947
[0m17:26:36.196287 [debug] [MainThread]: Databricks adapter: conn: 5627301584: idle check connection: sess: 01eee565-d14f-1d9c-a473-3871921cc259, name: master, idle: 0.0012612342834472656s, acqrelcnt: 1, lang: None, thrd: (87258, 7965269056), cmpt: ``, lut: 1710793596.194947
[0m17:26:36.196632 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:26:36.197059 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:26:36.197262 [debug] [MainThread]: Databricks adapter: conn: 5627301584: _release sess: 01eee565-d14f-1d9c-a473-3871921cc259, name: master, idle: 2.1457672119140625e-06s, acqrelcnt: 0, lang: None, thrd: (87258, 7965269056), cmpt: ``, lut: 1710793596.197206
[0m17:26:36.197649 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:26:36.197780 [debug] [MainThread]: On master: ROLLBACK
[0m17:26:36.197917 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:26:36.198033 [debug] [MainThread]: On master: Close
[0m17:26:36.356944 [debug] [MainThread]: Connection 'model.default.stage_payments' was properly closed.
[0m17:26:36.359913 [debug] [MainThread]: On model.default.stage_payments: ROLLBACK
[0m17:26:36.360821 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:26:36.361530 [debug] [MainThread]: On model.default.stage_payments: Close
[0m17:26:36.547917 [info ] [MainThread]: 
[0m17:26:36.549025 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.31 seconds (3.31s).
[0m17:26:36.550348 [debug] [MainThread]: Command end result
[0m17:26:36.572010 [info ] [MainThread]: 
[0m17:26:36.572526 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:26:36.572814 [info ] [MainThread]: 
[0m17:26:36.573107 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m17:26:36.580960 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 4.5421405, "process_user_time": 1.924869, "process_kernel_time": 3.356217, "process_mem_max_rss": "220905472", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m17:26:36.581502 [debug] [MainThread]: Command `cli run` succeeded at 17:26:36.581396 after 4.54 seconds
[0m17:26:36.581873 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108836990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104effc50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a228d90>]}
[0m17:26:36.582159 [debug] [MainThread]: Flushing usage events
[0m17:26:39.059939 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12713dcd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1271c21d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1271c27d0>]}


============================== 17:26:39.062775 | b3735555-501d-421b-b61d-ef5c35ec3ac2 ==============================
[0m17:26:39.062775 [info ] [MainThread]: Running with dbt=1.7.8
[0m17:26:39.063098 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m17:26:40.186532 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b3735555-501d-421b-b61d-ef5c35ec3ac2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x28e621090>]}
[0m17:26:40.215201 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b3735555-501d-421b-b61d-ef5c35ec3ac2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12715ce90>]}
[0m17:26:40.215473 [info ] [MainThread]: Registered adapter: databricks=1.7.9
[0m17:26:40.243397 [debug] [MainThread]: checksum: 67f0013ca5f0bd43af9a0873dd50792fde83ef69de63b71cacd0b4ac656c52e5, vars: {}, profile: , target: , version: 1.7.8
[0m17:26:40.316497 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:26:40.316755 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:26:40.319619 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b3735555-501d-421b-b61d-ef5c35ec3ac2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16b6fa950>]}
[0m17:26:40.328765 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b3735555-501d-421b-b61d-ef5c35ec3ac2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x28e73c290>]}
[0m17:26:40.329005 [info ] [MainThread]: Found 3 models, 3 sources, 0 exposures, 0 metrics, 539 macros, 0 groups, 0 semantic models
[0m17:26:40.329176 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b3735555-501d-421b-b61d-ef5c35ec3ac2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x126dc6950>]}
[0m17:26:40.329867 [info ] [MainThread]: 
[0m17:26:40.330310 [debug] [MainThread]: Databricks adapter: conn: 10979891088: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (87287, 7965269056), cmpt: ``, lut: None
[0m17:26:40.330453 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:26:40.330564 [debug] [MainThread]: Databricks adapter: Thread (87287, 7965269056) using default compute resource.
[0m17:26:40.330673 [debug] [MainThread]: Databricks adapter: conn: 10979891088: _acquire sess: None, name: master, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (87287, 7965269056), cmpt: ``, lut: 1710793600.3306382
[0m17:26:40.331181 [debug] [ThreadPool]: Databricks adapter: conn: 10979894032: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore, idle: 0s, acqrelcnt: 0, lang: None, thrd: (87287, 10997100544), cmpt: ``, lut: None
[0m17:26:40.331416 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m17:26:40.331578 [debug] [ThreadPool]: Databricks adapter: Thread (87287, 10997100544) using default compute resource.
[0m17:26:40.331702 [debug] [ThreadPool]: Databricks adapter: conn: 10979894032: _acquire sess: None, name: list_hive_metastore, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (87287, 10997100544), cmpt: ``, lut: 1710793600.331666
[0m17:26:40.331836 [debug] [ThreadPool]: Databricks adapter: conn: 10979894032: get_thread_connection: sess: None, name: list_hive_metastore, idle: 0.00013399124145507812s, acqrelcnt: 1, lang: None, thrd: (87287, 10997100544), cmpt: ``, lut: 1710793600.331666
[0m17:26:40.331956 [debug] [ThreadPool]: Databricks adapter: conn: 10979894032: idle check connection: sess: None, name: list_hive_metastore, idle: 0.0002532005310058594s, acqrelcnt: 1, lang: None, thrd: (87287, 10997100544), cmpt: ``, lut: 1710793600.331666
[0m17:26:40.332060 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m17:26:40.332176 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m17:26:40.332278 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:26:40.824106 [debug] [ThreadPool]: Databricks adapter: conn: 10979894032: session opened sess: 01eee565-d40e-1580-9592-9773eb6f9256, name: list_hive_metastore, idle: 1.6927719116210938e-05s, acqrelcnt: 1, lang: None, thrd: (87287, 10997100544), cmpt: ``, lut: 1710793600.823528
[0m17:26:41.105728 [debug] [ThreadPool]: SQL status: OK in 0.7699999809265137 seconds
[0m17:26:41.116126 [debug] [ThreadPool]: Databricks adapter: conn: 10979894032: _release sess: 01eee565-d40e-1580-9592-9773eb6f9256, name: list_hive_metastore, idle: 1.2159347534179688e-05s, acqrelcnt: 0, lang: None, thrd: (87287, 10997100544), cmpt: ``, lut: 1710793601.115413
[0m17:26:41.119689 [debug] [ThreadPool]: Databricks adapter: conn: 10979894032: idle check connection: sess: 01eee565-d40e-1580-9592-9773eb6f9256, name: list_hive_metastore, idle: 0.004084110260009766s, acqrelcnt: 0, lang: None, thrd: (87287, 10997100544), cmpt: ``, lut: 1710793601.115413
[0m17:26:41.120457 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now list_hive_metastore_default)
[0m17:26:41.121039 [debug] [ThreadPool]: Databricks adapter: conn: 10979894032: reusing connection list_hive_metastore sess: 01eee565-d40e-1580-9592-9773eb6f9256, name: list_hive_metastore_default, idle: 0.005478858947753906s, acqrelcnt: 0, lang: None, thrd: (87287, 10997100544), cmpt: ``, lut: 1710793601.115413
[0m17:26:41.121532 [debug] [ThreadPool]: Databricks adapter: Thread (87287, 10997100544) using default compute resource.
[0m17:26:41.121970 [debug] [ThreadPool]: Databricks adapter: conn: 10979894032: _acquire sess: 01eee565-d40e-1580-9592-9773eb6f9256, name: list_hive_metastore_default, idle: 0.006453990936279297s, acqrelcnt: 1, lang: None, thrd: (87287, 10997100544), cmpt: ``, lut: 1710793601.115413
[0m17:26:41.130177 [debug] [ThreadPool]: Databricks adapter: conn: 10979894032: get_thread_connection: sess: 01eee565-d40e-1580-9592-9773eb6f9256, name: list_hive_metastore_default, idle: 0.014603137969970703s, acqrelcnt: 1, lang: None, thrd: (87287, 10997100544), cmpt: ``, lut: 1710793601.115413
[0m17:26:41.130613 [debug] [ThreadPool]: Databricks adapter: conn: 10979894032: idle check connection: sess: 01eee565-d40e-1580-9592-9773eb6f9256, name: list_hive_metastore_default, idle: 0.015119075775146484s, acqrelcnt: 1, lang: None, thrd: (87287, 10997100544), cmpt: ``, lut: 1710793601.115413
[0m17:26:41.131185 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:26:41.131733 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m17:26:41.434905 [debug] [ThreadPool]: SQL status: OK in 0.30000001192092896 seconds
[0m17:26:41.459766 [debug] [ThreadPool]: Databricks adapter: conn: 10979894032: get_thread_connection: sess: 01eee565-d40e-1580-9592-9773eb6f9256, name: list_hive_metastore_default, idle: 0.34415698051452637s, acqrelcnt: 1, lang: None, thrd: (87287, 10997100544), cmpt: ``, lut: 1710793601.115413
[0m17:26:41.460374 [debug] [ThreadPool]: Databricks adapter: conn: 10979894032: idle check connection: sess: 01eee565-d40e-1580-9592-9773eb6f9256, name: list_hive_metastore_default, idle: 0.34485816955566406s, acqrelcnt: 1, lang: None, thrd: (87287, 10997100544), cmpt: ``, lut: 1710793601.115413
[0m17:26:41.460714 [debug] [ThreadPool]: Databricks adapter: conn: 10979894032: get_thread_connection: sess: 01eee565-d40e-1580-9592-9773eb6f9256, name: list_hive_metastore_default, idle: 0.34521007537841797s, acqrelcnt: 1, lang: None, thrd: (87287, 10997100544), cmpt: ``, lut: 1710793601.115413
[0m17:26:41.461017 [debug] [ThreadPool]: Databricks adapter: conn: 10979894032: idle check connection: sess: 01eee565-d40e-1580-9592-9773eb6f9256, name: list_hive_metastore_default, idle: 0.3455171585083008s, acqrelcnt: 1, lang: None, thrd: (87287, 10997100544), cmpt: ``, lut: 1710793601.115413
[0m17:26:41.461839 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:26:41.462114 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:26:41.462391 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m17:26:41.713029 [debug] [ThreadPool]: SQL status: OK in 0.25 seconds
[0m17:26:41.717462 [debug] [ThreadPool]: Databricks adapter: conn: 10979894032: get_thread_connection: sess: 01eee565-d40e-1580-9592-9773eb6f9256, name: list_hive_metastore_default, idle: 0.6019470691680908s, acqrelcnt: 1, lang: None, thrd: (87287, 10997100544), cmpt: ``, lut: 1710793601.115413
[0m17:26:41.717754 [debug] [ThreadPool]: Databricks adapter: conn: 10979894032: idle check connection: sess: 01eee565-d40e-1580-9592-9773eb6f9256, name: list_hive_metastore_default, idle: 0.6022882461547852s, acqrelcnt: 1, lang: None, thrd: (87287, 10997100544), cmpt: ``, lut: 1710793601.115413
[0m17:26:41.717914 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:26:41.718073 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m17:26:42.012878 [debug] [ThreadPool]: SQL status: OK in 0.28999999165534973 seconds
[0m17:26:42.015488 [debug] [ThreadPool]: Databricks adapter: conn: 10979894032: _release sess: 01eee565-d40e-1580-9592-9773eb6f9256, name: list_hive_metastore_default, idle: 2.1457672119140625e-06s, acqrelcnt: 0, lang: None, thrd: (87287, 10997100544), cmpt: ``, lut: 1710793602.015401
[0m17:26:42.017504 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b3735555-501d-421b-b61d-ef5c35ec3ac2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16b723910>]}
[0m17:26:42.017811 [debug] [MainThread]: Databricks adapter: conn: 10979891088: get_thread_connection: sess: None, name: master, idle: 1.687119722366333s, acqrelcnt: 1, lang: None, thrd: (87287, 7965269056), cmpt: ``, lut: 1710793600.3306382
[0m17:26:42.017971 [debug] [MainThread]: Databricks adapter: conn: 10979891088: idle check connection: sess: None, name: master, idle: 1.6872897148132324s, acqrelcnt: 1, lang: None, thrd: (87287, 7965269056), cmpt: ``, lut: 1710793600.3306382
[0m17:26:42.018121 [debug] [MainThread]: Databricks adapter: conn: 10979891088: get_thread_connection: sess: None, name: master, idle: 1.6874396800994873s, acqrelcnt: 1, lang: None, thrd: (87287, 7965269056), cmpt: ``, lut: 1710793600.3306382
[0m17:26:42.018263 [debug] [MainThread]: Databricks adapter: conn: 10979891088: idle check connection: sess: None, name: master, idle: 1.6875848770141602s, acqrelcnt: 1, lang: None, thrd: (87287, 7965269056), cmpt: ``, lut: 1710793600.3306382
[0m17:26:42.018397 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:26:42.018527 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:26:42.018670 [debug] [MainThread]: Databricks adapter: conn: 10979891088: _release sess: None, name: master, idle: 0.0s, acqrelcnt: 0, lang: None, thrd: (87287, 7965269056), cmpt: ``, lut: 1710793602.0186281
[0m17:26:42.019036 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:26:42.019213 [info ] [MainThread]: 
[0m17:26:42.022681 [debug] [Thread-1 (]: Began running node model.default.stage_payments
[0m17:26:42.022953 [info ] [Thread-1 (]: 1 of 1 START sql view model default.stage_payments ............................. [RUN]
[0m17:26:42.023382 [debug] [Thread-1 (]: Databricks adapter: conn: 10979894032: idle check connection: sess: 01eee565-d40e-1580-9592-9773eb6f9256, name: list_hive_metastore_default, idle: 0.007903099060058594s, acqrelcnt: 0, lang: None, thrd: (87287, 10997100544), cmpt: ``, lut: 1710793602.015401
[0m17:26:42.023546 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.default.stage_payments)
[0m17:26:42.023705 [debug] [Thread-1 (]: Databricks adapter: conn: 10979894032: reusing connection list_hive_metastore_default sess: 01eee565-d40e-1580-9592-9773eb6f9256, name: model.default.stage_payments, idle: 0.008241891860961914s, acqrelcnt: 0, lang: None, thrd: (87287, 10997100544), cmpt: ``, lut: 1710793602.015401
[0m17:26:42.023858 [debug] [Thread-1 (]: Databricks adapter: On thread (87287, 10997100544): `hive_metastore`.`default`.`stage_payments` using default compute resource.
[0m17:26:42.024009 [debug] [Thread-1 (]: Databricks adapter: conn: 10979894032: _acquire sess: 01eee565-d40e-1580-9592-9773eb6f9256, name: model.default.stage_payments, idle: 0.008547067642211914s, acqrelcnt: 1, lang: sql, thrd: (87287, 10997100544), cmpt: ``, lut: 1710793602.015401
[0m17:26:42.024167 [debug] [Thread-1 (]: Began compiling node model.default.stage_payments
[0m17:26:42.028242 [debug] [Thread-1 (]: Writing injected SQL for node "model.default.stage_payments"
[0m17:26:42.045580 [debug] [Thread-1 (]: Timing info for model.default.stage_payments (compile): 17:26:42.024264 => 17:26:42.045324
[0m17:26:42.045854 [debug] [Thread-1 (]: Began executing node model.default.stage_payments
[0m17:26:42.059269 [debug] [Thread-1 (]: Writing runtime sql for node "model.default.stage_payments"
[0m17:26:42.061503 [debug] [Thread-1 (]: Databricks adapter: conn: 10979894032: get_thread_connection: sess: 01eee565-d40e-1580-9592-9773eb6f9256, name: model.default.stage_payments, idle: 0.0460202693939209s, acqrelcnt: 1, lang: sql, thrd: (87287, 10997100544), cmpt: ``, lut: 1710793602.015401
[0m17:26:42.061684 [debug] [Thread-1 (]: Databricks adapter: conn: 10979894032: idle check connection: sess: 01eee565-d40e-1580-9592-9773eb6f9256, name: model.default.stage_payments, idle: 0.0462191104888916s, acqrelcnt: 1, lang: sql, thrd: (87287, 10997100544), cmpt: ``, lut: 1710793602.015401
[0m17:26:42.061825 [debug] [Thread-1 (]: Using databricks connection "model.default.stage_payments"
[0m17:26:42.062010 [debug] [Thread-1 (]: On model.default.stage_payments: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_payments"} */
create or replace view `hive_metastore`.`default`.`stage_payments`
  
  
  
  as
    

select 
CAST(FLOOR(random() * (10000 - 1 + 1)) + 1 AS INTEGER)
 AS id,
       user_id AS user_id,
       case when gender = 'M' then 'male' else 'female' end as gender,
       country_code AS location,
       credit_card_type AS credit_card_type,
       datetime AS datetime,
       time AS time
from `hive_metastore`.`default`.`payments`

[0m17:26:43.345461 [debug] [Thread-1 (]: SQL status: OK in 1.2799999713897705 seconds
[0m17:26:43.361577 [debug] [Thread-1 (]: Timing info for model.default.stage_payments (execute): 17:26:42.045962 => 17:26:43.361361
[0m17:26:43.362031 [debug] [Thread-1 (]: Databricks adapter: conn: 10979894032: _release sess: 01eee565-d40e-1580-9592-9773eb6f9256, name: model.default.stage_payments, idle: 3.0994415283203125e-06s, acqrelcnt: 0, lang: sql, thrd: (87287, 10997100544), cmpt: ``, lut: 1710793603.3618839
[0m17:26:43.362677 [debug] [Thread-1 (]: Databricks adapter: conn: 10979894032: _release sess: 01eee565-d40e-1580-9592-9773eb6f9256, name: model.default.stage_payments, idle: 9.5367431640625e-07s, acqrelcnt: 0, lang: sql, thrd: (87287, 10997100544), cmpt: ``, lut: 1710793603.3625531
[0m17:26:43.363295 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b3735555-501d-421b-b61d-ef5c35ec3ac2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16b688d10>]}
[0m17:26:43.364268 [info ] [Thread-1 (]: 1 of 1 OK created sql view model default.stage_payments ........................ [[32mOK[0m in 1.34s]
[0m17:26:43.364786 [debug] [Thread-1 (]: Finished running node model.default.stage_payments
[0m17:26:43.366333 [debug] [MainThread]: Databricks adapter: conn: 10979891088: idle check connection: sess: None, name: master, idle: 1.3475658893585205s, acqrelcnt: 0, lang: None, thrd: (87287, 7965269056), cmpt: ``, lut: 1710793602.0186281
[0m17:26:43.366687 [debug] [MainThread]: Databricks adapter: conn: 10979891088: reusing connection master sess: None, name: master, idle: 1.3479807376861572s, acqrelcnt: 0, lang: None, thrd: (87287, 7965269056), cmpt: ``, lut: 1710793602.0186281
[0m17:26:43.366915 [debug] [MainThread]: Databricks adapter: Thread (87287, 7965269056) using default compute resource.
[0m17:26:43.367130 [debug] [MainThread]: Databricks adapter: conn: 10979891088: _acquire sess: None, name: master, idle: 1.3484396934509277s, acqrelcnt: 1, lang: None, thrd: (87287, 7965269056), cmpt: ``, lut: 1710793602.0186281
[0m17:26:43.367378 [debug] [MainThread]: Databricks adapter: conn: 10979891088: get_thread_connection: sess: None, name: master, idle: 1.3486857414245605s, acqrelcnt: 1, lang: None, thrd: (87287, 7965269056), cmpt: ``, lut: 1710793602.0186281
[0m17:26:43.367601 [debug] [MainThread]: Databricks adapter: conn: 10979891088: idle check connection: sess: None, name: master, idle: 1.3489117622375488s, acqrelcnt: 1, lang: None, thrd: (87287, 7965269056), cmpt: ``, lut: 1710793602.0186281
[0m17:26:43.367803 [debug] [MainThread]: On master: ROLLBACK
[0m17:26:43.368009 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:26:43.835708 [debug] [MainThread]: Databricks adapter: conn: 10979891088: session opened sess: 01eee565-d5dd-13d7-b8d9-399aec81fc07, name: master, idle: 9.059906005859375e-06s, acqrelcnt: 1, lang: None, thrd: (87287, 7965269056), cmpt: ``, lut: 1710793603.835326
[0m17:26:43.836834 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:26:43.837486 [debug] [MainThread]: Databricks adapter: conn: 10979891088: get_thread_connection: sess: 01eee565-d5dd-13d7-b8d9-399aec81fc07, name: master, idle: 0.002017974853515625s, acqrelcnt: 1, lang: None, thrd: (87287, 7965269056), cmpt: ``, lut: 1710793603.835326
[0m17:26:43.838110 [debug] [MainThread]: Databricks adapter: conn: 10979891088: idle check connection: sess: 01eee565-d5dd-13d7-b8d9-399aec81fc07, name: master, idle: 0.0026481151580810547s, acqrelcnt: 1, lang: None, thrd: (87287, 7965269056), cmpt: ``, lut: 1710793603.835326
[0m17:26:43.838601 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:26:43.839036 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:26:43.839499 [debug] [MainThread]: Databricks adapter: conn: 10979891088: _release sess: 01eee565-d5dd-13d7-b8d9-399aec81fc07, name: master, idle: 2.1457672119140625e-06s, acqrelcnt: 0, lang: None, thrd: (87287, 7965269056), cmpt: ``, lut: 1710793603.839392
[0m17:26:43.840711 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:26:43.841249 [debug] [MainThread]: On master: ROLLBACK
[0m17:26:43.841735 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:26:43.842158 [debug] [MainThread]: On master: Close
[0m17:26:44.010908 [debug] [MainThread]: Connection 'model.default.stage_payments' was properly closed.
[0m17:26:44.012565 [debug] [MainThread]: On model.default.stage_payments: ROLLBACK
[0m17:26:44.013213 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:26:44.013854 [debug] [MainThread]: On model.default.stage_payments: Close
[0m17:26:44.177869 [info ] [MainThread]: 
[0m17:26:44.178422 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.85 seconds (3.85s).
[0m17:26:44.178977 [debug] [MainThread]: Command end result
[0m17:26:44.196500 [info ] [MainThread]: 
[0m17:26:44.196798 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:26:44.197014 [info ] [MainThread]: 
[0m17:26:44.197269 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m17:26:44.204715 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 5.1753554, "process_user_time": 2.040064, "process_kernel_time": 3.32432, "process_mem_max_rss": "219004928", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m17:26:44.205130 [debug] [MainThread]: Command `cli run` succeeded at 17:26:44.205041 after 5.18 seconds
[0m17:26:44.205439 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1271c2b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1271c2710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1271c26d0>]}
[0m17:26:44.205715 [debug] [MainThread]: Flushing usage events
[0m17:26:47.110875 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1085ced90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1085aae90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1085fe1d0>]}


============================== 17:26:47.113735 | f62fdb9f-ce3b-4074-865f-16e7a62e66f4 ==============================
[0m17:26:47.113735 [info ] [MainThread]: Running with dbt=1.7.8
[0m17:26:47.114084 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m17:26:48.121612 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f62fdb9f-ce3b-4074-865f-16e7a62e66f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10861d690>]}
[0m17:26:48.152100 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f62fdb9f-ce3b-4074-865f-16e7a62e66f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10861d690>]}
[0m17:26:48.152490 [info ] [MainThread]: Registered adapter: databricks=1.7.9
[0m17:26:48.190238 [debug] [MainThread]: checksum: 67f0013ca5f0bd43af9a0873dd50792fde83ef69de63b71cacd0b4ac656c52e5, vars: {}, profile: , target: , version: 1.7.8
[0m17:26:48.258209 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:26:48.258508 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:26:48.261796 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f62fdb9f-ce3b-4074-865f-16e7a62e66f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13ff60c90>]}
[0m17:26:48.268194 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f62fdb9f-ce3b-4074-865f-16e7a62e66f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13fda07d0>]}
[0m17:26:48.268463 [info ] [MainThread]: Found 3 models, 3 sources, 0 exposures, 0 metrics, 539 macros, 0 groups, 0 semantic models
[0m17:26:48.268638 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f62fdb9f-ce3b-4074-865f-16e7a62e66f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13fcaf8d0>]}
[0m17:26:48.269354 [info ] [MainThread]: 
[0m17:26:48.269784 [debug] [MainThread]: Databricks adapter: conn: 5366222160: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (87314, 7965269056), cmpt: ``, lut: None
[0m17:26:48.269924 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:26:48.270044 [debug] [MainThread]: Databricks adapter: Thread (87314, 7965269056) using default compute resource.
[0m17:26:48.270155 [debug] [MainThread]: Databricks adapter: conn: 5366222160: _acquire sess: None, name: master, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (87314, 7965269056), cmpt: ``, lut: 1710793608.270118
[0m17:26:48.270680 [debug] [ThreadPool]: Databricks adapter: conn: 5366228048: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore, idle: 0s, acqrelcnt: 0, lang: None, thrd: (87314, 11425312768), cmpt: ``, lut: None
[0m17:26:48.270882 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m17:26:48.271012 [debug] [ThreadPool]: Databricks adapter: Thread (87314, 11425312768) using default compute resource.
[0m17:26:48.271139 [debug] [ThreadPool]: Databricks adapter: conn: 5366228048: _acquire sess: None, name: list_hive_metastore, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (87314, 11425312768), cmpt: ``, lut: 1710793608.2710972
[0m17:26:48.271278 [debug] [ThreadPool]: Databricks adapter: conn: 5366228048: get_thread_connection: sess: None, name: list_hive_metastore, idle: 0.0001437664031982422s, acqrelcnt: 1, lang: None, thrd: (87314, 11425312768), cmpt: ``, lut: 1710793608.2710972
[0m17:26:48.271398 [debug] [ThreadPool]: Databricks adapter: conn: 5366228048: idle check connection: sess: None, name: list_hive_metastore, idle: 0.0002627372741699219s, acqrelcnt: 1, lang: None, thrd: (87314, 11425312768), cmpt: ``, lut: 1710793608.2710972
[0m17:26:48.271506 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m17:26:48.271613 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m17:26:48.271728 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:26:48.729259 [debug] [ThreadPool]: Databricks adapter: conn: 5366228048: session opened sess: 01eee565-d8c7-17aa-a919-31cbbfdd2d52, name: list_hive_metastore, idle: 1.2159347534179688e-05s, acqrelcnt: 1, lang: None, thrd: (87314, 11425312768), cmpt: ``, lut: 1710793608.728256
[0m17:26:48.981843 [debug] [ThreadPool]: SQL status: OK in 0.7099999785423279 seconds
[0m17:26:48.989753 [debug] [ThreadPool]: Databricks adapter: conn: 5366228048: _release sess: 01eee565-d8c7-17aa-a919-31cbbfdd2d52, name: list_hive_metastore, idle: 1.4781951904296875e-05s, acqrelcnt: 0, lang: None, thrd: (87314, 11425312768), cmpt: ``, lut: 1710793608.9895191
[0m17:26:48.993077 [debug] [ThreadPool]: Databricks adapter: conn: 5366228048: idle check connection: sess: 01eee565-d8c7-17aa-a919-31cbbfdd2d52, name: list_hive_metastore, idle: 0.0033919811248779297s, acqrelcnt: 0, lang: None, thrd: (87314, 11425312768), cmpt: ``, lut: 1710793608.9895191
[0m17:26:48.993693 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now list_hive_metastore_default)
[0m17:26:48.994437 [debug] [ThreadPool]: Databricks adapter: conn: 5366228048: reusing connection list_hive_metastore sess: 01eee565-d8c7-17aa-a919-31cbbfdd2d52, name: list_hive_metastore_default, idle: 0.00473785400390625s, acqrelcnt: 0, lang: None, thrd: (87314, 11425312768), cmpt: ``, lut: 1710793608.9895191
[0m17:26:48.994874 [debug] [ThreadPool]: Databricks adapter: Thread (87314, 11425312768) using default compute resource.
[0m17:26:48.995196 [debug] [ThreadPool]: Databricks adapter: conn: 5366228048: _acquire sess: 01eee565-d8c7-17aa-a919-31cbbfdd2d52, name: list_hive_metastore_default, idle: 0.005585908889770508s, acqrelcnt: 1, lang: None, thrd: (87314, 11425312768), cmpt: ``, lut: 1710793608.9895191
[0m17:26:48.999852 [debug] [ThreadPool]: Databricks adapter: conn: 5366228048: get_thread_connection: sess: 01eee565-d8c7-17aa-a919-31cbbfdd2d52, name: list_hive_metastore_default, idle: 0.01022481918334961s, acqrelcnt: 1, lang: None, thrd: (87314, 11425312768), cmpt: ``, lut: 1710793608.9895191
[0m17:26:49.000174 [debug] [ThreadPool]: Databricks adapter: conn: 5366228048: idle check connection: sess: 01eee565-d8c7-17aa-a919-31cbbfdd2d52, name: list_hive_metastore_default, idle: 0.010576963424682617s, acqrelcnt: 1, lang: None, thrd: (87314, 11425312768), cmpt: ``, lut: 1710793608.9895191
[0m17:26:49.000416 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:26:49.000644 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m17:26:49.292544 [debug] [ThreadPool]: SQL status: OK in 0.28999999165534973 seconds
[0m17:26:49.300462 [debug] [ThreadPool]: Databricks adapter: conn: 5366228048: get_thread_connection: sess: 01eee565-d8c7-17aa-a919-31cbbfdd2d52, name: list_hive_metastore_default, idle: 0.31085801124572754s, acqrelcnt: 1, lang: None, thrd: (87314, 11425312768), cmpt: ``, lut: 1710793608.9895191
[0m17:26:49.300734 [debug] [ThreadPool]: Databricks adapter: conn: 5366228048: idle check connection: sess: 01eee565-d8c7-17aa-a919-31cbbfdd2d52, name: list_hive_metastore_default, idle: 0.3111560344696045s, acqrelcnt: 1, lang: None, thrd: (87314, 11425312768), cmpt: ``, lut: 1710793608.9895191
[0m17:26:49.300929 [debug] [ThreadPool]: Databricks adapter: conn: 5366228048: get_thread_connection: sess: 01eee565-d8c7-17aa-a919-31cbbfdd2d52, name: list_hive_metastore_default, idle: 0.3113570213317871s, acqrelcnt: 1, lang: None, thrd: (87314, 11425312768), cmpt: ``, lut: 1710793608.9895191
[0m17:26:49.301115 [debug] [ThreadPool]: Databricks adapter: conn: 5366228048: idle check connection: sess: 01eee565-d8c7-17aa-a919-31cbbfdd2d52, name: list_hive_metastore_default, idle: 0.311539888381958s, acqrelcnt: 1, lang: None, thrd: (87314, 11425312768), cmpt: ``, lut: 1710793608.9895191
[0m17:26:49.301286 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:26:49.301440 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:26:49.301612 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m17:26:49.568981 [debug] [ThreadPool]: SQL status: OK in 0.27000001072883606 seconds
[0m17:26:49.573530 [debug] [ThreadPool]: Databricks adapter: conn: 5366228048: get_thread_connection: sess: 01eee565-d8c7-17aa-a919-31cbbfdd2d52, name: list_hive_metastore_default, idle: 0.5839197635650635s, acqrelcnt: 1, lang: None, thrd: (87314, 11425312768), cmpt: ``, lut: 1710793608.9895191
[0m17:26:49.573799 [debug] [ThreadPool]: Databricks adapter: conn: 5366228048: idle check connection: sess: 01eee565-d8c7-17aa-a919-31cbbfdd2d52, name: list_hive_metastore_default, idle: 0.5842199325561523s, acqrelcnt: 1, lang: None, thrd: (87314, 11425312768), cmpt: ``, lut: 1710793608.9895191
[0m17:26:49.573979 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:26:49.574168 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m17:26:49.906252 [debug] [ThreadPool]: SQL status: OK in 0.33000001311302185 seconds
[0m17:26:49.913881 [debug] [ThreadPool]: Databricks adapter: conn: 5366228048: _release sess: 01eee565-d8c7-17aa-a919-31cbbfdd2d52, name: list_hive_metastore_default, idle: 1.0013580322265625e-05s, acqrelcnt: 0, lang: None, thrd: (87314, 11425312768), cmpt: ``, lut: 1710793609.913602
[0m17:26:49.919845 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f62fdb9f-ce3b-4074-865f-16e7a62e66f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13fc58dd0>]}
[0m17:26:49.921062 [debug] [MainThread]: Databricks adapter: conn: 5366222160: get_thread_connection: sess: None, name: master, idle: 1.6507391929626465s, acqrelcnt: 1, lang: None, thrd: (87314, 7965269056), cmpt: ``, lut: 1710793608.270118
[0m17:26:49.921692 [debug] [MainThread]: Databricks adapter: conn: 5366222160: idle check connection: sess: None, name: master, idle: 1.65144681930542s, acqrelcnt: 1, lang: None, thrd: (87314, 7965269056), cmpt: ``, lut: 1710793608.270118
[0m17:26:49.922020 [debug] [MainThread]: Databricks adapter: conn: 5366222160: get_thread_connection: sess: None, name: master, idle: 1.6518230438232422s, acqrelcnt: 1, lang: None, thrd: (87314, 7965269056), cmpt: ``, lut: 1710793608.270118
[0m17:26:49.922311 [debug] [MainThread]: Databricks adapter: conn: 5366222160: idle check connection: sess: None, name: master, idle: 1.6521141529083252s, acqrelcnt: 1, lang: None, thrd: (87314, 7965269056), cmpt: ``, lut: 1710793608.270118
[0m17:26:49.922582 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:26:49.922858 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:26:49.923187 [debug] [MainThread]: Databricks adapter: conn: 5366222160: _release sess: None, name: master, idle: 3.814697265625e-06s, acqrelcnt: 0, lang: None, thrd: (87314, 7965269056), cmpt: ``, lut: 1710793609.9230962
[0m17:26:49.924395 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:26:49.926691 [info ] [MainThread]: 
[0m17:26:49.931806 [debug] [Thread-1 (]: Began running node model.default.stage_payments
[0m17:26:49.932289 [info ] [Thread-1 (]: 1 of 1 START sql view model default.stage_payments ............................. [RUN]
[0m17:26:49.933032 [debug] [Thread-1 (]: Databricks adapter: conn: 5366228048: idle check connection: sess: 01eee565-d8c7-17aa-a919-31cbbfdd2d52, name: list_hive_metastore_default, idle: 0.01930689811706543s, acqrelcnt: 0, lang: None, thrd: (87314, 11425312768), cmpt: ``, lut: 1710793609.913602
[0m17:26:49.933271 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.default.stage_payments)
[0m17:26:49.933620 [debug] [Thread-1 (]: Databricks adapter: conn: 5366228048: reusing connection list_hive_metastore_default sess: 01eee565-d8c7-17aa-a919-31cbbfdd2d52, name: model.default.stage_payments, idle: 0.019877910614013672s, acqrelcnt: 0, lang: None, thrd: (87314, 11425312768), cmpt: ``, lut: 1710793609.913602
[0m17:26:49.933896 [debug] [Thread-1 (]: Databricks adapter: On thread (87314, 11425312768): `hive_metastore`.`default`.`stage_payments` using default compute resource.
[0m17:26:49.934130 [debug] [Thread-1 (]: Databricks adapter: conn: 5366228048: _acquire sess: 01eee565-d8c7-17aa-a919-31cbbfdd2d52, name: model.default.stage_payments, idle: 0.020434856414794922s, acqrelcnt: 1, lang: sql, thrd: (87314, 11425312768), cmpt: ``, lut: 1710793609.913602
[0m17:26:49.934368 [debug] [Thread-1 (]: Began compiling node model.default.stage_payments
[0m17:26:49.940580 [debug] [Thread-1 (]: Writing injected SQL for node "model.default.stage_payments"
[0m17:26:49.942614 [debug] [Thread-1 (]: Timing info for model.default.stage_payments (compile): 17:26:49.934524 => 17:26:49.942434
[0m17:26:49.942880 [debug] [Thread-1 (]: Began executing node model.default.stage_payments
[0m17:26:49.963657 [debug] [Thread-1 (]: Writing runtime sql for node "model.default.stage_payments"
[0m17:26:49.970385 [debug] [Thread-1 (]: Databricks adapter: conn: 5366228048: get_thread_connection: sess: 01eee565-d8c7-17aa-a919-31cbbfdd2d52, name: model.default.stage_payments, idle: 0.056549787521362305s, acqrelcnt: 1, lang: sql, thrd: (87314, 11425312768), cmpt: ``, lut: 1710793609.913602
[0m17:26:49.970784 [debug] [Thread-1 (]: Databricks adapter: conn: 5366228048: idle check connection: sess: 01eee565-d8c7-17aa-a919-31cbbfdd2d52, name: model.default.stage_payments, idle: 0.05706381797790527s, acqrelcnt: 1, lang: sql, thrd: (87314, 11425312768), cmpt: ``, lut: 1710793609.913602
[0m17:26:49.971014 [debug] [Thread-1 (]: Using databricks connection "model.default.stage_payments"
[0m17:26:49.971310 [debug] [Thread-1 (]: On model.default.stage_payments: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_payments"} */
create or replace view `hive_metastore`.`default`.`stage_payments`
  
  
  
  as
    

select 
CAST(FLOOR(random() * (10000 - 1 + 1)) + 1 AS INTEGER)
 AS id,
       user_id AS user_id,
       case when gender = 'M' then 'male' else 'female' end as gender,
       country_code AS location,
       credit_card_type AS credit_card_type,
       datetime AS datetime,
       time AS time
from `hive_metastore`.`default`.`payments`

[0m17:26:50.994567 [debug] [Thread-1 (]: SQL status: OK in 1.0199999809265137 seconds
[0m17:26:51.019294 [debug] [Thread-1 (]: Timing info for model.default.stage_payments (execute): 17:26:49.943022 => 17:26:51.019023
[0m17:26:51.019955 [debug] [Thread-1 (]: Databricks adapter: conn: 5366228048: _release sess: 01eee565-d8c7-17aa-a919-31cbbfdd2d52, name: model.default.stage_payments, idle: 5.0067901611328125e-06s, acqrelcnt: 0, lang: sql, thrd: (87314, 11425312768), cmpt: ``, lut: 1710793611.019762
[0m17:26:51.020848 [debug] [Thread-1 (]: Databricks adapter: conn: 5366228048: _release sess: 01eee565-d8c7-17aa-a919-31cbbfdd2d52, name: model.default.stage_payments, idle: 2.86102294921875e-06s, acqrelcnt: 0, lang: sql, thrd: (87314, 11425312768), cmpt: ``, lut: 1710793611.020687
[0m17:26:51.021369 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f62fdb9f-ce3b-4074-865f-16e7a62e66f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x14e844610>]}
[0m17:26:51.022204 [info ] [Thread-1 (]: 1 of 1 OK created sql view model default.stage_payments ........................ [[32mOK[0m in 1.09s]
[0m17:26:51.022947 [debug] [Thread-1 (]: Finished running node model.default.stage_payments
[0m17:26:51.024384 [debug] [MainThread]: Databricks adapter: conn: 5366222160: idle check connection: sess: None, name: master, idle: 1.1011838912963867s, acqrelcnt: 0, lang: None, thrd: (87314, 7965269056), cmpt: ``, lut: 1710793609.9230962
[0m17:26:51.024668 [debug] [MainThread]: Databricks adapter: conn: 5366222160: reusing connection master sess: None, name: master, idle: 1.1015028953552246s, acqrelcnt: 0, lang: None, thrd: (87314, 7965269056), cmpt: ``, lut: 1710793609.9230962
[0m17:26:51.024883 [debug] [MainThread]: Databricks adapter: Thread (87314, 7965269056) using default compute resource.
[0m17:26:51.025086 [debug] [MainThread]: Databricks adapter: conn: 5366222160: _acquire sess: None, name: master, idle: 1.1019299030303955s, acqrelcnt: 1, lang: None, thrd: (87314, 7965269056), cmpt: ``, lut: 1710793609.9230962
[0m17:26:51.025319 [debug] [MainThread]: Databricks adapter: conn: 5366222160: get_thread_connection: sess: None, name: master, idle: 1.102158784866333s, acqrelcnt: 1, lang: None, thrd: (87314, 7965269056), cmpt: ``, lut: 1710793609.9230962
[0m17:26:51.025529 [debug] [MainThread]: Databricks adapter: conn: 5366222160: idle check connection: sess: None, name: master, idle: 1.102372646331787s, acqrelcnt: 1, lang: None, thrd: (87314, 7965269056), cmpt: ``, lut: 1710793609.9230962
[0m17:26:51.025730 [debug] [MainThread]: On master: ROLLBACK
[0m17:26:51.025926 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:26:51.501485 [debug] [MainThread]: Databricks adapter: conn: 5366222160: session opened sess: 01eee565-da6f-1596-bdfa-f40369f3bf19, name: master, idle: 8.106231689453125e-06s, acqrelcnt: 1, lang: None, thrd: (87314, 7965269056), cmpt: ``, lut: 1710793611.501009
[0m17:26:51.502625 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:26:51.503510 [debug] [MainThread]: Databricks adapter: conn: 5366222160: get_thread_connection: sess: 01eee565-da6f-1596-bdfa-f40369f3bf19, name: master, idle: 0.002271890640258789s, acqrelcnt: 1, lang: None, thrd: (87314, 7965269056), cmpt: ``, lut: 1710793611.501009
[0m17:26:51.504298 [debug] [MainThread]: Databricks adapter: conn: 5366222160: idle check connection: sess: 01eee565-da6f-1596-bdfa-f40369f3bf19, name: master, idle: 0.003070831298828125s, acqrelcnt: 1, lang: None, thrd: (87314, 7965269056), cmpt: ``, lut: 1710793611.501009
[0m17:26:51.504984 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:26:51.505916 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:26:51.506747 [debug] [MainThread]: Databricks adapter: conn: 5366222160: _release sess: 01eee565-da6f-1596-bdfa-f40369f3bf19, name: master, idle: 3.814697265625e-06s, acqrelcnt: 0, lang: None, thrd: (87314, 7965269056), cmpt: ``, lut: 1710793611.5065222
[0m17:26:51.508723 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:26:51.509498 [debug] [MainThread]: On master: ROLLBACK
[0m17:26:51.510301 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:26:51.511566 [debug] [MainThread]: On master: Close
[0m17:26:51.679498 [debug] [MainThread]: Connection 'model.default.stage_payments' was properly closed.
[0m17:26:51.681362 [debug] [MainThread]: On model.default.stage_payments: ROLLBACK
[0m17:26:51.682162 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:26:51.682699 [debug] [MainThread]: On model.default.stage_payments: Close
[0m17:26:51.850341 [info ] [MainThread]: 
[0m17:26:51.851432 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 3.58 seconds (3.58s).
[0m17:26:51.853025 [debug] [MainThread]: Command end result
[0m17:26:51.891453 [info ] [MainThread]: 
[0m17:26:51.892354 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:26:51.892849 [info ] [MainThread]: 
[0m17:26:51.893424 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m17:26:51.904573 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 4.822503, "process_user_time": 2.145302, "process_kernel_time": 3.371071, "process_mem_max_rss": "221396992", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m17:26:51.905389 [debug] [MainThread]: Command `cli run` succeeded at 17:26:51.905251 after 4.82 seconds
[0m17:26:51.905838 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108599390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1085d86d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13fc25f90>]}
[0m17:26:51.906219 [debug] [MainThread]: Flushing usage events
[0m17:30:13.732708 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075700d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075f1a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075f2090>]}


============================== 17:30:13.735647 | 94614cb2-e104-4a04-b459-878039406fa5 ==============================
[0m17:30:13.735647 [info ] [MainThread]: Running with dbt=1.7.8
[0m17:30:13.735945 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m17:30:15.045537 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '94614cb2-e104-4a04-b459-878039406fa5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13f135410>]}
[0m17:30:15.073803 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '94614cb2-e104-4a04-b459-878039406fa5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075f1a10>]}
[0m17:30:15.074047 [info ] [MainThread]: Registered adapter: databricks=1.7.9
[0m17:30:15.093038 [debug] [MainThread]: checksum: 67f0013ca5f0bd43af9a0873dd50792fde83ef69de63b71cacd0b4ac656c52e5, vars: {}, profile: , target: , version: 1.7.8
[0m17:30:15.178873 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:30:15.179122 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:30:15.181752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '94614cb2-e104-4a04-b459-878039406fa5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13f25aa50>]}
[0m17:30:15.187993 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '94614cb2-e104-4a04-b459-878039406fa5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13f3a1490>]}
[0m17:30:15.188223 [info ] [MainThread]: Found 4 models, 3 sources, 0 exposures, 0 metrics, 539 macros, 0 groups, 0 semantic models
[0m17:30:15.188397 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '94614cb2-e104-4a04-b459-878039406fa5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13f22d750>]}
[0m17:30:15.189102 [info ] [MainThread]: 
[0m17:30:15.189547 [debug] [MainThread]: Databricks adapter: conn: 5355734608: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (87917, 7965269056), cmpt: ``, lut: None
[0m17:30:15.189704 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:30:15.189817 [debug] [MainThread]: Databricks adapter: Thread (87917, 7965269056) using default compute resource.
[0m17:30:15.189930 [debug] [MainThread]: Databricks adapter: conn: 5355734608: _acquire sess: None, name: master, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (87917, 7965269056), cmpt: ``, lut: 1710793815.189894
[0m17:30:15.190397 [debug] [ThreadPool]: Databricks adapter: conn: 5355743504: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore, idle: 0s, acqrelcnt: 0, lang: None, thrd: (87917, 6140391424), cmpt: ``, lut: None
[0m17:30:15.190549 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m17:30:15.190664 [debug] [ThreadPool]: Databricks adapter: Thread (87917, 6140391424) using default compute resource.
[0m17:30:15.190778 [debug] [ThreadPool]: Databricks adapter: conn: 5355743504: _acquire sess: None, name: list_hive_metastore, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (87917, 6140391424), cmpt: ``, lut: 1710793815.190742
[0m17:30:15.190906 [debug] [ThreadPool]: Databricks adapter: conn: 5355743504: get_thread_connection: sess: None, name: list_hive_metastore, idle: 0.00012993812561035156s, acqrelcnt: 1, lang: None, thrd: (87917, 6140391424), cmpt: ``, lut: 1710793815.190742
[0m17:30:15.191026 [debug] [ThreadPool]: Databricks adapter: conn: 5355743504: idle check connection: sess: None, name: list_hive_metastore, idle: 0.00024700164794921875s, acqrelcnt: 1, lang: None, thrd: (87917, 6140391424), cmpt: ``, lut: 1710793815.190742
[0m17:30:15.191134 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m17:30:15.191244 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m17:30:15.191349 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:30:15.701673 [debug] [ThreadPool]: Databricks adapter: conn: 5355743504: session opened sess: 01eee566-541f-1e00-951a-65fe0d442689, name: list_hive_metastore, idle: 8.106231689453125e-06s, acqrelcnt: 1, lang: None, thrd: (87917, 6140391424), cmpt: ``, lut: 1710793815.701498
[0m17:30:16.071781 [debug] [ThreadPool]: SQL status: OK in 0.8799999952316284 seconds
[0m17:30:16.080095 [debug] [ThreadPool]: Databricks adapter: conn: 5355743504: _release sess: 01eee566-541f-1e00-951a-65fe0d442689, name: list_hive_metastore, idle: 9.059906005859375e-06s, acqrelcnt: 0, lang: None, thrd: (87917, 6140391424), cmpt: ``, lut: 1710793816.0798929
[0m17:30:16.082335 [debug] [ThreadPool]: Databricks adapter: conn: 5355743504: idle check connection: sess: 01eee566-541f-1e00-951a-65fe0d442689, name: list_hive_metastore, idle: 0.0023202896118164062s, acqrelcnt: 0, lang: None, thrd: (87917, 6140391424), cmpt: ``, lut: 1710793816.0798929
[0m17:30:16.082748 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now list_hive_metastore_default)
[0m17:30:16.083128 [debug] [ThreadPool]: Databricks adapter: conn: 5355743504: reusing connection list_hive_metastore sess: 01eee566-541f-1e00-951a-65fe0d442689, name: list_hive_metastore_default, idle: 0.0031430721282958984s, acqrelcnt: 0, lang: None, thrd: (87917, 6140391424), cmpt: ``, lut: 1710793816.0798929
[0m17:30:16.083411 [debug] [ThreadPool]: Databricks adapter: Thread (87917, 6140391424) using default compute resource.
[0m17:30:16.083673 [debug] [ThreadPool]: Databricks adapter: conn: 5355743504: _acquire sess: 01eee566-541f-1e00-951a-65fe0d442689, name: list_hive_metastore_default, idle: 0.0037031173706054688s, acqrelcnt: 1, lang: None, thrd: (87917, 6140391424), cmpt: ``, lut: 1710793816.0798929
[0m17:30:16.088305 [debug] [ThreadPool]: Databricks adapter: conn: 5355743504: get_thread_connection: sess: 01eee566-541f-1e00-951a-65fe0d442689, name: list_hive_metastore_default, idle: 0.008312225341796875s, acqrelcnt: 1, lang: None, thrd: (87917, 6140391424), cmpt: ``, lut: 1710793816.0798929
[0m17:30:16.088580 [debug] [ThreadPool]: Databricks adapter: conn: 5355743504: idle check connection: sess: 01eee566-541f-1e00-951a-65fe0d442689, name: list_hive_metastore_default, idle: 0.008622169494628906s, acqrelcnt: 1, lang: None, thrd: (87917, 6140391424), cmpt: ``, lut: 1710793816.0798929
[0m17:30:16.088783 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:30:16.088980 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m17:30:16.410400 [debug] [ThreadPool]: SQL status: OK in 0.3199999928474426 seconds
[0m17:30:16.419786 [debug] [ThreadPool]: Databricks adapter: conn: 5355743504: get_thread_connection: sess: 01eee566-541f-1e00-951a-65fe0d442689, name: list_hive_metastore_default, idle: 0.339780330657959s, acqrelcnt: 1, lang: None, thrd: (87917, 6140391424), cmpt: ``, lut: 1710793816.0798929
[0m17:30:16.420112 [debug] [ThreadPool]: Databricks adapter: conn: 5355743504: idle check connection: sess: 01eee566-541f-1e00-951a-65fe0d442689, name: list_hive_metastore_default, idle: 0.34015321731567383s, acqrelcnt: 1, lang: None, thrd: (87917, 6140391424), cmpt: ``, lut: 1710793816.0798929
[0m17:30:16.420333 [debug] [ThreadPool]: Databricks adapter: conn: 5355743504: get_thread_connection: sess: 01eee566-541f-1e00-951a-65fe0d442689, name: list_hive_metastore_default, idle: 0.3403799533843994s, acqrelcnt: 1, lang: None, thrd: (87917, 6140391424), cmpt: ``, lut: 1710793816.0798929
[0m17:30:16.420530 [debug] [ThreadPool]: Databricks adapter: conn: 5355743504: idle check connection: sess: 01eee566-541f-1e00-951a-65fe0d442689, name: list_hive_metastore_default, idle: 0.34058117866516113s, acqrelcnt: 1, lang: None, thrd: (87917, 6140391424), cmpt: ``, lut: 1710793816.0798929
[0m17:30:16.420723 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:30:16.420890 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:30:16.421076 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m17:30:17.016980 [debug] [ThreadPool]: SQL status: OK in 0.6000000238418579 seconds
[0m17:30:17.023945 [debug] [ThreadPool]: Databricks adapter: conn: 5355743504: get_thread_connection: sess: 01eee566-541f-1e00-951a-65fe0d442689, name: list_hive_metastore_default, idle: 0.9439160823822021s, acqrelcnt: 1, lang: None, thrd: (87917, 6140391424), cmpt: ``, lut: 1710793816.0798929
[0m17:30:17.024334 [debug] [ThreadPool]: Databricks adapter: conn: 5355743504: idle check connection: sess: 01eee566-541f-1e00-951a-65fe0d442689, name: list_hive_metastore_default, idle: 0.9443552494049072s, acqrelcnt: 1, lang: None, thrd: (87917, 6140391424), cmpt: ``, lut: 1710793816.0798929
[0m17:30:17.024588 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:30:17.024858 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m17:30:17.356770 [debug] [ThreadPool]: SQL status: OK in 0.33000001311302185 seconds
[0m17:30:17.366481 [debug] [ThreadPool]: Databricks adapter: conn: 5355743504: _release sess: 01eee566-541f-1e00-951a-65fe0d442689, name: list_hive_metastore_default, idle: 9.059906005859375e-06s, acqrelcnt: 0, lang: None, thrd: (87917, 6140391424), cmpt: ``, lut: 1710793817.366165
[0m17:30:17.372604 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '94614cb2-e104-4a04-b459-878039406fa5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13f4aa190>]}
[0m17:30:17.373634 [debug] [MainThread]: Databricks adapter: conn: 5355734608: get_thread_connection: sess: None, name: master, idle: 2.183601140975952s, acqrelcnt: 1, lang: None, thrd: (87917, 7965269056), cmpt: ``, lut: 1710793815.189894
[0m17:30:17.374165 [debug] [MainThread]: Databricks adapter: conn: 5355734608: idle check connection: sess: None, name: master, idle: 2.1841659545898438s, acqrelcnt: 1, lang: None, thrd: (87917, 7965269056), cmpt: ``, lut: 1710793815.189894
[0m17:30:17.374629 [debug] [MainThread]: Databricks adapter: conn: 5355734608: get_thread_connection: sess: None, name: master, idle: 2.1846351623535156s, acqrelcnt: 1, lang: None, thrd: (87917, 7965269056), cmpt: ``, lut: 1710793815.189894
[0m17:30:17.375054 [debug] [MainThread]: Databricks adapter: conn: 5355734608: idle check connection: sess: None, name: master, idle: 2.185060977935791s, acqrelcnt: 1, lang: None, thrd: (87917, 7965269056), cmpt: ``, lut: 1710793815.189894
[0m17:30:17.375455 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:30:17.375860 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:30:17.376386 [debug] [MainThread]: Databricks adapter: conn: 5355734608: _release sess: None, name: master, idle: 2.86102294921875e-06s, acqrelcnt: 0, lang: None, thrd: (87917, 7965269056), cmpt: ``, lut: 1710793817.376223
[0m17:30:17.377755 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:30:17.378356 [info ] [MainThread]: 
[0m17:30:17.386675 [debug] [Thread-1 (]: Began running node model.default.stage_payments
[0m17:30:17.387418 [info ] [Thread-1 (]: 1 of 1 START sql view model default.stage_payments ............................. [RUN]
[0m17:30:17.388397 [debug] [Thread-1 (]: Databricks adapter: conn: 5355743504: idle check connection: sess: 01eee566-541f-1e00-951a-65fe0d442689, name: list_hive_metastore_default, idle: 0.022036075592041016s, acqrelcnt: 0, lang: None, thrd: (87917, 6140391424), cmpt: ``, lut: 1710793817.366165
[0m17:30:17.388748 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.default.stage_payments)
[0m17:30:17.389105 [debug] [Thread-1 (]: Databricks adapter: conn: 5355743504: reusing connection list_hive_metastore_default sess: 01eee566-541f-1e00-951a-65fe0d442689, name: model.default.stage_payments, idle: 0.022800207138061523s, acqrelcnt: 0, lang: None, thrd: (87917, 6140391424), cmpt: ``, lut: 1710793817.366165
[0m17:30:17.389446 [debug] [Thread-1 (]: Databricks adapter: On thread (87917, 6140391424): `hive_metastore`.`default`.`stage_payments` using default compute resource.
[0m17:30:17.389771 [debug] [Thread-1 (]: Databricks adapter: conn: 5355743504: _acquire sess: 01eee566-541f-1e00-951a-65fe0d442689, name: model.default.stage_payments, idle: 0.023478031158447266s, acqrelcnt: 1, lang: sql, thrd: (87917, 6140391424), cmpt: ``, lut: 1710793817.366165
[0m17:30:17.390099 [debug] [Thread-1 (]: Began compiling node model.default.stage_payments
[0m17:30:17.398085 [debug] [Thread-1 (]: Writing injected SQL for node "model.default.stage_payments"
[0m17:30:17.400009 [debug] [Thread-1 (]: Timing info for model.default.stage_payments (compile): 17:30:17.390310 => 17:30:17.399819
[0m17:30:17.400311 [debug] [Thread-1 (]: Began executing node model.default.stage_payments
[0m17:30:17.419173 [debug] [Thread-1 (]: Writing runtime sql for node "model.default.stage_payments"
[0m17:30:17.421670 [debug] [Thread-1 (]: Databricks adapter: conn: 5355743504: get_thread_connection: sess: 01eee566-541f-1e00-951a-65fe0d442689, name: model.default.stage_payments, idle: 0.05538797378540039s, acqrelcnt: 1, lang: sql, thrd: (87917, 6140391424), cmpt: ``, lut: 1710793817.366165
[0m17:30:17.421921 [debug] [Thread-1 (]: Databricks adapter: conn: 5355743504: idle check connection: sess: 01eee566-541f-1e00-951a-65fe0d442689, name: model.default.stage_payments, idle: 0.05567002296447754s, acqrelcnt: 1, lang: sql, thrd: (87917, 6140391424), cmpt: ``, lut: 1710793817.366165
[0m17:30:17.422096 [debug] [Thread-1 (]: Using databricks connection "model.default.stage_payments"
[0m17:30:17.422330 [debug] [Thread-1 (]: On model.default.stage_payments: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "node_id": "model.default.stage_payments"} */
create or replace view `hive_metastore`.`default`.`stage_payments`
  
  
  
  as
    

select 
CAST(FLOOR(random() * (10000 - 1 + 1)) + 1 AS INTEGER)
 AS id,
       user_id AS user_id,
       case when gender = 'M' then 'male' else 'female' end as gender,
       country_code AS location,
       country AS country,
       credit_card_type AS credit_card_type,
       datetime AS datetime,
       time AS time
from `hive_metastore`.`default`.`payments`

[0m17:30:18.484583 [debug] [Thread-1 (]: SQL status: OK in 1.059999942779541 seconds
[0m17:30:18.513694 [debug] [Thread-1 (]: Timing info for model.default.stage_payments (execute): 17:30:17.400481 => 17:30:18.513428
[0m17:30:18.514326 [debug] [Thread-1 (]: Databricks adapter: conn: 5355743504: _release sess: 01eee566-541f-1e00-951a-65fe0d442689, name: model.default.stage_payments, idle: 4.0531158447265625e-06s, acqrelcnt: 0, lang: sql, thrd: (87917, 6140391424), cmpt: ``, lut: 1710793818.514148
[0m17:30:18.515119 [debug] [Thread-1 (]: Databricks adapter: conn: 5355743504: _release sess: 01eee566-541f-1e00-951a-65fe0d442689, name: model.default.stage_payments, idle: 1.9073486328125e-06s, acqrelcnt: 0, lang: sql, thrd: (87917, 6140391424), cmpt: ``, lut: 1710793818.514954
[0m17:30:18.515566 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '94614cb2-e104-4a04-b459-878039406fa5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13f4a8050>]}
[0m17:30:18.516231 [info ] [Thread-1 (]: 1 of 1 OK created sql view model default.stage_payments ........................ [[32mOK[0m in 1.13s]
[0m17:30:18.516778 [debug] [Thread-1 (]: Finished running node model.default.stage_payments
[0m17:30:18.518410 [debug] [MainThread]: Databricks adapter: conn: 5355734608: idle check connection: sess: None, name: master, idle: 1.142043113708496s, acqrelcnt: 0, lang: None, thrd: (87917, 7965269056), cmpt: ``, lut: 1710793817.376223
[0m17:30:18.518812 [debug] [MainThread]: Databricks adapter: conn: 5355734608: reusing connection master sess: None, name: master, idle: 1.1425037384033203s, acqrelcnt: 0, lang: None, thrd: (87917, 7965269056), cmpt: ``, lut: 1710793817.376223
[0m17:30:18.519073 [debug] [MainThread]: Databricks adapter: Thread (87917, 7965269056) using default compute resource.
[0m17:30:18.519319 [debug] [MainThread]: Databricks adapter: conn: 5355734608: _acquire sess: None, name: master, idle: 1.1430246829986572s, acqrelcnt: 1, lang: None, thrd: (87917, 7965269056), cmpt: ``, lut: 1710793817.376223
[0m17:30:18.519586 [debug] [MainThread]: Databricks adapter: conn: 5355734608: get_thread_connection: sess: None, name: master, idle: 1.1432878971099854s, acqrelcnt: 1, lang: None, thrd: (87917, 7965269056), cmpt: ``, lut: 1710793817.376223
[0m17:30:18.519833 [debug] [MainThread]: Databricks adapter: conn: 5355734608: idle check connection: sess: None, name: master, idle: 1.1435389518737793s, acqrelcnt: 1, lang: None, thrd: (87917, 7965269056), cmpt: ``, lut: 1710793817.376223
[0m17:30:18.520071 [debug] [MainThread]: On master: ROLLBACK
[0m17:30:18.520305 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:30:19.002125 [debug] [MainThread]: Databricks adapter: conn: 5355734608: session opened sess: 01eee566-561c-1520-a2b6-cd275e502c1f, name: master, idle: 1.1205673217773438e-05s, acqrelcnt: 1, lang: None, thrd: (87917, 7965269056), cmpt: ``, lut: 1710793819.0017579
[0m17:30:19.003096 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:30:19.003919 [debug] [MainThread]: Databricks adapter: conn: 5355734608: get_thread_connection: sess: 01eee566-561c-1520-a2b6-cd275e502c1f, name: master, idle: 0.0018832683563232422s, acqrelcnt: 1, lang: None, thrd: (87917, 7965269056), cmpt: ``, lut: 1710793819.0017579
[0m17:30:19.004735 [debug] [MainThread]: Databricks adapter: conn: 5355734608: idle check connection: sess: 01eee566-561c-1520-a2b6-cd275e502c1f, name: master, idle: 0.0027179718017578125s, acqrelcnt: 1, lang: None, thrd: (87917, 7965269056), cmpt: ``, lut: 1710793819.0017579
[0m17:30:19.005376 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:30:19.005859 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:30:19.006834 [debug] [MainThread]: Databricks adapter: conn: 5355734608: _release sess: 01eee566-561c-1520-a2b6-cd275e502c1f, name: master, idle: 2.6226043701171875e-06s, acqrelcnt: 0, lang: None, thrd: (87917, 7965269056), cmpt: ``, lut: 1710793819.0066922
[0m17:30:19.008307 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:30:19.008929 [debug] [MainThread]: On master: ROLLBACK
[0m17:30:19.009346 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:30:19.009727 [debug] [MainThread]: On master: Close
[0m17:30:19.174267 [debug] [MainThread]: Connection 'model.default.stage_payments' was properly closed.
[0m17:30:19.177368 [debug] [MainThread]: On model.default.stage_payments: ROLLBACK
[0m17:30:19.178602 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:30:19.179258 [debug] [MainThread]: On model.default.stage_payments: Close
[0m17:30:19.355727 [info ] [MainThread]: 
[0m17:30:19.356161 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 4.17 seconds (4.17s).
[0m17:30:19.356540 [debug] [MainThread]: Command end result
[0m17:30:19.369397 [info ] [MainThread]: 
[0m17:30:19.369722 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:30:19.369891 [info ] [MainThread]: 
[0m17:30:19.370130 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m17:30:19.380852 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 5.675253, "process_user_time": 2.031372, "process_kernel_time": 3.297047, "process_mem_max_rss": "221724672", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m17:30:19.381315 [debug] [MainThread]: Command `cli run` succeeded at 17:30:19.381258 after 5.68 seconds
[0m17:30:19.381577 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075f15d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103397d50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105d55dd0>]}
[0m17:30:19.381775 [debug] [MainThread]: Flushing usage events
[0m17:31:35.159449 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106142150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106141b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1061c2190>]}


============================== 17:31:35.162693 | b42c350f-83d7-43f8-8418-6ed1c86aa9dc ==============================
[0m17:31:35.162693 [info ] [MainThread]: Running with dbt=1.7.8
[0m17:31:35.162980 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m17:31:36.719972 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b42c350f-83d7-43f8-8418-6ed1c86aa9dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13f674550>]}
[0m17:31:36.748957 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b42c350f-83d7-43f8-8418-6ed1c86aa9dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105545150>]}
[0m17:31:36.749280 [info ] [MainThread]: Registered adapter: databricks=1.7.9
[0m17:31:36.766958 [debug] [MainThread]: checksum: 67f0013ca5f0bd43af9a0873dd50792fde83ef69de63b71cacd0b4ac656c52e5, vars: {}, profile: , target: , version: 1.7.8
[0m17:31:36.869016 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:31:36.869287 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:31:36.872300 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b42c350f-83d7-43f8-8418-6ed1c86aa9dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13f8cc990>]}
[0m17:31:36.894062 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b42c350f-83d7-43f8-8418-6ed1c86aa9dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13f654b50>]}
[0m17:31:36.894335 [info ] [MainThread]: Found 4 models, 3 sources, 0 exposures, 0 metrics, 539 macros, 0 groups, 0 semantic models
[0m17:31:36.894498 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b42c350f-83d7-43f8-8418-6ed1c86aa9dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105dc6890>]}
[0m17:31:36.895230 [info ] [MainThread]: 
[0m17:31:36.895710 [debug] [MainThread]: Databricks adapter: conn: 5359590160: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (88158, 7965269056), cmpt: ``, lut: None
[0m17:31:36.895859 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:31:36.895974 [debug] [MainThread]: Databricks adapter: Thread (88158, 7965269056) using default compute resource.
[0m17:31:36.896092 [debug] [MainThread]: Databricks adapter: conn: 5359590160: _acquire sess: None, name: master, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (88158, 7965269056), cmpt: ``, lut: 1710793896.896053
[0m17:31:36.896636 [debug] [ThreadPool]: Databricks adapter: conn: 5359934160: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore, idle: 0s, acqrelcnt: 0, lang: None, thrd: (88158, 10932187136), cmpt: ``, lut: None
[0m17:31:36.896826 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m17:31:36.896958 [debug] [ThreadPool]: Databricks adapter: Thread (88158, 10932187136) using default compute resource.
[0m17:31:36.897083 [debug] [ThreadPool]: Databricks adapter: conn: 5359934160: _acquire sess: None, name: list_hive_metastore, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (88158, 10932187136), cmpt: ``, lut: 1710793896.897044
[0m17:31:36.897226 [debug] [ThreadPool]: Databricks adapter: conn: 5359934160: get_thread_connection: sess: None, name: list_hive_metastore, idle: 0.00014209747314453125s, acqrelcnt: 1, lang: None, thrd: (88158, 10932187136), cmpt: ``, lut: 1710793896.897044
[0m17:31:36.897352 [debug] [ThreadPool]: Databricks adapter: conn: 5359934160: idle check connection: sess: None, name: list_hive_metastore, idle: 0.00026917457580566406s, acqrelcnt: 1, lang: None, thrd: (88158, 10932187136), cmpt: ``, lut: 1710793896.897044
[0m17:31:36.897465 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m17:31:36.897587 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m17:31:36.897698 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:31:37.368258 [debug] [ThreadPool]: Databricks adapter: conn: 5359934160: session opened sess: 01eee566-84d1-1ab8-81df-aab9a1ee3fe0, name: list_hive_metastore, idle: 1.3113021850585938e-05s, acqrelcnt: 1, lang: None, thrd: (88158, 10932187136), cmpt: ``, lut: 1710793897.367467
[0m17:31:37.671254 [debug] [ThreadPool]: SQL status: OK in 0.7699999809265137 seconds
[0m17:31:37.675680 [debug] [ThreadPool]: Databricks adapter: conn: 5359934160: _release sess: 01eee566-84d1-1ab8-81df-aab9a1ee3fe0, name: list_hive_metastore, idle: 5.0067901611328125e-06s, acqrelcnt: 0, lang: None, thrd: (88158, 10932187136), cmpt: ``, lut: 1710793897.675549
[0m17:31:37.677479 [debug] [ThreadPool]: Databricks adapter: conn: 5359934160: idle check connection: sess: 01eee566-84d1-1ab8-81df-aab9a1ee3fe0, name: list_hive_metastore, idle: 0.0018091201782226562s, acqrelcnt: 0, lang: None, thrd: (88158, 10932187136), cmpt: ``, lut: 1710793897.675549
[0m17:31:37.677891 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now list_hive_metastore_default)
[0m17:31:37.678229 [debug] [ThreadPool]: Databricks adapter: conn: 5359934160: reusing connection list_hive_metastore sess: 01eee566-84d1-1ab8-81df-aab9a1ee3fe0, name: list_hive_metastore_default, idle: 0.0025899410247802734s, acqrelcnt: 0, lang: None, thrd: (88158, 10932187136), cmpt: ``, lut: 1710793897.675549
[0m17:31:37.678492 [debug] [ThreadPool]: Databricks adapter: Thread (88158, 10932187136) using default compute resource.
[0m17:31:37.678746 [debug] [ThreadPool]: Databricks adapter: conn: 5359934160: _acquire sess: 01eee566-84d1-1ab8-81df-aab9a1ee3fe0, name: list_hive_metastore_default, idle: 0.0031211376190185547s, acqrelcnt: 1, lang: None, thrd: (88158, 10932187136), cmpt: ``, lut: 1710793897.675549
[0m17:31:37.683159 [debug] [ThreadPool]: Databricks adapter: conn: 5359934160: get_thread_connection: sess: 01eee566-84d1-1ab8-81df-aab9a1ee3fe0, name: list_hive_metastore_default, idle: 0.007523059844970703s, acqrelcnt: 1, lang: None, thrd: (88158, 10932187136), cmpt: ``, lut: 1710793897.675549
[0m17:31:37.683415 [debug] [ThreadPool]: Databricks adapter: conn: 5359934160: idle check connection: sess: 01eee566-84d1-1ab8-81df-aab9a1ee3fe0, name: list_hive_metastore_default, idle: 0.007798910140991211s, acqrelcnt: 1, lang: None, thrd: (88158, 10932187136), cmpt: ``, lut: 1710793897.675549
[0m17:31:37.683621 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:31:37.683825 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m17:31:37.967532 [debug] [ThreadPool]: SQL status: OK in 0.2800000011920929 seconds
[0m17:31:37.980391 [debug] [ThreadPool]: Databricks adapter: conn: 5359934160: get_thread_connection: sess: 01eee566-84d1-1ab8-81df-aab9a1ee3fe0, name: list_hive_metastore_default, idle: 0.30472803115844727s, acqrelcnt: 1, lang: None, thrd: (88158, 10932187136), cmpt: ``, lut: 1710793897.675549
[0m17:31:37.980761 [debug] [ThreadPool]: Databricks adapter: conn: 5359934160: idle check connection: sess: 01eee566-84d1-1ab8-81df-aab9a1ee3fe0, name: list_hive_metastore_default, idle: 0.3051340579986572s, acqrelcnt: 1, lang: None, thrd: (88158, 10932187136), cmpt: ``, lut: 1710793897.675549
[0m17:31:37.981041 [debug] [ThreadPool]: Databricks adapter: conn: 5359934160: get_thread_connection: sess: 01eee566-84d1-1ab8-81df-aab9a1ee3fe0, name: list_hive_metastore_default, idle: 0.3054189682006836s, acqrelcnt: 1, lang: None, thrd: (88158, 10932187136), cmpt: ``, lut: 1710793897.675549
[0m17:31:37.981291 [debug] [ThreadPool]: Databricks adapter: conn: 5359934160: idle check connection: sess: 01eee566-84d1-1ab8-81df-aab9a1ee3fe0, name: list_hive_metastore_default, idle: 0.30567097663879395s, acqrelcnt: 1, lang: None, thrd: (88158, 10932187136), cmpt: ``, lut: 1710793897.675549
[0m17:31:37.981528 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:31:37.981745 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:31:37.981953 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m17:31:38.258304 [debug] [ThreadPool]: SQL status: OK in 0.2800000011920929 seconds
[0m17:31:38.263764 [debug] [ThreadPool]: Databricks adapter: conn: 5359934160: get_thread_connection: sess: 01eee566-84d1-1ab8-81df-aab9a1ee3fe0, name: list_hive_metastore_default, idle: 0.5881130695343018s, acqrelcnt: 1, lang: None, thrd: (88158, 10932187136), cmpt: ``, lut: 1710793897.675549
[0m17:31:38.264063 [debug] [ThreadPool]: Databricks adapter: conn: 5359934160: idle check connection: sess: 01eee566-84d1-1ab8-81df-aab9a1ee3fe0, name: list_hive_metastore_default, idle: 0.5884451866149902s, acqrelcnt: 1, lang: None, thrd: (88158, 10932187136), cmpt: ``, lut: 1710793897.675549
[0m17:31:38.264271 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:31:38.264487 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m17:31:38.590390 [debug] [ThreadPool]: SQL status: OK in 0.33000001311302185 seconds
[0m17:31:38.597230 [debug] [ThreadPool]: Databricks adapter: conn: 5359934160: _release sess: 01eee566-84d1-1ab8-81df-aab9a1ee3fe0, name: list_hive_metastore_default, idle: 4.0531158447265625e-06s, acqrelcnt: 0, lang: None, thrd: (88158, 10932187136), cmpt: ``, lut: 1710793898.597041
[0m17:31:38.601095 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b42c350f-83d7-43f8-8418-6ed1c86aa9dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13f645b50>]}
[0m17:31:38.601547 [debug] [MainThread]: Databricks adapter: conn: 5359590160: get_thread_connection: sess: None, name: master, idle: 1.7054059505462646s, acqrelcnt: 1, lang: None, thrd: (88158, 7965269056), cmpt: ``, lut: 1710793896.896053
[0m17:31:38.601835 [debug] [MainThread]: Databricks adapter: conn: 5359590160: idle check connection: sess: None, name: master, idle: 1.7057068347930908s, acqrelcnt: 1, lang: None, thrd: (88158, 7965269056), cmpt: ``, lut: 1710793896.896053
[0m17:31:38.602104 [debug] [MainThread]: Databricks adapter: conn: 5359590160: get_thread_connection: sess: None, name: master, idle: 1.7059729099273682s, acqrelcnt: 1, lang: None, thrd: (88158, 7965269056), cmpt: ``, lut: 1710793896.896053
[0m17:31:38.602352 [debug] [MainThread]: Databricks adapter: conn: 5359590160: idle check connection: sess: None, name: master, idle: 1.7062268257141113s, acqrelcnt: 1, lang: None, thrd: (88158, 7965269056), cmpt: ``, lut: 1710793896.896053
[0m17:31:38.602584 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:31:38.602805 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:31:38.603046 [debug] [MainThread]: Databricks adapter: conn: 5359590160: _release sess: None, name: master, idle: 9.5367431640625e-07s, acqrelcnt: 0, lang: None, thrd: (88158, 7965269056), cmpt: ``, lut: 1710793898.602973
[0m17:31:38.603601 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:31:38.603889 [info ] [MainThread]: 
[0m17:31:38.609030 [debug] [Thread-1 (]: Began running node model.default.credit_card_type
[0m17:31:38.609462 [info ] [Thread-1 (]: 1 of 1 START sql table model default.credit_card_type .......................... [RUN]
[0m17:31:38.610163 [debug] [Thread-1 (]: Databricks adapter: conn: 5359934160: idle check connection: sess: 01eee566-84d1-1ab8-81df-aab9a1ee3fe0, name: list_hive_metastore_default, idle: 0.012980222702026367s, acqrelcnt: 0, lang: None, thrd: (88158, 10932187136), cmpt: ``, lut: 1710793898.597041
[0m17:31:38.610443 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.default.credit_card_type)
[0m17:31:38.610735 [debug] [Thread-1 (]: Databricks adapter: conn: 5359934160: reusing connection list_hive_metastore_default sess: 01eee566-84d1-1ab8-81df-aab9a1ee3fe0, name: model.default.credit_card_type, idle: 0.013584136962890625s, acqrelcnt: 0, lang: None, thrd: (88158, 10932187136), cmpt: ``, lut: 1710793898.597041
[0m17:31:38.610984 [debug] [Thread-1 (]: Databricks adapter: On thread (88158, 10932187136): `hive_metastore`.`default`.`credit_card_type` using default compute resource.
[0m17:31:38.611231 [debug] [Thread-1 (]: Databricks adapter: conn: 5359934160: _acquire sess: 01eee566-84d1-1ab8-81df-aab9a1ee3fe0, name: model.default.credit_card_type, idle: 0.01409602165222168s, acqrelcnt: 1, lang: sql, thrd: (88158, 10932187136), cmpt: ``, lut: 1710793898.597041
[0m17:31:38.611469 [debug] [Thread-1 (]: Began compiling node model.default.credit_card_type
[0m17:31:38.616915 [debug] [Thread-1 (]: Writing injected SQL for node "model.default.credit_card_type"
[0m17:31:38.619742 [debug] [Thread-1 (]: Timing info for model.default.credit_card_type (compile): 17:31:38.611621 => 17:31:38.619554
[0m17:31:38.620020 [debug] [Thread-1 (]: Began executing node model.default.credit_card_type
[0m17:31:38.650179 [debug] [Thread-1 (]: Writing runtime sql for node "model.default.credit_card_type"
[0m17:31:38.653053 [debug] [Thread-1 (]: Databricks adapter: conn: 5359934160: get_thread_connection: sess: 01eee566-84d1-1ab8-81df-aab9a1ee3fe0, name: model.default.credit_card_type, idle: 0.05591917037963867s, acqrelcnt: 1, lang: sql, thrd: (88158, 10932187136), cmpt: ``, lut: 1710793898.597041
[0m17:31:38.653262 [debug] [Thread-1 (]: Databricks adapter: conn: 5359934160: idle check connection: sess: 01eee566-84d1-1ab8-81df-aab9a1ee3fe0, name: model.default.credit_card_type, idle: 0.05614829063415527s, acqrelcnt: 1, lang: sql, thrd: (88158, 10932187136), cmpt: ``, lut: 1710793898.597041
[0m17:31:38.653411 [debug] [Thread-1 (]: Using databricks connection "model.default.credit_card_type"
[0m17:31:38.653607 [debug] [Thread-1 (]: On model.default.credit_card_type: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "node_id": "model.default.credit_card_type"} */

  
    
        create or replace table `hive_metastore`.`default`.`credit_card_type`
      
      
    using delta
      
      
      
      
      
      
      
      as
      

WITH payments AS (
    SELECT *
    FROM `hive_metastore`.`default`.`stage_payments`
)
SELECT country AS issued_at,
       credit_card_type AS credit_card_type,
       COUNT(*)
FROM payments AS p
GROUP BY country, credit_card_type
  
[0m17:31:39.689571 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "node_id": "model.default.credit_card_type"} */

  
    
        create or replace table `hive_metastore`.`default`.`credit_card_type`
      
      
    using delta
      
      
      
      
      
      
      
      as
      

WITH payments AS (
    SELECT *
    FROM `hive_metastore`.`default`.`stage_payments`
)
SELECT country AS issued_at,
       credit_card_type AS credit_card_type,
       COUNT(*)
FROM payments AS p
GROUP BY country, credit_card_type
  
[0m17:31:39.690590 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m17:31:39.692603 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_INVALID_CHARACTERS_IN_COLUMN_NAMES] com.databricks.sql.transaction.tahoe.DeltaAnalysisException: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:697)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:574)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:423)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:65)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:65)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:401)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:386)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:435)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: com.databricks.sql.transaction.tahoe.DeltaAnalysisException: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.foundInvalidCharsInColumnNames(DeltaErrors.scala:2091)
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.foundInvalidCharsInColumnNames$(DeltaErrors.scala:2090)
	at com.databricks.sql.transaction.tahoe.DeltaErrors$.foundInvalidCharsInColumnNames(DeltaErrors.scala:3145)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkSchemaFieldNames(SchemaUtils.scala:1169)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.assertMetadata(OptimisticTransaction.scala:666)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.assertMetadata$(OptimisticTransaction.scala:661)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.assertMetadata(OptimisticTransaction.scala:157)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataInternal(OptimisticTransaction.scala:631)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataInternal$(OptimisticTransaction.scala:441)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadataInternal(OptimisticTransaction.scala:157)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadata(OptimisticTransaction.scala:417)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadata$(OptimisticTransaction.scala:410)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadata(OptimisticTransaction.scala:157)
	at com.databricks.sql.transaction.tahoe.schema.ImplicitMetadataOperation.updateMetadata(ImplicitMetadataOperation.scala:108)
	at com.databricks.sql.transaction.tahoe.schema.ImplicitMetadataOperation.updateMetadata$(ImplicitMetadataOperation.scala:60)
	at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.updateMetadata(WriteIntoDelta.scala:83)
	at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.write(WriteIntoDelta.scala:284)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.doDeltaWrite$1(CreateDeltaTableCommand.scala:329)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCreateTableAsSelect(CreateDeltaTableCommand.scala:365)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCommit(CreateDeltaTableCommand.scala:200)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:147)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:197)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:184)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:67)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:161)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:266)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:264)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:67)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:160)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:668)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:686)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:663)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:66)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:148)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:68)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:55)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:107)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:429)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:408)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:67)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:159)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:149)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:139)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:67)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:133)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:326)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:266)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:264)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:106)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:147)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:1115)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:266)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:264)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:106)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:1074)
	at org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$2(WriteToDataSourceV2Exec.scala:652)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1621)
	at org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:639)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:657)
	at org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:633)
	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:209)
	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:267)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:47)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:54)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:301)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:301)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:322)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:589)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:216)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1153)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:156)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:531)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:300)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:274)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:295)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:280)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:320)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:316)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:280)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:376)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:280)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:232)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:229)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:262)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:515)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:537)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:610)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:606)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:610)
	... 36 more
Caused by: org.apache.spark.sql.AnalysisException: [INVALID_COLUMN_NAME_AS_PATH] The datasource delta cannot save the column `count(1)` because its name contains some characters that are not allowed in file paths. Please, use an alias to rename it.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.invalidColumnNameAsPathError(QueryCompilationErrors.scala:3252)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.$anonfun$checkFieldNames$1(SchemaUtils.scala:1182)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.$anonfun$checkFieldNames$1$adapted(SchemaUtils.scala:1179)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkFieldNames(SchemaUtils.scala:1179)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkSchemaFieldNames(SchemaUtils.scala:1166)
	... 151 more

[0m17:31:39.694329 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01eee566-85af-1ac5-85e1-47d47759327a
[0m17:31:39.695722 [debug] [Thread-1 (]: Timing info for model.default.credit_card_type (execute): 17:31:38.620169 => 17:31:39.695296
[0m17:31:39.696554 [debug] [Thread-1 (]: Databricks adapter: conn: 5359934160: _release sess: 01eee566-84d1-1ab8-81df-aab9a1ee3fe0, name: model.default.credit_card_type, idle: 6.9141387939453125e-06s, acqrelcnt: 0, lang: sql, thrd: (88158, 10932187136), cmpt: ``, lut: 1710793899.6963022
[0m17:31:39.724535 [debug] [Thread-1 (]: Runtime Error in model credit_card_type (models/trusted/credit_card_type.sql)
  Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m17:31:39.725214 [debug] [Thread-1 (]: Databricks adapter: conn: 5359934160: _release sess: 01eee566-84d1-1ab8-81df-aab9a1ee3fe0, name: model.default.credit_card_type, idle: 2.86102294921875e-06s, acqrelcnt: 0, lang: sql, thrd: (88158, 10932187136), cmpt: ``, lut: 1710793899.7250361
[0m17:31:39.725650 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b42c350f-83d7-43f8-8418-6ed1c86aa9dc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13f7bd890>]}
[0m17:31:39.726213 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model default.credit_card_type ................. [[31mERROR[0m in 1.12s]
[0m17:31:39.726708 [debug] [Thread-1 (]: Finished running node model.default.credit_card_type
[0m17:31:39.728148 [debug] [MainThread]: Databricks adapter: conn: 5359590160: idle check connection: sess: None, name: master, idle: 1.1250522136688232s, acqrelcnt: 0, lang: None, thrd: (88158, 7965269056), cmpt: ``, lut: 1710793898.602973
[0m17:31:39.728514 [debug] [MainThread]: Databricks adapter: conn: 5359590160: reusing connection master sess: None, name: master, idle: 1.1254570484161377s, acqrelcnt: 0, lang: None, thrd: (88158, 7965269056), cmpt: ``, lut: 1710793898.602973
[0m17:31:39.728773 [debug] [MainThread]: Databricks adapter: Thread (88158, 7965269056) using default compute resource.
[0m17:31:39.729016 [debug] [MainThread]: Databricks adapter: conn: 5359590160: _acquire sess: None, name: master, idle: 1.1259710788726807s, acqrelcnt: 1, lang: None, thrd: (88158, 7965269056), cmpt: ``, lut: 1710793898.602973
[0m17:31:39.729324 [debug] [MainThread]: Databricks adapter: conn: 5359590160: get_thread_connection: sess: None, name: master, idle: 1.1262791156768799s, acqrelcnt: 1, lang: None, thrd: (88158, 7965269056), cmpt: ``, lut: 1710793898.602973
[0m17:31:39.729565 [debug] [MainThread]: Databricks adapter: conn: 5359590160: idle check connection: sess: None, name: master, idle: 1.1265230178833008s, acqrelcnt: 1, lang: None, thrd: (88158, 7965269056), cmpt: ``, lut: 1710793898.602973
[0m17:31:39.729796 [debug] [MainThread]: On master: ROLLBACK
[0m17:31:39.730018 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:31:40.220895 [debug] [MainThread]: Databricks adapter: conn: 5359590160: session opened sess: 01eee566-8683-1f67-8332-c11c398feb56, name: master, idle: 1.1205673217773438e-05s, acqrelcnt: 1, lang: None, thrd: (88158, 7965269056), cmpt: ``, lut: 1710793900.220363
[0m17:31:40.222145 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:31:40.222770 [debug] [MainThread]: Databricks adapter: conn: 5359590160: get_thread_connection: sess: 01eee566-8683-1f67-8332-c11c398feb56, name: master, idle: 0.002257108688354492s, acqrelcnt: 1, lang: None, thrd: (88158, 7965269056), cmpt: ``, lut: 1710793900.220363
[0m17:31:40.223232 [debug] [MainThread]: Databricks adapter: conn: 5359590160: idle check connection: sess: 01eee566-8683-1f67-8332-c11c398feb56, name: master, idle: 0.002746105194091797s, acqrelcnt: 1, lang: None, thrd: (88158, 7965269056), cmpt: ``, lut: 1710793900.220363
[0m17:31:40.223663 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:31:40.224025 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:31:40.224405 [debug] [MainThread]: Databricks adapter: conn: 5359590160: _release sess: 01eee566-8683-1f67-8332-c11c398feb56, name: master, idle: 2.86102294921875e-06s, acqrelcnt: 0, lang: None, thrd: (88158, 7965269056), cmpt: ``, lut: 1710793900.224291
[0m17:31:40.225743 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:31:40.226441 [debug] [MainThread]: On master: ROLLBACK
[0m17:31:40.226977 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:31:40.227390 [debug] [MainThread]: On master: Close
[0m17:31:40.394929 [debug] [MainThread]: Connection 'model.default.credit_card_type' was properly closed.
[0m17:31:40.396113 [debug] [MainThread]: On model.default.credit_card_type: ROLLBACK
[0m17:31:40.396775 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:31:40.397283 [debug] [MainThread]: On model.default.credit_card_type: Close
[0m17:31:40.575948 [info ] [MainThread]: 
[0m17:31:40.577864 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 3.68 seconds (3.68s).
[0m17:31:40.580706 [debug] [MainThread]: Command end result
[0m17:31:40.602780 [info ] [MainThread]: 
[0m17:31:40.603195 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m17:31:40.603457 [info ] [MainThread]: 
[0m17:31:40.603802 [error] [MainThread]:   Runtime Error in model credit_card_type (models/trusted/credit_card_type.sql)
  Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m17:31:40.604122 [info ] [MainThread]: 
[0m17:31:40.604418 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m17:31:40.612118 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 5.48212, "process_user_time": 2.238848, "process_kernel_time": 3.448763, "process_mem_max_rss": "220282880", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m17:31:40.612594 [debug] [MainThread]: Command `cli run` failed at 17:31:40.612497 after 5.48 seconds
[0m17:31:40.612937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1061c2310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1061c1fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100c9bcd0>]}
[0m17:31:40.613236 [debug] [MainThread]: Flushing usage events
[0m17:33:17.350861 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1158623d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1158c1e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1158c2410>]}


============================== 17:33:17.353681 | 73db5e6a-bc59-4542-8218-d21d44315bf6 ==============================
[0m17:33:17.353681 [info ] [MainThread]: Running with dbt=1.7.8
[0m17:33:17.353972 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m17:33:18.883317 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '73db5e6a-bc59-4542-8218-d21d44315bf6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1158c7c10>]}
[0m17:33:18.912102 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '73db5e6a-bc59-4542-8218-d21d44315bf6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16ea60e10>]}
[0m17:33:18.912383 [info ] [MainThread]: Registered adapter: databricks=1.7.9
[0m17:33:18.930237 [debug] [MainThread]: checksum: 67f0013ca5f0bd43af9a0873dd50792fde83ef69de63b71cacd0b4ac656c52e5, vars: {}, profile: , target: , version: 1.7.8
[0m17:33:19.004572 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:33:19.004834 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:33:19.007828 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '73db5e6a-bc59-4542-8218-d21d44315bf6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16ea22e90>]}
[0m17:33:19.014734 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '73db5e6a-bc59-4542-8218-d21d44315bf6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16e947150>]}
[0m17:33:19.014980 [info ] [MainThread]: Found 4 models, 3 sources, 0 exposures, 0 metrics, 539 macros, 0 groups, 0 semantic models
[0m17:33:19.015145 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '73db5e6a-bc59-4542-8218-d21d44315bf6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16e904750>]}
[0m17:33:19.015860 [info ] [MainThread]: 
[0m17:33:19.016329 [debug] [MainThread]: Databricks adapter: conn: 6150107600: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (88455, 7965269056), cmpt: ``, lut: None
[0m17:33:19.016474 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:33:19.016596 [debug] [MainThread]: Databricks adapter: Thread (88455, 7965269056) using default compute resource.
[0m17:33:19.016716 [debug] [MainThread]: Databricks adapter: conn: 6150107600: _acquire sess: None, name: master, idle: 1.1920928955078125e-06s, acqrelcnt: 1, lang: None, thrd: (88455, 7965269056), cmpt: ``, lut: 1710793999.0166788
[0m17:33:19.017268 [debug] [ThreadPool]: Databricks adapter: conn: 6151608592: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore, idle: 0s, acqrelcnt: 0, lang: None, thrd: (88455, 6169849856), cmpt: ``, lut: None
[0m17:33:19.017470 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m17:33:19.017613 [debug] [ThreadPool]: Databricks adapter: Thread (88455, 6169849856) using default compute resource.
[0m17:33:19.017744 [debug] [ThreadPool]: Databricks adapter: conn: 6151608592: _acquire sess: None, name: list_hive_metastore, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (88455, 6169849856), cmpt: ``, lut: 1710793999.017705
[0m17:33:19.017885 [debug] [ThreadPool]: Databricks adapter: conn: 6151608592: get_thread_connection: sess: None, name: list_hive_metastore, idle: 0.0001430511474609375s, acqrelcnt: 1, lang: None, thrd: (88455, 6169849856), cmpt: ``, lut: 1710793999.017705
[0m17:33:19.018016 [debug] [ThreadPool]: Databricks adapter: conn: 6151608592: idle check connection: sess: None, name: list_hive_metastore, idle: 0.00026798248291015625s, acqrelcnt: 1, lang: None, thrd: (88455, 6169849856), cmpt: ``, lut: 1710793999.017705
[0m17:33:19.018129 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m17:33:19.018253 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m17:33:19.018370 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:33:19.539555 [debug] [ThreadPool]: Databricks adapter: conn: 6151608592: session opened sess: 01eee566-c1b9-1e5c-9540-59a0c4e9a926, name: list_hive_metastore, idle: 2.1457672119140625e-06s, acqrelcnt: 1, lang: None, thrd: (88455, 6169849856), cmpt: ``, lut: 1710793999.5394619
[0m17:33:19.816899 [debug] [ThreadPool]: SQL status: OK in 0.800000011920929 seconds
[0m17:33:19.820767 [debug] [ThreadPool]: Databricks adapter: conn: 6151608592: _release sess: 01eee566-c1b9-1e5c-9540-59a0c4e9a926, name: list_hive_metastore, idle: 5.9604644775390625e-06s, acqrelcnt: 0, lang: None, thrd: (88455, 6169849856), cmpt: ``, lut: 1710793999.820642
[0m17:33:19.822478 [debug] [ThreadPool]: Databricks adapter: conn: 6151608592: idle check connection: sess: 01eee566-c1b9-1e5c-9540-59a0c4e9a926, name: list_hive_metastore, idle: 0.001714944839477539s, acqrelcnt: 0, lang: None, thrd: (88455, 6169849856), cmpt: ``, lut: 1710793999.820642
[0m17:33:19.822900 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now list_hive_metastore_default)
[0m17:33:19.823236 [debug] [ThreadPool]: Databricks adapter: conn: 6151608592: reusing connection list_hive_metastore sess: 01eee566-c1b9-1e5c-9540-59a0c4e9a926, name: list_hive_metastore_default, idle: 0.002502918243408203s, acqrelcnt: 0, lang: None, thrd: (88455, 6169849856), cmpt: ``, lut: 1710793999.820642
[0m17:33:19.823495 [debug] [ThreadPool]: Databricks adapter: Thread (88455, 6169849856) using default compute resource.
[0m17:33:19.823741 [debug] [ThreadPool]: Databricks adapter: conn: 6151608592: _acquire sess: 01eee566-c1b9-1e5c-9540-59a0c4e9a926, name: list_hive_metastore_default, idle: 0.003022909164428711s, acqrelcnt: 1, lang: None, thrd: (88455, 6169849856), cmpt: ``, lut: 1710793999.820642
[0m17:33:19.827863 [debug] [ThreadPool]: Databricks adapter: conn: 6151608592: get_thread_connection: sess: 01eee566-c1b9-1e5c-9540-59a0c4e9a926, name: list_hive_metastore_default, idle: 0.007133007049560547s, acqrelcnt: 1, lang: None, thrd: (88455, 6169849856), cmpt: ``, lut: 1710793999.820642
[0m17:33:19.828121 [debug] [ThreadPool]: Databricks adapter: conn: 6151608592: idle check connection: sess: 01eee566-c1b9-1e5c-9540-59a0c4e9a926, name: list_hive_metastore_default, idle: 0.007415056228637695s, acqrelcnt: 1, lang: None, thrd: (88455, 6169849856), cmpt: ``, lut: 1710793999.820642
[0m17:33:19.828322 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:33:19.828513 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m17:33:20.090754 [debug] [ThreadPool]: SQL status: OK in 0.25999999046325684 seconds
[0m17:33:20.097130 [debug] [ThreadPool]: Databricks adapter: conn: 6151608592: get_thread_connection: sess: 01eee566-c1b9-1e5c-9540-59a0c4e9a926, name: list_hive_metastore_default, idle: 0.27640604972839355s, acqrelcnt: 1, lang: None, thrd: (88455, 6169849856), cmpt: ``, lut: 1710793999.820642
[0m17:33:20.097376 [debug] [ThreadPool]: Databricks adapter: conn: 6151608592: idle check connection: sess: 01eee566-c1b9-1e5c-9540-59a0c4e9a926, name: list_hive_metastore_default, idle: 0.2766878604888916s, acqrelcnt: 1, lang: None, thrd: (88455, 6169849856), cmpt: ``, lut: 1710793999.820642
[0m17:33:20.097526 [debug] [ThreadPool]: Databricks adapter: conn: 6151608592: get_thread_connection: sess: 01eee566-c1b9-1e5c-9540-59a0c4e9a926, name: list_hive_metastore_default, idle: 0.2768440246582031s, acqrelcnt: 1, lang: None, thrd: (88455, 6169849856), cmpt: ``, lut: 1710793999.820642
[0m17:33:20.097663 [debug] [ThreadPool]: Databricks adapter: conn: 6151608592: idle check connection: sess: 01eee566-c1b9-1e5c-9540-59a0c4e9a926, name: list_hive_metastore_default, idle: 0.2769811153411865s, acqrelcnt: 1, lang: None, thrd: (88455, 6169849856), cmpt: ``, lut: 1710793999.820642
[0m17:33:20.097798 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:33:20.097918 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:33:20.098052 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m17:33:20.364493 [debug] [ThreadPool]: SQL status: OK in 0.27000001072883606 seconds
[0m17:33:20.371447 [debug] [ThreadPool]: Databricks adapter: conn: 6151608592: get_thread_connection: sess: 01eee566-c1b9-1e5c-9540-59a0c4e9a926, name: list_hive_metastore_default, idle: 0.5506901741027832s, acqrelcnt: 1, lang: None, thrd: (88455, 6169849856), cmpt: ``, lut: 1710793999.820642
[0m17:33:20.371802 [debug] [ThreadPool]: Databricks adapter: conn: 6151608592: idle check connection: sess: 01eee566-c1b9-1e5c-9540-59a0c4e9a926, name: list_hive_metastore_default, idle: 0.5510809421539307s, acqrelcnt: 1, lang: None, thrd: (88455, 6169849856), cmpt: ``, lut: 1710793999.820642
[0m17:33:20.372049 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:33:20.372303 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m17:33:20.674130 [debug] [ThreadPool]: SQL status: OK in 0.30000001192092896 seconds
[0m17:33:20.677163 [debug] [ThreadPool]: Databricks adapter: conn: 6151608592: _release sess: 01eee566-c1b9-1e5c-9540-59a0c4e9a926, name: list_hive_metastore_default, idle: 3.0994415283203125e-06s, acqrelcnt: 0, lang: None, thrd: (88455, 6169849856), cmpt: ``, lut: 1710794000.677022
[0m17:33:20.680445 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '73db5e6a-bc59-4542-8218-d21d44315bf6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16ea2df10>]}
[0m17:33:20.680872 [debug] [MainThread]: Databricks adapter: conn: 6150107600: get_thread_connection: sess: None, name: master, idle: 1.6641051769256592s, acqrelcnt: 1, lang: None, thrd: (88455, 7965269056), cmpt: ``, lut: 1710793999.0166788
[0m17:33:20.681141 [debug] [MainThread]: Databricks adapter: conn: 6150107600: idle check connection: sess: None, name: master, idle: 1.6643872261047363s, acqrelcnt: 1, lang: None, thrd: (88455, 7965269056), cmpt: ``, lut: 1710793999.0166788
[0m17:33:20.681399 [debug] [MainThread]: Databricks adapter: conn: 6150107600: get_thread_connection: sess: None, name: master, idle: 1.6646442413330078s, acqrelcnt: 1, lang: None, thrd: (88455, 7965269056), cmpt: ``, lut: 1710793999.0166788
[0m17:33:20.681650 [debug] [MainThread]: Databricks adapter: conn: 6150107600: idle check connection: sess: None, name: master, idle: 1.664900302886963s, acqrelcnt: 1, lang: None, thrd: (88455, 7965269056), cmpt: ``, lut: 1710793999.0166788
[0m17:33:20.681877 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:33:20.682098 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:33:20.682345 [debug] [MainThread]: Databricks adapter: conn: 6150107600: _release sess: None, name: master, idle: 1.9073486328125e-06s, acqrelcnt: 0, lang: None, thrd: (88455, 7965269056), cmpt: ``, lut: 1710794000.682267
[0m17:33:20.683626 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:33:20.684115 [info ] [MainThread]: 
[0m17:33:20.688922 [debug] [Thread-1 (]: Began running node model.default.credit_card_type
[0m17:33:20.689465 [info ] [Thread-1 (]: 1 of 1 START sql table model default.credit_card_type .......................... [RUN]
[0m17:33:20.690159 [debug] [Thread-1 (]: Databricks adapter: conn: 6151608592: idle check connection: sess: 01eee566-c1b9-1e5c-9540-59a0c4e9a926, name: list_hive_metastore_default, idle: 0.013010978698730469s, acqrelcnt: 0, lang: None, thrd: (88455, 6169849856), cmpt: ``, lut: 1710794000.677022
[0m17:33:20.690406 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.default.credit_card_type)
[0m17:33:20.690668 [debug] [Thread-1 (]: Databricks adapter: conn: 6151608592: reusing connection list_hive_metastore_default sess: 01eee566-c1b9-1e5c-9540-59a0c4e9a926, name: model.default.credit_card_type, idle: 0.013545036315917969s, acqrelcnt: 0, lang: None, thrd: (88455, 6169849856), cmpt: ``, lut: 1710794000.677022
[0m17:33:20.690914 [debug] [Thread-1 (]: Databricks adapter: On thread (88455, 6169849856): `hive_metastore`.`default`.`credit_card_type` using default compute resource.
[0m17:33:20.691181 [debug] [Thread-1 (]: Databricks adapter: conn: 6151608592: _acquire sess: 01eee566-c1b9-1e5c-9540-59a0c4e9a926, name: model.default.credit_card_type, idle: 0.014049053192138672s, acqrelcnt: 1, lang: sql, thrd: (88455, 6169849856), cmpt: ``, lut: 1710794000.677022
[0m17:33:20.691472 [debug] [Thread-1 (]: Began compiling node model.default.credit_card_type
[0m17:33:20.697405 [debug] [Thread-1 (]: Writing injected SQL for node "model.default.credit_card_type"
[0m17:33:20.699989 [debug] [Thread-1 (]: Timing info for model.default.credit_card_type (compile): 17:33:20.691649 => 17:33:20.699637
[0m17:33:20.700404 [debug] [Thread-1 (]: Began executing node model.default.credit_card_type
[0m17:33:20.733715 [debug] [Thread-1 (]: Writing runtime sql for node "model.default.credit_card_type"
[0m17:33:20.736314 [debug] [Thread-1 (]: Databricks adapter: conn: 6151608592: get_thread_connection: sess: 01eee566-c1b9-1e5c-9540-59a0c4e9a926, name: model.default.credit_card_type, idle: 0.05918312072753906s, acqrelcnt: 1, lang: sql, thrd: (88455, 6169849856), cmpt: ``, lut: 1710794000.677022
[0m17:33:20.736540 [debug] [Thread-1 (]: Databricks adapter: conn: 6151608592: idle check connection: sess: 01eee566-c1b9-1e5c-9540-59a0c4e9a926, name: model.default.credit_card_type, idle: 0.05944180488586426s, acqrelcnt: 1, lang: sql, thrd: (88455, 6169849856), cmpt: ``, lut: 1710794000.677022
[0m17:33:20.736703 [debug] [Thread-1 (]: Using databricks connection "model.default.credit_card_type"
[0m17:33:20.736912 [debug] [Thread-1 (]: On model.default.credit_card_type: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "node_id": "model.default.credit_card_type"} */

  
    
        create or replace table `hive_metastore`.`default`.`credit_card_type`
      
      
    using delta
      
      
      
      
      
      
      
      as
      

WITH payments AS (
    SELECT *
    FROM `hive_metastore`.`default`.`stage_payments`
)
SELECT country AS issued_at,
       credit_card_type AS credit_card_type,
       COUNT(*)
FROM payments AS p
GROUP BY country, credit_card_type
  
[0m17:33:21.444727 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "node_id": "model.default.credit_card_type"} */

  
    
        create or replace table `hive_metastore`.`default`.`credit_card_type`
      
      
    using delta
      
      
      
      
      
      
      
      as
      

WITH payments AS (
    SELECT *
    FROM `hive_metastore`.`default`.`stage_payments`
)
SELECT country AS issued_at,
       credit_card_type AS credit_card_type,
       COUNT(*)
FROM payments AS p
GROUP BY country, credit_card_type
  
[0m17:33:21.447450 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m17:33:21.449377 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_INVALID_CHARACTERS_IN_COLUMN_NAMES] com.databricks.sql.transaction.tahoe.DeltaAnalysisException: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:697)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:574)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:423)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:65)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:65)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:401)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:386)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:435)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: com.databricks.sql.transaction.tahoe.DeltaAnalysisException: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.foundInvalidCharsInColumnNames(DeltaErrors.scala:2091)
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.foundInvalidCharsInColumnNames$(DeltaErrors.scala:2090)
	at com.databricks.sql.transaction.tahoe.DeltaErrors$.foundInvalidCharsInColumnNames(DeltaErrors.scala:3145)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkSchemaFieldNames(SchemaUtils.scala:1169)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.assertMetadata(OptimisticTransaction.scala:666)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.assertMetadata$(OptimisticTransaction.scala:661)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.assertMetadata(OptimisticTransaction.scala:157)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataInternal(OptimisticTransaction.scala:631)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataInternal$(OptimisticTransaction.scala:441)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadataInternal(OptimisticTransaction.scala:157)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadata(OptimisticTransaction.scala:417)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadata$(OptimisticTransaction.scala:410)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadata(OptimisticTransaction.scala:157)
	at com.databricks.sql.transaction.tahoe.schema.ImplicitMetadataOperation.updateMetadata(ImplicitMetadataOperation.scala:108)
	at com.databricks.sql.transaction.tahoe.schema.ImplicitMetadataOperation.updateMetadata$(ImplicitMetadataOperation.scala:60)
	at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.updateMetadata(WriteIntoDelta.scala:83)
	at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.write(WriteIntoDelta.scala:284)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.doDeltaWrite$1(CreateDeltaTableCommand.scala:329)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCreateTableAsSelect(CreateDeltaTableCommand.scala:365)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCommit(CreateDeltaTableCommand.scala:200)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:147)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:197)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:184)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:67)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:161)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:266)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:264)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:67)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:160)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:668)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:686)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:663)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:66)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:148)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:68)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:55)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:107)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:429)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:408)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:67)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:159)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:149)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:139)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:67)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:133)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:326)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:266)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:264)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:106)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:147)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:1115)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:266)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:264)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:106)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:1074)
	at org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$2(WriteToDataSourceV2Exec.scala:652)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1621)
	at org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:639)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:657)
	at org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:633)
	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:209)
	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:267)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:47)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:54)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:301)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:301)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:322)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:589)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:216)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1153)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:156)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:531)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:300)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:274)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:295)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:280)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:320)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:316)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:280)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:376)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:280)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:232)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:229)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:262)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:515)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:537)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:610)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:606)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:610)
	... 36 more
Caused by: org.apache.spark.sql.AnalysisException: [INVALID_COLUMN_NAME_AS_PATH] The datasource delta cannot save the column `count(1)` because its name contains some characters that are not allowed in file paths. Please, use an alias to rename it.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.invalidColumnNameAsPathError(QueryCompilationErrors.scala:3252)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.$anonfun$checkFieldNames$1(SchemaUtils.scala:1182)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.$anonfun$checkFieldNames$1$adapted(SchemaUtils.scala:1179)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkFieldNames(SchemaUtils.scala:1179)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkSchemaFieldNames(SchemaUtils.scala:1166)
	... 151 more

[0m17:33:21.450920 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01eee566-c288-13ed-aa9b-f4fb3d7acabc
[0m17:33:21.451658 [debug] [Thread-1 (]: Timing info for model.default.credit_card_type (execute): 17:33:20.700571 => 17:33:21.451407
[0m17:33:21.452231 [debug] [Thread-1 (]: Databricks adapter: conn: 6151608592: _release sess: 01eee566-c1b9-1e5c-9540-59a0c4e9a926, name: model.default.credit_card_type, idle: 6.9141387939453125e-06s, acqrelcnt: 0, lang: sql, thrd: (88455, 6169849856), cmpt: ``, lut: 1710794001.452078
[0m17:33:21.482447 [debug] [Thread-1 (]: Runtime Error in model credit_card_type (models/trusted/credit_card_type.sql)
  Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m17:33:21.483037 [debug] [Thread-1 (]: Databricks adapter: conn: 6151608592: _release sess: 01eee566-c1b9-1e5c-9540-59a0c4e9a926, name: model.default.credit_card_type, idle: 2.86102294921875e-06s, acqrelcnt: 0, lang: sql, thrd: (88455, 6169849856), cmpt: ``, lut: 1710794001.4828641
[0m17:33:21.483491 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '73db5e6a-bc59-4542-8218-d21d44315bf6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16eaf7bd0>]}
[0m17:33:21.484122 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model default.credit_card_type ................. [[31mERROR[0m in 0.79s]
[0m17:33:21.484603 [debug] [Thread-1 (]: Finished running node model.default.credit_card_type
[0m17:33:21.486115 [debug] [MainThread]: Databricks adapter: conn: 6150107600: idle check connection: sess: None, name: master, idle: 0.8037271499633789s, acqrelcnt: 0, lang: None, thrd: (88455, 7965269056), cmpt: ``, lut: 1710794000.682267
[0m17:33:21.486491 [debug] [MainThread]: Databricks adapter: conn: 6150107600: reusing connection master sess: None, name: master, idle: 0.8041341304779053s, acqrelcnt: 0, lang: None, thrd: (88455, 7965269056), cmpt: ``, lut: 1710794000.682267
[0m17:33:21.486763 [debug] [MainThread]: Databricks adapter: Thread (88455, 7965269056) using default compute resource.
[0m17:33:21.487010 [debug] [MainThread]: Databricks adapter: conn: 6150107600: _acquire sess: None, name: master, idle: 0.8046700954437256s, acqrelcnt: 1, lang: None, thrd: (88455, 7965269056), cmpt: ``, lut: 1710794000.682267
[0m17:33:21.487292 [debug] [MainThread]: Databricks adapter: conn: 6150107600: get_thread_connection: sess: None, name: master, idle: 0.8049461841583252s, acqrelcnt: 1, lang: None, thrd: (88455, 7965269056), cmpt: ``, lut: 1710794000.682267
[0m17:33:21.487546 [debug] [MainThread]: Databricks adapter: conn: 6150107600: idle check connection: sess: None, name: master, idle: 0.8052060604095459s, acqrelcnt: 1, lang: None, thrd: (88455, 7965269056), cmpt: ``, lut: 1710794000.682267
[0m17:33:21.487782 [debug] [MainThread]: On master: ROLLBACK
[0m17:33:21.488015 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:33:21.965710 [debug] [MainThread]: Databricks adapter: conn: 6150107600: session opened sess: 01eee566-c329-1b6d-9aae-881f6ae1e4df, name: master, idle: 1.1920928955078125e-05s, acqrelcnt: 1, lang: None, thrd: (88455, 7965269056), cmpt: ``, lut: 1710794001.9651752
[0m17:33:21.966946 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:33:21.967710 [debug] [MainThread]: Databricks adapter: conn: 6150107600: get_thread_connection: sess: 01eee566-c329-1b6d-9aae-881f6ae1e4df, name: master, idle: 0.0023648738861083984s, acqrelcnt: 1, lang: None, thrd: (88455, 7965269056), cmpt: ``, lut: 1710794001.9651752
[0m17:33:21.968295 [debug] [MainThread]: Databricks adapter: conn: 6150107600: idle check connection: sess: 01eee566-c329-1b6d-9aae-881f6ae1e4df, name: master, idle: 0.0029878616333007812s, acqrelcnt: 1, lang: None, thrd: (88455, 7965269056), cmpt: ``, lut: 1710794001.9651752
[0m17:33:21.968775 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:33:21.969198 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:33:21.969692 [debug] [MainThread]: Databricks adapter: conn: 6150107600: _release sess: 01eee566-c329-1b6d-9aae-881f6ae1e4df, name: master, idle: 3.0994415283203125e-06s, acqrelcnt: 0, lang: None, thrd: (88455, 7965269056), cmpt: ``, lut: 1710794001.969578
[0m17:33:21.970969 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:33:21.971431 [debug] [MainThread]: On master: ROLLBACK
[0m17:33:21.972138 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:33:21.972613 [debug] [MainThread]: On master: Close
[0m17:33:22.134141 [debug] [MainThread]: Connection 'model.default.credit_card_type' was properly closed.
[0m17:33:22.134469 [debug] [MainThread]: On model.default.credit_card_type: ROLLBACK
[0m17:33:22.134674 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:33:22.134855 [debug] [MainThread]: On model.default.credit_card_type: Close
[0m17:33:22.296570 [info ] [MainThread]: 
[0m17:33:22.297693 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 3.28 seconds (3.28s).
[0m17:33:22.299293 [debug] [MainThread]: Command end result
[0m17:33:22.317656 [info ] [MainThread]: 
[0m17:33:22.318090 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m17:33:22.318351 [info ] [MainThread]: 
[0m17:33:22.318598 [error] [MainThread]:   Runtime Error in model credit_card_type (models/trusted/credit_card_type.sql)
  Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m17:33:22.318838 [info ] [MainThread]: 
[0m17:33:22.319107 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m17:33:22.329603 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 5.006882, "process_user_time": 2.099075, "process_kernel_time": 3.457658, "process_mem_max_rss": "219955200", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m17:33:22.330158 [debug] [MainThread]: Command `cli run` failed at 17:33:22.330061 after 5.01 seconds
[0m17:33:22.330514 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11589e750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1158c1dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104ce3c90>]}
[0m17:33:22.330819 [debug] [MainThread]: Flushing usage events
[0m17:34:39.030974 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109f83350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109fce790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109ffe150>]}


============================== 17:34:39.034144 | 87b4c0f7-87e5-4eb4-af37-d1e304f29faf ==============================
[0m17:34:39.034144 [info ] [MainThread]: Running with dbt=1.7.8
[0m17:34:39.034436 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m17:34:40.390025 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '87b4c0f7-87e5-4eb4-af37-d1e304f29faf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109f6fcd0>]}
[0m17:34:40.418237 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '87b4c0f7-87e5-4eb4-af37-d1e304f29faf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16f1e4bd0>]}
[0m17:34:40.418495 [info ] [MainThread]: Registered adapter: databricks=1.7.9
[0m17:34:40.437211 [debug] [MainThread]: checksum: 67f0013ca5f0bd43af9a0873dd50792fde83ef69de63b71cacd0b4ac656c52e5, vars: {}, profile: , target: , version: 1.7.8
[0m17:34:40.518309 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:34:40.518546 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:34:40.521339 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '87b4c0f7-87e5-4eb4-af37-d1e304f29faf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16d430890>]}
[0m17:34:40.530459 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '87b4c0f7-87e5-4eb4-af37-d1e304f29faf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16f411350>]}
[0m17:34:40.530685 [info ] [MainThread]: Found 4 models, 3 sources, 0 exposures, 0 metrics, 539 macros, 0 groups, 0 semantic models
[0m17:34:40.530856 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '87b4c0f7-87e5-4eb4-af37-d1e304f29faf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16f404a10>]}
[0m17:34:40.531579 [info ] [MainThread]: 
[0m17:34:40.532011 [debug] [MainThread]: Databricks adapter: conn: 6161506320: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (88711, 7965269056), cmpt: ``, lut: None
[0m17:34:40.532165 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:34:40.532277 [debug] [MainThread]: Databricks adapter: Thread (88711, 7965269056) using default compute resource.
[0m17:34:40.532390 [debug] [MainThread]: Databricks adapter: conn: 6161506320: _acquire sess: None, name: master, idle: 1.1920928955078125e-06s, acqrelcnt: 1, lang: None, thrd: (88711, 7965269056), cmpt: ``, lut: 1710794080.532353
[0m17:34:40.532914 [debug] [ThreadPool]: Databricks adapter: conn: 6162094672: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore, idle: 0s, acqrelcnt: 0, lang: None, thrd: (88711, 11433504768), cmpt: ``, lut: None
[0m17:34:40.533105 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m17:34:40.533227 [debug] [ThreadPool]: Databricks adapter: Thread (88711, 11433504768) using default compute resource.
[0m17:34:40.533347 [debug] [ThreadPool]: Databricks adapter: conn: 6162094672: _acquire sess: None, name: list_hive_metastore, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (88711, 11433504768), cmpt: ``, lut: 1710794080.5333068
[0m17:34:40.533480 [debug] [ThreadPool]: Databricks adapter: conn: 6162094672: get_thread_connection: sess: None, name: list_hive_metastore, idle: 0.00013828277587890625s, acqrelcnt: 1, lang: None, thrd: (88711, 11433504768), cmpt: ``, lut: 1710794080.5333068
[0m17:34:40.533601 [debug] [ThreadPool]: Databricks adapter: conn: 6162094672: idle check connection: sess: None, name: list_hive_metastore, idle: 0.00025916099548339844s, acqrelcnt: 1, lang: None, thrd: (88711, 11433504768), cmpt: ``, lut: 1710794080.5333068
[0m17:34:40.533708 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m17:34:40.533821 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m17:34:40.533930 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:34:41.018406 [debug] [ThreadPool]: Databricks adapter: conn: 6162094672: session opened sess: 01eee566-f247-135d-9fb1-4c505a6cbedc, name: list_hive_metastore, idle: 1.3113021850585938e-05s, acqrelcnt: 1, lang: None, thrd: (88711, 11433504768), cmpt: ``, lut: 1710794081.0180008
[0m17:34:41.388616 [debug] [ThreadPool]: SQL status: OK in 0.8500000238418579 seconds
[0m17:34:41.394163 [debug] [ThreadPool]: Databricks adapter: conn: 6162094672: _release sess: 01eee566-f247-135d-9fb1-4c505a6cbedc, name: list_hive_metastore, idle: 5.7220458984375e-06s, acqrelcnt: 0, lang: None, thrd: (88711, 11433504768), cmpt: ``, lut: 1710794081.3939872
[0m17:34:41.396397 [debug] [ThreadPool]: Databricks adapter: conn: 6162094672: idle check connection: sess: 01eee566-f247-135d-9fb1-4c505a6cbedc, name: list_hive_metastore, idle: 0.0022106170654296875s, acqrelcnt: 0, lang: None, thrd: (88711, 11433504768), cmpt: ``, lut: 1710794081.3939872
[0m17:34:41.397009 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now list_hive_metastore_default)
[0m17:34:41.397371 [debug] [ThreadPool]: Databricks adapter: conn: 6162094672: reusing connection list_hive_metastore sess: 01eee566-f247-135d-9fb1-4c505a6cbedc, name: list_hive_metastore_default, idle: 0.003290891647338867s, acqrelcnt: 0, lang: None, thrd: (88711, 11433504768), cmpt: ``, lut: 1710794081.3939872
[0m17:34:41.397652 [debug] [ThreadPool]: Databricks adapter: Thread (88711, 11433504768) using default compute resource.
[0m17:34:41.397919 [debug] [ThreadPool]: Databricks adapter: conn: 6162094672: _acquire sess: 01eee566-f247-135d-9fb1-4c505a6cbedc, name: list_hive_metastore_default, idle: 0.0038557052612304688s, acqrelcnt: 1, lang: None, thrd: (88711, 11433504768), cmpt: ``, lut: 1710794081.3939872
[0m17:34:41.402718 [debug] [ThreadPool]: Databricks adapter: conn: 6162094672: get_thread_connection: sess: 01eee566-f247-135d-9fb1-4c505a6cbedc, name: list_hive_metastore_default, idle: 0.008630990982055664s, acqrelcnt: 1, lang: None, thrd: (88711, 11433504768), cmpt: ``, lut: 1710794081.3939872
[0m17:34:41.403015 [debug] [ThreadPool]: Databricks adapter: conn: 6162094672: idle check connection: sess: 01eee566-f247-135d-9fb1-4c505a6cbedc, name: list_hive_metastore_default, idle: 0.00895380973815918s, acqrelcnt: 1, lang: None, thrd: (88711, 11433504768), cmpt: ``, lut: 1710794081.3939872
[0m17:34:41.403229 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:34:41.403442 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m17:34:41.703123 [debug] [ThreadPool]: SQL status: OK in 0.30000001192092896 seconds
[0m17:34:41.712116 [debug] [ThreadPool]: Databricks adapter: conn: 6162094672: get_thread_connection: sess: 01eee566-f247-135d-9fb1-4c505a6cbedc, name: list_hive_metastore_default, idle: 0.31802868843078613s, acqrelcnt: 1, lang: None, thrd: (88711, 11433504768), cmpt: ``, lut: 1710794081.3939872
[0m17:34:41.712426 [debug] [ThreadPool]: Databricks adapter: conn: 6162094672: idle check connection: sess: 01eee566-f247-135d-9fb1-4c505a6cbedc, name: list_hive_metastore_default, idle: 0.3183727264404297s, acqrelcnt: 1, lang: None, thrd: (88711, 11433504768), cmpt: ``, lut: 1710794081.3939872
[0m17:34:41.712653 [debug] [ThreadPool]: Databricks adapter: conn: 6162094672: get_thread_connection: sess: 01eee566-f247-135d-9fb1-4c505a6cbedc, name: list_hive_metastore_default, idle: 0.3186068534851074s, acqrelcnt: 1, lang: None, thrd: (88711, 11433504768), cmpt: ``, lut: 1710794081.3939872
[0m17:34:41.712853 [debug] [ThreadPool]: Databricks adapter: conn: 6162094672: idle check connection: sess: 01eee566-f247-135d-9fb1-4c505a6cbedc, name: list_hive_metastore_default, idle: 0.31880879402160645s, acqrelcnt: 1, lang: None, thrd: (88711, 11433504768), cmpt: ``, lut: 1710794081.3939872
[0m17:34:41.713042 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:34:41.713216 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:34:41.713401 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m17:34:41.960513 [debug] [ThreadPool]: SQL status: OK in 0.25 seconds
[0m17:34:41.966869 [debug] [ThreadPool]: Databricks adapter: conn: 6162094672: get_thread_connection: sess: 01eee566-f247-135d-9fb1-4c505a6cbedc, name: list_hive_metastore_default, idle: 0.5727648735046387s, acqrelcnt: 1, lang: None, thrd: (88711, 11433504768), cmpt: ``, lut: 1710794081.3939872
[0m17:34:41.967229 [debug] [ThreadPool]: Databricks adapter: conn: 6162094672: idle check connection: sess: 01eee566-f247-135d-9fb1-4c505a6cbedc, name: list_hive_metastore_default, idle: 0.5731618404388428s, acqrelcnt: 1, lang: None, thrd: (88711, 11433504768), cmpt: ``, lut: 1710794081.3939872
[0m17:34:41.967472 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:34:41.967729 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m17:34:42.252756 [debug] [ThreadPool]: SQL status: OK in 0.2800000011920929 seconds
[0m17:34:42.260095 [debug] [ThreadPool]: Databricks adapter: conn: 6162094672: _release sess: 01eee566-f247-135d-9fb1-4c505a6cbedc, name: list_hive_metastore_default, idle: 1.3113021850585938e-05s, acqrelcnt: 0, lang: None, thrd: (88711, 11433504768), cmpt: ``, lut: 1710794082.2597299
[0m17:34:42.264435 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '87b4c0f7-87e5-4eb4-af37-d1e304f29faf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109f87610>]}
[0m17:34:42.265226 [debug] [MainThread]: Databricks adapter: conn: 6161506320: get_thread_connection: sess: None, name: master, idle: 1.7327618598937988s, acqrelcnt: 1, lang: None, thrd: (88711, 7965269056), cmpt: ``, lut: 1710794080.532353
[0m17:34:42.265546 [debug] [MainThread]: Databricks adapter: conn: 6161506320: idle check connection: sess: None, name: master, idle: 1.7331101894378662s, acqrelcnt: 1, lang: None, thrd: (88711, 7965269056), cmpt: ``, lut: 1710794080.532353
[0m17:34:42.265823 [debug] [MainThread]: Databricks adapter: conn: 6161506320: get_thread_connection: sess: None, name: master, idle: 1.7333881855010986s, acqrelcnt: 1, lang: None, thrd: (88711, 7965269056), cmpt: ``, lut: 1710794080.532353
[0m17:34:42.266101 [debug] [MainThread]: Databricks adapter: conn: 6161506320: idle check connection: sess: None, name: master, idle: 1.7336771488189697s, acqrelcnt: 1, lang: None, thrd: (88711, 7965269056), cmpt: ``, lut: 1710794080.532353
[0m17:34:42.266340 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:34:42.266561 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:34:42.266804 [debug] [MainThread]: Databricks adapter: conn: 6161506320: _release sess: None, name: master, idle: 9.5367431640625e-07s, acqrelcnt: 0, lang: None, thrd: (88711, 7965269056), cmpt: ``, lut: 1710794082.266731
[0m17:34:42.267460 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:34:42.267750 [info ] [MainThread]: 
[0m17:34:42.272617 [debug] [Thread-1 (]: Began running node model.default.credit_card_type
[0m17:34:42.273131 [info ] [Thread-1 (]: 1 of 1 START sql table model default.credit_card_type .......................... [RUN]
[0m17:34:42.273844 [debug] [Thread-1 (]: Databricks adapter: conn: 6162094672: idle check connection: sess: 01eee566-f247-135d-9fb1-4c505a6cbedc, name: list_hive_metastore_default, idle: 0.013987064361572266s, acqrelcnt: 0, lang: None, thrd: (88711, 11433504768), cmpt: ``, lut: 1710794082.2597299
[0m17:34:42.274088 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.default.credit_card_type)
[0m17:34:42.274362 [debug] [Thread-1 (]: Databricks adapter: conn: 6162094672: reusing connection list_hive_metastore_default sess: 01eee566-f247-135d-9fb1-4c505a6cbedc, name: model.default.credit_card_type, idle: 0.014522075653076172s, acqrelcnt: 0, lang: None, thrd: (88711, 11433504768), cmpt: ``, lut: 1710794082.2597299
[0m17:34:42.274608 [debug] [Thread-1 (]: Databricks adapter: On thread (88711, 11433504768): `hive_metastore`.`default`.`credit_card_type` using default compute resource.
[0m17:34:42.274856 [debug] [Thread-1 (]: Databricks adapter: conn: 6162094672: _acquire sess: 01eee566-f247-135d-9fb1-4c505a6cbedc, name: model.default.credit_card_type, idle: 0.015032052993774414s, acqrelcnt: 1, lang: sql, thrd: (88711, 11433504768), cmpt: ``, lut: 1710794082.2597299
[0m17:34:42.275096 [debug] [Thread-1 (]: Began compiling node model.default.credit_card_type
[0m17:34:42.280527 [debug] [Thread-1 (]: Writing injected SQL for node "model.default.credit_card_type"
[0m17:34:42.309884 [debug] [Thread-1 (]: Timing info for model.default.credit_card_type (compile): 17:34:42.275253 => 17:34:42.309619
[0m17:34:42.310215 [debug] [Thread-1 (]: Began executing node model.default.credit_card_type
[0m17:34:42.340626 [debug] [Thread-1 (]: Writing runtime sql for node "model.default.credit_card_type"
[0m17:34:42.353750 [debug] [Thread-1 (]: Databricks adapter: conn: 6162094672: get_thread_connection: sess: 01eee566-f247-135d-9fb1-4c505a6cbedc, name: model.default.credit_card_type, idle: 0.09385013580322266s, acqrelcnt: 1, lang: sql, thrd: (88711, 11433504768), cmpt: ``, lut: 1710794082.2597299
[0m17:34:42.354075 [debug] [Thread-1 (]: Databricks adapter: conn: 6162094672: idle check connection: sess: 01eee566-f247-135d-9fb1-4c505a6cbedc, name: model.default.credit_card_type, idle: 0.09427213668823242s, acqrelcnt: 1, lang: sql, thrd: (88711, 11433504768), cmpt: ``, lut: 1710794082.2597299
[0m17:34:42.354221 [debug] [Thread-1 (]: Using databricks connection "model.default.credit_card_type"
[0m17:34:42.354410 [debug] [Thread-1 (]: On model.default.credit_card_type: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "node_id": "model.default.credit_card_type"} */

  
    
        create or replace table `hive_metastore`.`default`.`credit_card_type`
      
      
    using delta
      
      
      
      
      
      
      
      as
      

WITH payments AS (
    SELECT *
    FROM `hive_metastore`.`default`.`stage_payments`
)
SELECT country AS issued_at,
       credit_card_type AS credit_card_type,
       COUNT(*)
FROM payments AS p
GROUP BY country, credit_card_type
  
[0m17:34:43.067560 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "node_id": "model.default.credit_card_type"} */

  
    
        create or replace table `hive_metastore`.`default`.`credit_card_type`
      
      
    using delta
      
      
      
      
      
      
      
      as
      

WITH payments AS (
    SELECT *
    FROM `hive_metastore`.`default`.`stage_payments`
)
SELECT country AS issued_at,
       credit_card_type AS credit_card_type,
       COUNT(*)
FROM payments AS p
GROUP BY country, credit_card_type
  
[0m17:34:43.068690 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m17:34:43.070419 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_INVALID_CHARACTERS_IN_COLUMN_NAMES] com.databricks.sql.transaction.tahoe.DeltaAnalysisException: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:697)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:41)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:99)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:574)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:423)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:420)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:70)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:170)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:603)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:612)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:491)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:489)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:65)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$8(ThriftLocalProperties.scala:161)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:160)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:51)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:65)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:401)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:386)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:435)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: com.databricks.sql.transaction.tahoe.DeltaAnalysisException: Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.foundInvalidCharsInColumnNames(DeltaErrors.scala:2091)
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.foundInvalidCharsInColumnNames$(DeltaErrors.scala:2090)
	at com.databricks.sql.transaction.tahoe.DeltaErrors$.foundInvalidCharsInColumnNames(DeltaErrors.scala:3145)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkSchemaFieldNames(SchemaUtils.scala:1169)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.assertMetadata(OptimisticTransaction.scala:666)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.assertMetadata$(OptimisticTransaction.scala:661)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.assertMetadata(OptimisticTransaction.scala:157)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataInternal(OptimisticTransaction.scala:631)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataInternal$(OptimisticTransaction.scala:441)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadataInternal(OptimisticTransaction.scala:157)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadata(OptimisticTransaction.scala:417)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadata$(OptimisticTransaction.scala:410)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadata(OptimisticTransaction.scala:157)
	at com.databricks.sql.transaction.tahoe.schema.ImplicitMetadataOperation.updateMetadata(ImplicitMetadataOperation.scala:108)
	at com.databricks.sql.transaction.tahoe.schema.ImplicitMetadataOperation.updateMetadata$(ImplicitMetadataOperation.scala:60)
	at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.updateMetadata(WriteIntoDelta.scala:83)
	at com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.write(WriteIntoDelta.scala:284)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.doDeltaWrite$1(CreateDeltaTableCommand.scala:329)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCreateTableAsSelect(CreateDeltaTableCommand.scala:365)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCommit(CreateDeltaTableCommand.scala:200)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:147)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:197)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:184)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:67)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:161)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:266)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:264)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:67)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:160)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:668)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:686)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:663)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:25)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:66)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:148)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:68)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:55)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:107)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:429)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:408)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:67)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:159)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:149)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:139)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:67)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:133)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:326)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:266)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:264)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:106)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:147)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:1115)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:266)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:264)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:106)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:1074)
	at org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$2(WriteToDataSourceV2Exec.scala:652)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1621)
	at org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:639)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:657)
	at org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:633)
	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:209)
	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:267)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:47)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:54)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:301)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:301)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:322)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:589)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:216)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1153)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:156)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:531)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:300)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:274)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:295)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:280)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:320)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:316)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:280)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:376)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:280)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:232)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:229)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:262)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:515)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:537)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:610)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:606)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:610)
	... 36 more
Caused by: org.apache.spark.sql.AnalysisException: [INVALID_COLUMN_NAME_AS_PATH] The datasource delta cannot save the column `count(1)` because its name contains some characters that are not allowed in file paths. Please, use an alias to rename it.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.invalidColumnNameAsPathError(QueryCompilationErrors.scala:3252)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.$anonfun$checkFieldNames$1(SchemaUtils.scala:1182)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.$anonfun$checkFieldNames$1$adapted(SchemaUtils.scala:1179)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkFieldNames(SchemaUtils.scala:1179)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkSchemaFieldNames(SchemaUtils.scala:1166)
	... 151 more

[0m17:34:43.072525 [debug] [Thread-1 (]: Databricks adapter: operation-id: 01eee566-f32f-17b7-8510-9f1f971dabc6
[0m17:34:43.073494 [debug] [Thread-1 (]: Timing info for model.default.credit_card_type (execute): 17:34:42.310355 => 17:34:43.073120
[0m17:34:43.074069 [debug] [Thread-1 (]: Databricks adapter: conn: 6162094672: _release sess: 01eee566-f247-135d-9fb1-4c505a6cbedc, name: model.default.credit_card_type, idle: 6.198883056640625e-06s, acqrelcnt: 0, lang: sql, thrd: (88711, 11433504768), cmpt: ``, lut: 1710794083.073865
[0m17:34:43.102325 [debug] [Thread-1 (]: Runtime Error in model credit_card_type (models/trusted/credit_card_type.sql)
  Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m17:34:43.102977 [debug] [Thread-1 (]: Databricks adapter: conn: 6162094672: _release sess: 01eee566-f247-135d-9fb1-4c505a6cbedc, name: model.default.credit_card_type, idle: 3.0994415283203125e-06s, acqrelcnt: 0, lang: sql, thrd: (88711, 11433504768), cmpt: ``, lut: 1710794083.1028
[0m17:34:43.103419 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '87b4c0f7-87e5-4eb4-af37-d1e304f29faf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16f4d9650>]}
[0m17:34:43.104068 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model default.credit_card_type ................. [[31mERROR[0m in 0.83s]
[0m17:34:43.104615 [debug] [Thread-1 (]: Finished running node model.default.credit_card_type
[0m17:34:43.106008 [debug] [MainThread]: Databricks adapter: conn: 6161506320: idle check connection: sess: None, name: master, idle: 0.8391590118408203s, acqrelcnt: 0, lang: None, thrd: (88711, 7965269056), cmpt: ``, lut: 1710794082.266731
[0m17:34:43.106369 [debug] [MainThread]: Databricks adapter: conn: 6161506320: reusing connection master sess: None, name: master, idle: 0.8395540714263916s, acqrelcnt: 0, lang: None, thrd: (88711, 7965269056), cmpt: ``, lut: 1710794082.266731
[0m17:34:43.106626 [debug] [MainThread]: Databricks adapter: Thread (88711, 7965269056) using default compute resource.
[0m17:34:43.106871 [debug] [MainThread]: Databricks adapter: conn: 6161506320: _acquire sess: None, name: master, idle: 0.8400659561157227s, acqrelcnt: 1, lang: None, thrd: (88711, 7965269056), cmpt: ``, lut: 1710794082.266731
[0m17:34:43.107151 [debug] [MainThread]: Databricks adapter: conn: 6161506320: get_thread_connection: sess: None, name: master, idle: 0.8403439521789551s, acqrelcnt: 1, lang: None, thrd: (88711, 7965269056), cmpt: ``, lut: 1710794082.266731
[0m17:34:43.107397 [debug] [MainThread]: Databricks adapter: conn: 6161506320: idle check connection: sess: None, name: master, idle: 0.8405938148498535s, acqrelcnt: 1, lang: None, thrd: (88711, 7965269056), cmpt: ``, lut: 1710794082.266731
[0m17:34:43.107623 [debug] [MainThread]: On master: ROLLBACK
[0m17:34:43.107855 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:34:43.589545 [debug] [MainThread]: Databricks adapter: conn: 6161506320: session opened sess: 01eee566-f3d0-12bb-815a-69ddca5b5eb6, name: master, idle: 8.821487426757812e-06s, acqrelcnt: 1, lang: None, thrd: (88711, 7965269056), cmpt: ``, lut: 1710794083.589183
[0m17:34:43.590993 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:34:43.591693 [debug] [MainThread]: Databricks adapter: conn: 6161506320: get_thread_connection: sess: 01eee566-f3d0-12bb-815a-69ddca5b5eb6, name: master, idle: 0.002307891845703125s, acqrelcnt: 1, lang: None, thrd: (88711, 7965269056), cmpt: ``, lut: 1710794083.589183
[0m17:34:43.592204 [debug] [MainThread]: Databricks adapter: conn: 6161506320: idle check connection: sess: 01eee566-f3d0-12bb-815a-69ddca5b5eb6, name: master, idle: 0.0028929710388183594s, acqrelcnt: 1, lang: None, thrd: (88711, 7965269056), cmpt: ``, lut: 1710794083.589183
[0m17:34:43.592596 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:34:43.592994 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:34:43.593522 [debug] [MainThread]: Databricks adapter: conn: 6161506320: _release sess: 01eee566-f3d0-12bb-815a-69ddca5b5eb6, name: master, idle: 3.814697265625e-06s, acqrelcnt: 0, lang: None, thrd: (88711, 7965269056), cmpt: ``, lut: 1710794083.5933912
[0m17:34:43.595152 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:34:43.595866 [debug] [MainThread]: On master: ROLLBACK
[0m17:34:43.596390 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:34:43.597071 [debug] [MainThread]: On master: Close
[0m17:34:43.769371 [debug] [MainThread]: Connection 'model.default.credit_card_type' was properly closed.
[0m17:34:43.770428 [debug] [MainThread]: On model.default.credit_card_type: ROLLBACK
[0m17:34:43.771492 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:34:43.772158 [debug] [MainThread]: On model.default.credit_card_type: Close
[0m17:34:43.958951 [info ] [MainThread]: 
[0m17:34:43.960403 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 3.43 seconds (3.43s).
[0m17:34:43.961846 [debug] [MainThread]: Command end result
[0m17:34:44.017458 [info ] [MainThread]: 
[0m17:34:44.018038 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m17:34:44.018308 [info ] [MainThread]: 
[0m17:34:44.018565 [error] [MainThread]:   Runtime Error in model credit_card_type (models/trusted/credit_card_type.sql)
  Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema. Please use other characters and try again.
[0m17:34:44.018810 [info ] [MainThread]: 
[0m17:34:44.019097 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m17:34:44.024815 [debug] [MainThread]: Resource report: {"command_name": "run", "command_wall_clock_time": 5.0266457, "process_user_time": 2.187431, "process_kernel_time": 3.511648, "process_mem_max_rss": "221364224", "command_success": false, "process_in_blocks": "0", "process_out_blocks": "0"}
[0m17:34:44.025308 [debug] [MainThread]: Command `cli run` failed at 17:34:44.025204 after 5.03 seconds
[0m17:34:44.025699 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109fcf010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109ffd010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102fbfcd0>]}
[0m17:34:44.026024 [debug] [MainThread]: Flushing usage events
[0m17:35:07.534489 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123645c90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1236c1a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1236c2010>]}


============================== 17:35:07.537521 | db3b23a9-5c79-491e-97fb-70b671d5e27b ==============================
[0m17:35:07.537521 [info ] [MainThread]: Running with dbt=1.7.8
[0m17:35:07.537837 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m17:35:08.788504 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'db3b23a9-5c79-491e-97fb-70b671d5e27b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123633ed0>]}
[0m17:35:08.818809 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'db3b23a9-5c79-491e-97fb-70b671d5e27b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x2a23a7c10>]}
[0m17:35:08.819126 [info ] [MainThread]: Registered adapter: databricks=1.7.9
[0m17:35:08.834567 [debug] [MainThread]: checksum: 67f0013ca5f0bd43af9a0873dd50792fde83ef69de63b71cacd0b4ac656c52e5, vars: {}, profile: , target: , version: 1.7.8
[0m17:35:08.913151 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:35:08.913417 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:35:08.916307 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'db3b23a9-5c79-491e-97fb-70b671d5e27b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123633ed0>]}
[0m17:35:08.922816 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'db3b23a9-5c79-491e-97fb-70b671d5e27b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x2a245a090>]}
[0m17:35:08.923098 [info ] [MainThread]: Found 4 models, 3 sources, 0 exposures, 0 metrics, 539 macros, 0 groups, 0 semantic models
[0m17:35:08.923276 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'db3b23a9-5c79-491e-97fb-70b671d5e27b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x2a252ebd0>]}
[0m17:35:08.924009 [info ] [MainThread]: 
[0m17:35:08.924480 [debug] [MainThread]: Databricks adapter: conn: 11311207952: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (88795, 7965269056), cmpt: ``, lut: None
[0m17:35:08.924624 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m17:35:08.924748 [debug] [MainThread]: Databricks adapter: Thread (88795, 7965269056) using default compute resource.
[0m17:35:08.924870 [debug] [MainThread]: Databricks adapter: conn: 11311207952: _acquire sess: None, name: master, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (88795, 7965269056), cmpt: ``, lut: 1710794108.92483
[0m17:35:08.925403 [debug] [ThreadPool]: Databricks adapter: conn: 11313747664: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore, idle: 0s, acqrelcnt: 0, lang: None, thrd: (88795, 11386335232), cmpt: ``, lut: None
[0m17:35:08.925591 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m17:35:08.925718 [debug] [ThreadPool]: Databricks adapter: Thread (88795, 11386335232) using default compute resource.
[0m17:35:08.925844 [debug] [ThreadPool]: Databricks adapter: conn: 11313747664: _acquire sess: None, name: list_hive_metastore, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (88795, 11386335232), cmpt: ``, lut: 1710794108.925805
[0m17:35:08.925989 [debug] [ThreadPool]: Databricks adapter: conn: 11313747664: get_thread_connection: sess: None, name: list_hive_metastore, idle: 0.00014591217041015625s, acqrelcnt: 1, lang: None, thrd: (88795, 11386335232), cmpt: ``, lut: 1710794108.925805
[0m17:35:08.926123 [debug] [ThreadPool]: Databricks adapter: conn: 11313747664: idle check connection: sess: None, name: list_hive_metastore, idle: 0.0002739429473876953s, acqrelcnt: 1, lang: None, thrd: (88795, 11386335232), cmpt: ``, lut: 1710794108.925805
[0m17:35:08.926243 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m17:35:08.926368 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m17:35:08.926488 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:35:09.443482 [debug] [ThreadPool]: Databricks adapter: conn: 11313747664: session opened sess: 01eee567-0338-1e5b-ad7c-e568fe454432, name: list_hive_metastore, idle: 1.71661376953125e-05s, acqrelcnt: 1, lang: None, thrd: (88795, 11386335232), cmpt: ``, lut: 1710794109.443081
[0m17:35:09.745986 [debug] [ThreadPool]: SQL status: OK in 0.8199999928474426 seconds
[0m17:35:09.749669 [debug] [ThreadPool]: Databricks adapter: conn: 11313747664: _release sess: 01eee567-0338-1e5b-ad7c-e568fe454432, name: list_hive_metastore, idle: 6.198883056640625e-06s, acqrelcnt: 0, lang: None, thrd: (88795, 11386335232), cmpt: ``, lut: 1710794109.749522
[0m17:35:09.751536 [debug] [ThreadPool]: Databricks adapter: conn: 11313747664: idle check connection: sess: 01eee567-0338-1e5b-ad7c-e568fe454432, name: list_hive_metastore, idle: 0.001867055892944336s, acqrelcnt: 0, lang: None, thrd: (88795, 11386335232), cmpt: ``, lut: 1710794109.749522
[0m17:35:09.751999 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now list_hive_metastore_default)
[0m17:35:09.752319 [debug] [ThreadPool]: Databricks adapter: conn: 11313747664: reusing connection list_hive_metastore sess: 01eee567-0338-1e5b-ad7c-e568fe454432, name: list_hive_metastore_default, idle: 0.0027120113372802734s, acqrelcnt: 0, lang: None, thrd: (88795, 11386335232), cmpt: ``, lut: 1710794109.749522
[0m17:35:09.752570 [debug] [ThreadPool]: Databricks adapter: Thread (88795, 11386335232) using default compute resource.
[0m17:35:09.752816 [debug] [ThreadPool]: Databricks adapter: conn: 11313747664: _acquire sess: 01eee567-0338-1e5b-ad7c-e568fe454432, name: list_hive_metastore_default, idle: 0.0032198429107666016s, acqrelcnt: 1, lang: None, thrd: (88795, 11386335232), cmpt: ``, lut: 1710794109.749522
[0m17:35:09.757037 [debug] [ThreadPool]: Databricks adapter: conn: 11313747664: get_thread_connection: sess: 01eee567-0338-1e5b-ad7c-e568fe454432, name: list_hive_metastore_default, idle: 0.00740814208984375s, acqrelcnt: 1, lang: None, thrd: (88795, 11386335232), cmpt: ``, lut: 1710794109.749522
[0m17:35:09.757347 [debug] [ThreadPool]: Databricks adapter: conn: 11313747664: idle check connection: sess: 01eee567-0338-1e5b-ad7c-e568fe454432, name: list_hive_metastore_default, idle: 0.00774383544921875s, acqrelcnt: 1, lang: None, thrd: (88795, 11386335232), cmpt: ``, lut: 1710794109.749522
[0m17:35:09.757587 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:35:09.757815 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m17:35:10.070812 [debug] [ThreadPool]: SQL status: OK in 0.3100000023841858 seconds
[0m17:35:10.082593 [debug] [ThreadPool]: Databricks adapter: conn: 11313747664: get_thread_connection: sess: 01eee567-0338-1e5b-ad7c-e568fe454432, name: list_hive_metastore_default, idle: 0.33280515670776367s, acqrelcnt: 1, lang: None, thrd: (88795, 11386335232), cmpt: ``, lut: 1710794109.749522
[0m17:35:10.083101 [debug] [ThreadPool]: Databricks adapter: conn: 11313747664: idle check connection: sess: 01eee567-0338-1e5b-ad7c-e568fe454432, name: list_hive_metastore_default, idle: 0.3334639072418213s, acqrelcnt: 1, lang: None, thrd: (88795, 11386335232), cmpt: ``, lut: 1710794109.749522
[0m17:35:10.083371 [debug] [ThreadPool]: Databricks adapter: conn: 11313747664: get_thread_connection: sess: 01eee567-0338-1e5b-ad7c-e568fe454432, name: list_hive_metastore_default, idle: 0.33377599716186523s, acqrelcnt: 1, lang: None, thrd: (88795, 11386335232), cmpt: ``, lut: 1710794109.749522
[0m17:35:10.083605 [debug] [ThreadPool]: Databricks adapter: conn: 11313747664: idle check connection: sess: 01eee567-0338-1e5b-ad7c-e568fe454432, name: list_hive_metastore_default, idle: 0.3340120315551758s, acqrelcnt: 1, lang: None, thrd: (88795, 11386335232), cmpt: ``, lut: 1710794109.749522
[0m17:35:10.083827 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m17:35:10.084025 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:35:10.084267 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m17:35:10.333748 [debug] [ThreadPool]: SQL status: OK in 0.25 seconds
[0m17:35:10.345639 [debug] [ThreadPool]: Databricks adapter: conn: 11313747664: get_thread_connection: sess: 01eee567-0338-1e5b-ad7c-e568fe454432, name: list_hive_metastore_default, idle: 0.5959641933441162s, acqrelcnt: 1, lang: None, thrd: (88795, 11386335232), cmpt: ``, lut: 1710794109.749522
[0m17:35:10.346077 [debug] [ThreadPool]: Databricks adapter: conn: 11313747664: idle check connection: sess: 01eee567-0338-1e5b-ad7c-e568fe454432, name: list_hive_metastore_default, idle: 0.5964710712432861s, acqrelcnt: 1, lang: None, thrd: (88795, 11386335232), cmpt: ``, lut: 1710794109.749522
[0m17:35:10.346331 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m17:35:10.346595 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m17:35:10.667082 [debug] [ThreadPool]: SQL status: OK in 0.3199999928474426 seconds
[0m17:35:10.672276 [debug] [ThreadPool]: Databricks adapter: conn: 11313747664: _release sess: 01eee567-0338-1e5b-ad7c-e568fe454432, name: list_hive_metastore_default, idle: 2.6226043701171875e-06s, acqrelcnt: 0, lang: None, thrd: (88795, 11386335232), cmpt: ``, lut: 1710794110.6721332
[0m17:35:10.675708 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'db3b23a9-5c79-491e-97fb-70b671d5e27b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x2a2459710>]}
[0m17:35:10.676247 [debug] [MainThread]: Databricks adapter: conn: 11311207952: get_thread_connection: sess: None, name: master, idle: 1.7513179779052734s, acqrelcnt: 1, lang: None, thrd: (88795, 7965269056), cmpt: ``, lut: 1710794108.92483
[0m17:35:10.676544 [debug] [MainThread]: Databricks adapter: conn: 11311207952: idle check connection: sess: None, name: master, idle: 1.7516348361968994s, acqrelcnt: 1, lang: None, thrd: (88795, 7965269056), cmpt: ``, lut: 1710794108.92483
[0m17:35:10.676821 [debug] [MainThread]: Databricks adapter: conn: 11311207952: get_thread_connection: sess: None, name: master, idle: 1.7519118785858154s, acqrelcnt: 1, lang: None, thrd: (88795, 7965269056), cmpt: ``, lut: 1710794108.92483
[0m17:35:10.677077 [debug] [MainThread]: Databricks adapter: conn: 11311207952: idle check connection: sess: None, name: master, idle: 1.7521729469299316s, acqrelcnt: 1, lang: None, thrd: (88795, 7965269056), cmpt: ``, lut: 1710794108.92483
[0m17:35:10.677323 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:35:10.677547 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:35:10.677795 [debug] [MainThread]: Databricks adapter: conn: 11311207952: _release sess: None, name: master, idle: 9.5367431640625e-07s, acqrelcnt: 0, lang: None, thrd: (88795, 7965269056), cmpt: ``, lut: 1710794110.67772
[0m17:35:10.678373 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:35:10.678659 [info ] [MainThread]: 
[0m17:35:10.683552 [debug] [Thread-1 (]: Began running node model.default.credit_card_type
[0m17:35:10.684028 [info ] [Thread-1 (]: 1 of 1 START sql table model default.credit_card_type .......................... [RUN]
[0m17:35:10.684759 [debug] [Thread-1 (]: Databricks adapter: conn: 11313747664: idle check connection: sess: 01eee567-0338-1e5b-ad7c-e568fe454432, name: list_hive_metastore_default, idle: 0.01248788833618164s, acqrelcnt: 0, lang: None, thrd: (88795, 11386335232), cmpt: ``, lut: 1710794110.6721332
[0m17:35:10.685027 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.default.credit_card_type)
[0m17:35:10.685290 [debug] [Thread-1 (]: Databricks adapter: conn: 11313747664: reusing connection list_hive_metastore_default sess: 01eee567-0338-1e5b-ad7c-e568fe454432, name: model.default.credit_card_type, idle: 0.013056755065917969s, acqrelcnt: 0, lang: None, thrd: (88795, 11386335232), cmpt: ``, lut: 1710794110.6721332
[0m17:35:10.685540 [debug] [Thread-1 (]: Databricks adapter: On thread (88795, 11386335232): `hive_metastore`.`default`.`credit_card_type` using default compute resource.
[0m17:35:10.685779 [debug] [Thread-1 (]: Databricks adapter: conn: 11313747664: _acquire sess: 01eee567-0338-1e5b-ad7c-e568fe454432, name: model.default.credit_card_type, idle: 0.013550758361816406s, acqrelcnt: 1, lang: sql, thrd: (88795, 11386335232), cmpt: ``, lut: 1710794110.6721332
[0m17:35:10.686028 [debug] [Thread-1 (]: Began compiling node model.default.credit_card_type
[0m17:35:10.691565 [debug] [Thread-1 (]: Writing injected SQL for node "model.default.credit_card_type"
[0m17:35:10.693143 [debug] [Thread-1 (]: Timing info for model.default.credit_card_type (compile): 17:35:10.686185 => 17:35:10.692940
[0m17:35:10.693433 [debug] [Thread-1 (]: Began executing node model.default.credit_card_type
[0m17:35:10.724602 [debug] [Thread-1 (]: Writing runtime sql for node "model.default.credit_card_type"
[0m17:35:10.727786 [debug] [Thread-1 (]: Databricks adapter: conn: 11313747664: get_thread_connection: sess: 01eee567-0338-1e5b-ad7c-e568fe454432, name: model.default.credit_card_type, idle: 0.0555577278137207s, acqrelcnt: 1, lang: sql, thrd: (88795, 11386335232), cmpt: ``, lut: 1710794110.6721332
[0m17:35:10.727994 [debug] [Thread-1 (]: Databricks adapter: conn: 11313747664: idle check connection: sess: 01eee567-0338-1e5b-ad7c-e568fe454432, name: model.default.credit_card_type, idle: 0.05578875541687012s, acqrelcnt: 1, lang: sql, thrd: (88795, 11386335232), cmpt: ``, lut: 1710794110.6721332
[0m17:35:10.728142 [debug] [Thread-1 (]: Using databricks connection "model.default.credit_card_type"
[0m17:35:10.728350 [debug] [Thread-1 (]: On model.default.credit_card_type: /* {"app": "dbt", "dbt_version": "1.7.8", "dbt_databricks_version": "1.7.9", "databricks_sql_connector_version": "2.9.4", "profile_name": "default", "target_name": "dev", "node_id": "model.default.credit_card_type"} */

  
    
        create or replace table `hive_metastore`.`default`.`credit_card_type`
      
      
    using delta
      
      
      
      
      
      
      
      as
      

WITH payments AS (
    SELECT *
    FROM `hive_metastore`.`default`.`stage_payments`
)
SELECT country AS issued_at,
       credit_card_type AS credit_card_type,
       COUNT(*) AS total_count
FROM payments AS p
GROUP BY country, credit_card_type
  
[0m17:35:17.690548 [debug] [Thread-1 (]: SQL status: OK in 6.960000038146973 seconds
[0m17:35:17.905633 [debug] [Thread-1 (]: Timing info for model.default.credit_card_type (execute): 17:35:10.693580 => 17:35:17.905442
[0m17:35:17.906142 [debug] [Thread-1 (]: Databricks adapter: conn: 11313747664: _release sess: 01eee567-0338-1e5b-ad7c-e568fe454432, name: model.default.credit_card_type, idle: 3.0994415283203125e-06s, acqrelcnt: 0, lang: sql, thrd: (88795, 11386335232), cmpt: ``, lut: 1710794117.906005
[0m17:35:17.906668 [debug] [Thread-1 (]: Databricks adapter: conn: 11313747664: _release sess: 01eee567-0338-1e5b-ad7c-e568fe454432, name: model.default.credit_card_type, idle: 0.0s, acqrelcnt: 0, lang: sql, thrd: (88795, 11386335232), cmpt: ``, lut: 1710794117.906566
[0m17:35:17.906972 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'db3b23a9-5c79-491e-97fb-70b671d5e27b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16f3db4d0>]}
[0m17:35:17.907398 [info ] [Thread-1 (]: 1 of 1 OK created sql table model default.credit_card_type ..................... [[32mOK[0m in 7.22s]
[0m17:35:17.907755 [debug] [Thread-1 (]: Finished running node model.default.credit_card_type
[0m17:35:17.908888 [debug] [MainThread]: Databricks adapter: conn: 11311207952: idle check connection: sess: None, name: master, idle: 7.23106575012207s, acqrelcnt: 0, lang: None, thrd: (88795, 7965269056), cmpt: ``, lut: 1710794110.67772
[0m17:35:17.909206 [debug] [MainThread]: Databricks adapter: conn: 11311207952: reusing connection master sess: None, name: master, idle: 7.2314229011535645s, acqrelcnt: 0, lang: None, thrd: (88795, 7965269056), cmpt: ``, lut: 1710794110.67772
[0m17:35:17.909401 [debug] [MainThread]: Databricks adapter: Thread (88795, 7965269056) using default compute resource.
[0m17:35:17.909580 [debug] [MainThread]: Databricks adapter: conn: 11311207952: _acquire sess: None, name: master, idle: 7.231806039810181s, acqrelcnt: 1, lang: None, thrd: (88795, 7965269056), cmpt: ``, lut: 1710794110.67772
[0m17:35:17.909782 [debug] [MainThread]: Databricks adapter: conn: 11311207952: get_thread_connection: sess: None, name: master, idle: 7.23200798034668s, acqrelcnt: 1, lang: None, thrd: (88795, 7965269056), cmpt: ``, lut: 1710794110.67772
[0m17:35:17.909959 [debug] [MainThread]: Databricks adapter: conn: 11311207952: idle check connection: sess: None, name: master, idle: 7.232188940048218s, acqrelcnt: 1, lang: None, thrd: (88795, 7965269056), cmpt: ``, lut: 1710794110.67772
[0m17:35:17.910130 [debug] [MainThread]: On master: ROLLBACK
[0m17:35:17.910300 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:35:18.369815 [debug] [MainThread]: Databricks adapter: conn: 11311207952: session opened sess: 01eee567-088b-14e0-8fa6-5c615c9d41f6, name: master, idle: 1.1920928955078125e-05s, acqrelcnt: 1, lang: None, thrd: (88795, 7965269056), cmpt: ``, lut: 1710794118.3693042
[0m17:35:18.371233 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:35:18.372054 [debug] [MainThread]: Databricks adapter: conn: 11311207952: get_thread_connection: sess: 01eee567-088b-14e0-8fa6-5c615c9d41f6, name: master, idle: 0.002569913864135742s, acqrelcnt: 1, lang: None, thrd: (88795, 7965269056), cmpt: ``, lut: 1710794118.3693042
[0m17:35:18.372736 [debug] [MainThread]: Databricks adapter: conn: 11311207952: idle check connection: sess: 01eee567-088b-14e0-8fa6-5c615c9d41f6, name: master, idle: 0.0033006668090820312s, acqrelcnt: 1, lang: None, thrd: (88795, 7965269056), cmpt: ``, lut: 1710794118.3693042
[0m17:35:18.373619 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m17:35:18.374159 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m17:35:18.374942 [debug] [MainThread]: Databricks adapter: conn: 11311207952: _release sess: 01eee567-088b-14e0-8fa6-5c615c9d41f6, name: master, idle: 4.0531158447265625e-06s, acqrelcnt: 0, lang: None, thrd: (88795, 7965269056), cmpt: ``, lut: 1710794118.3747258
[0m17:35:18.376776 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:35:18.377484 [debug] [MainThread]: On master: ROLLBACK
[0m17:35:18.377967 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:35:18.378365 [debug] [MainThread]: On master: Close
[0m17:35:18.534308 [debug] [MainThread]: Connection 'model.default.credit_card_type' was properly closed.
[0m17:35:18.535520 [debug] [MainThread]: On model.default.credit_card_type: ROLLBACK
[0m17:35:18.536332 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m17:35:18.537131 [debug] [MainThread]: On model.default.credit_card_type: Close
[0m17:35:18.717764 [info ] [MainThread]: 
[0m17:35:18.718880 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 9.79 seconds (9.79s).
[0m17:35:18.720516 [debug] [MainThread]: Command end result
[0m17:35:18.735819 [info ] [MainThread]: 
[0m17:35:18.736321 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:35:18.736593 [info ] [MainThread]: 
[0m17:35:18.736876 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m17:35:18.742287 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 11.239088, "process_user_time": 2.241655, "process_kernel_time": 3.424661, "process_mem_max_rss": "221528064", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m17:35:18.742829 [debug] [MainThread]: Command `cli run` succeeded at 17:35:18.742713 after 11.24 seconds
[0m17:35:18.743251 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1236c3ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x101a87c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121cda610>]}
[0m17:35:18.743586 [debug] [MainThread]: Flushing usage events
[0m19:47:34.551035 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f18af0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110d18280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110d74d60>]}


============================== 19:47:34.560385 | e5fcfc99-1c4e-42cf-81b9-363764cf5392 ==============================
[0m19:47:34.560385 [info ] [MainThread]: Running with dbt=1.7.7
[0m19:47:34.560791 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --select trusted_rides_km_real.sql', 'send_anonymous_usage_stats': 'True'}
[0m19:47:40.902076 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e5fcfc99-1c4e-42cf-81b9-363764cf5392', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110d09f10>]}
[0m19:47:40.943842 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e5fcfc99-1c4e-42cf-81b9-363764cf5392', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110d09ee0>]}
[0m19:47:40.944167 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m19:47:40.961521 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m19:47:40.971052 [info ] [MainThread]: Unable to do partial parsing because of a version mismatch
[0m19:47:40.971265 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'e5fcfc99-1c4e-42cf-81b9-363764cf5392', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1367e7790>]}
[0m19:47:42.517116 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e5fcfc99-1c4e-42cf-81b9-363764cf5392', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1370ba0d0>]}
[0m19:47:42.528581 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e5fcfc99-1c4e-42cf-81b9-363764cf5392', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1370ba070>]}
[0m19:47:42.528834 [info ] [MainThread]: Found 7 models, 3 sources, 0 exposures, 0 metrics, 537 macros, 0 groups, 0 semantic models
[0m19:47:42.529016 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e5fcfc99-1c4e-42cf-81b9-363764cf5392', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x137d02f70>]}
[0m19:47:42.529735 [info ] [MainThread]: 
[0m19:47:42.530160 [debug] [MainThread]: Databricks adapter: conn: 5218054000: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (68873, 7965269056), cmpt: ``, lut: None
[0m19:47:42.530308 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:47:42.530444 [debug] [MainThread]: Databricks adapter: Thread (68873, 7965269056) using default compute resource.
[0m19:47:42.530596 [debug] [MainThread]: Databricks adapter: conn: 5218054000: _acquire sess: None, name: master, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (68873, 7965269056), cmpt: ``, lut: 1710888462.530549
[0m19:47:42.531130 [debug] [ThreadPool]: Databricks adapter: conn: 4417679808: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore, idle: 0s, acqrelcnt: 0, lang: None, thrd: (68873, 6437613568), cmpt: ``, lut: None
[0m19:47:42.531337 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m19:47:42.531492 [debug] [ThreadPool]: Databricks adapter: Thread (68873, 6437613568) using default compute resource.
[0m19:47:42.531645 [debug] [ThreadPool]: Databricks adapter: conn: 4417679808: _acquire sess: None, name: list_hive_metastore, idle: 1.1920928955078125e-06s, acqrelcnt: 1, lang: None, thrd: (68873, 6437613568), cmpt: ``, lut: 1710888462.5315998
[0m19:47:42.531801 [debug] [ThreadPool]: Databricks adapter: conn: 4417679808: get_thread_connection: sess: None, name: list_hive_metastore, idle: 0.00015735626220703125s, acqrelcnt: 1, lang: None, thrd: (68873, 6437613568), cmpt: ``, lut: 1710888462.5315998
[0m19:47:42.532019 [debug] [ThreadPool]: Databricks adapter: conn: 4417679808: idle check connection: sess: None, name: list_hive_metastore, idle: 0.0003752708435058594s, acqrelcnt: 1, lang: None, thrd: (68873, 6437613568), cmpt: ``, lut: 1710888462.5315998
[0m19:47:42.532162 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m19:47:42.532311 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m19:47:42.532455 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:47:43.329998 [debug] [ThreadPool]: Databricks adapter: conn: 4417679808: session opened sess: 01eee642-b27f-1515-9637-b7406fb4f917, name: list_hive_metastore, idle: 1.5735626220703125e-05s, acqrelcnt: 1, lang: None, thrd: (68873, 6437613568), cmpt: ``, lut: 1710888463.32941
[0m19:47:58.657762 [debug] [ThreadPool]: SQL status: OK in 16.1299991607666 seconds
[0m19:47:58.665015 [debug] [ThreadPool]: Databricks adapter: conn: 4417679808: _release sess: 01eee642-b27f-1515-9637-b7406fb4f917, name: list_hive_metastore, idle: 8.106231689453125e-06s, acqrelcnt: 0, lang: None, thrd: (68873, 6437613568), cmpt: ``, lut: 1710888478.664839
[0m19:47:58.668420 [debug] [ThreadPool]: Databricks adapter: conn: 4417679808: idle check connection: sess: 01eee642-b27f-1515-9637-b7406fb4f917, name: list_hive_metastore, idle: 0.0033409595489501953s, acqrelcnt: 0, lang: None, thrd: (68873, 6437613568), cmpt: ``, lut: 1710888478.664839
[0m19:47:58.669136 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now list_hive_metastore_default)
[0m19:47:58.669604 [debug] [ThreadPool]: Databricks adapter: conn: 4417679808: reusing connection list_hive_metastore sess: 01eee642-b27f-1515-9637-b7406fb4f917, name: list_hive_metastore_default, idle: 0.00465083122253418s, acqrelcnt: 0, lang: None, thrd: (68873, 6437613568), cmpt: ``, lut: 1710888478.664839
[0m19:47:58.669950 [debug] [ThreadPool]: Databricks adapter: Thread (68873, 6437613568) using default compute resource.
[0m19:47:58.670296 [debug] [ThreadPool]: Databricks adapter: conn: 4417679808: _acquire sess: 01eee642-b27f-1515-9637-b7406fb4f917, name: list_hive_metastore_default, idle: 0.0053558349609375s, acqrelcnt: 1, lang: None, thrd: (68873, 6437613568), cmpt: ``, lut: 1710888478.664839
[0m19:47:58.675875 [debug] [ThreadPool]: Databricks adapter: conn: 4417679808: get_thread_connection: sess: 01eee642-b27f-1515-9637-b7406fb4f917, name: list_hive_metastore_default, idle: 0.010931968688964844s, acqrelcnt: 1, lang: None, thrd: (68873, 6437613568), cmpt: ``, lut: 1710888478.664839
[0m19:47:58.676178 [debug] [ThreadPool]: Databricks adapter: conn: 4417679808: idle check connection: sess: 01eee642-b27f-1515-9637-b7406fb4f917, name: list_hive_metastore_default, idle: 0.01125788688659668s, acqrelcnt: 1, lang: None, thrd: (68873, 6437613568), cmpt: ``, lut: 1710888478.664839
[0m19:47:58.676420 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m19:47:58.676667 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m19:47:59.106522 [debug] [ThreadPool]: SQL status: OK in 0.4300000071525574 seconds
[0m19:47:59.121962 [debug] [ThreadPool]: Databricks adapter: conn: 4417679808: get_thread_connection: sess: 01eee642-b27f-1515-9637-b7406fb4f917, name: list_hive_metastore_default, idle: 0.45694708824157715s, acqrelcnt: 1, lang: None, thrd: (68873, 6437613568), cmpt: ``, lut: 1710888478.664839
[0m19:47:59.122464 [debug] [ThreadPool]: Databricks adapter: conn: 4417679808: idle check connection: sess: 01eee642-b27f-1515-9637-b7406fb4f917, name: list_hive_metastore_default, idle: 0.4575378894805908s, acqrelcnt: 1, lang: None, thrd: (68873, 6437613568), cmpt: ``, lut: 1710888478.664839
[0m19:47:59.122751 [debug] [ThreadPool]: Databricks adapter: conn: 4417679808: get_thread_connection: sess: 01eee642-b27f-1515-9637-b7406fb4f917, name: list_hive_metastore_default, idle: 0.45782995223999023s, acqrelcnt: 1, lang: None, thrd: (68873, 6437613568), cmpt: ``, lut: 1710888478.664839
[0m19:47:59.123022 [debug] [ThreadPool]: Databricks adapter: conn: 4417679808: idle check connection: sess: 01eee642-b27f-1515-9637-b7406fb4f917, name: list_hive_metastore_default, idle: 0.45810604095458984s, acqrelcnt: 1, lang: None, thrd: (68873, 6437613568), cmpt: ``, lut: 1710888478.664839
[0m19:47:59.123271 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m19:47:59.123502 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m19:47:59.123781 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m19:48:00.328134 [debug] [ThreadPool]: SQL status: OK in 1.2000000476837158 seconds
[0m19:48:00.335346 [debug] [ThreadPool]: Databricks adapter: conn: 4417679808: get_thread_connection: sess: 01eee642-b27f-1515-9637-b7406fb4f917, name: list_hive_metastore_default, idle: 1.6703541278839111s, acqrelcnt: 1, lang: None, thrd: (68873, 6437613568), cmpt: ``, lut: 1710888478.664839
[0m19:48:00.335795 [debug] [ThreadPool]: Databricks adapter: conn: 4417679808: idle check connection: sess: 01eee642-b27f-1515-9637-b7406fb4f917, name: list_hive_metastore_default, idle: 1.6708569526672363s, acqrelcnt: 1, lang: None, thrd: (68873, 6437613568), cmpt: ``, lut: 1710888478.664839
[0m19:48:00.336092 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m19:48:00.336440 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m19:48:00.908931 [debug] [ThreadPool]: SQL status: OK in 0.5699999928474426 seconds
[0m19:48:00.915525 [debug] [ThreadPool]: Databricks adapter: conn: 4417679808: _release sess: 01eee642-b27f-1515-9637-b7406fb4f917, name: list_hive_metastore_default, idle: 1.9073486328125e-06s, acqrelcnt: 0, lang: None, thrd: (68873, 6437613568), cmpt: ``, lut: 1710888480.915353
[0m19:48:00.920595 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e5fcfc99-1c4e-42cf-81b9-363764cf5392', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1370ba5b0>]}
[0m19:48:00.921337 [debug] [MainThread]: Databricks adapter: conn: 5218054000: get_thread_connection: sess: None, name: master, idle: 18.390662908554077s, acqrelcnt: 1, lang: None, thrd: (68873, 7965269056), cmpt: ``, lut: 1710888462.530549
[0m19:48:00.921671 [debug] [MainThread]: Databricks adapter: conn: 5218054000: idle check connection: sess: None, name: master, idle: 18.391032934188843s, acqrelcnt: 1, lang: None, thrd: (68873, 7965269056), cmpt: ``, lut: 1710888462.530549
[0m19:48:00.921972 [debug] [MainThread]: Databricks adapter: conn: 5218054000: get_thread_connection: sess: None, name: master, idle: 18.391334772109985s, acqrelcnt: 1, lang: None, thrd: (68873, 7965269056), cmpt: ``, lut: 1710888462.530549
[0m19:48:00.922273 [debug] [MainThread]: Databricks adapter: conn: 5218054000: idle check connection: sess: None, name: master, idle: 18.391636848449707s, acqrelcnt: 1, lang: None, thrd: (68873, 7965269056), cmpt: ``, lut: 1710888462.530549
[0m19:48:00.922555 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m19:48:00.922826 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m19:48:00.923119 [debug] [MainThread]: Databricks adapter: conn: 5218054000: _release sess: None, name: master, idle: 1.9073486328125e-06s, acqrelcnt: 0, lang: None, thrd: (68873, 7965269056), cmpt: ``, lut: 1710888480.923031
[0m19:48:00.923894 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:48:00.924210 [info ] [MainThread]: 
[0m19:48:00.934281 [debug] [Thread-1  ]: Began running node model.default.trusted_rides_km_real
[0m19:48:00.934819 [info ] [Thread-1  ]: 1 of 1 START sql table model default.trusted_rides_km_real ..................... [RUN]
[0m19:48:00.935589 [debug] [Thread-1  ]: Databricks adapter: conn: 4417679808: idle check connection: sess: 01eee642-b27f-1515-9637-b7406fb4f917, name: list_hive_metastore_default, idle: 0.02008509635925293s, acqrelcnt: 0, lang: None, thrd: (68873, 6437613568), cmpt: ``, lut: 1710888480.915353
[0m19:48:00.935886 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.default.trusted_rides_km_real)
[0m19:48:00.936202 [debug] [Thread-1  ]: Databricks adapter: conn: 4417679808: reusing connection list_hive_metastore_default sess: 01eee642-b27f-1515-9637-b7406fb4f917, name: model.default.trusted_rides_km_real, idle: 0.02072596549987793s, acqrelcnt: 0, lang: None, thrd: (68873, 6437613568), cmpt: ``, lut: 1710888480.915353
[0m19:48:00.936508 [debug] [Thread-1  ]: Databricks adapter: On thread (68873, 6437613568): `hive_metastore`.`default`.`trusted_rides_km_real` using default compute resource.
[0m19:48:00.936824 [debug] [Thread-1  ]: Databricks adapter: conn: 4417679808: _acquire sess: 01eee642-b27f-1515-9637-b7406fb4f917, name: model.default.trusted_rides_km_real, idle: 0.021348953247070312s, acqrelcnt: 1, lang: sql, thrd: (68873, 6437613568), cmpt: ``, lut: 1710888480.915353
[0m19:48:00.937125 [debug] [Thread-1  ]: Began compiling node model.default.trusted_rides_km_real
[0m19:48:00.944901 [debug] [Thread-1  ]: Writing injected SQL for node "model.default.trusted_rides_km_real"
[0m19:48:00.945790 [debug] [Thread-1  ]: Timing info for model.default.trusted_rides_km_real (compile): 19:48:00.937315 => 19:48:00.945617
[0m19:48:00.946055 [debug] [Thread-1  ]: Began executing node model.default.trusted_rides_km_real
[0m19:48:00.956786 [debug] [Thread-1  ]: Databricks adapter: conn: 4417679808: get_thread_connection: sess: 01eee642-b27f-1515-9637-b7406fb4f917, name: model.default.trusted_rides_km_real, idle: 0.04131484031677246s, acqrelcnt: 1, lang: sql, thrd: (68873, 6437613568), cmpt: ``, lut: 1710888480.915353
[0m19:48:00.957056 [debug] [Thread-1  ]: Databricks adapter: conn: 4417679808: idle check connection: sess: 01eee642-b27f-1515-9637-b7406fb4f917, name: model.default.trusted_rides_km_real, idle: 0.04160594940185547s, acqrelcnt: 1, lang: sql, thrd: (68873, 6437613568), cmpt: ``, lut: 1710888480.915353
[0m19:48:00.957243 [debug] [Thread-1  ]: Using databricks connection "model.default.trusted_rides_km_real"
[0m19:48:00.957480 [debug] [Thread-1  ]: On model.default.trusted_rides_km_real: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "node_id": "model.default.trusted_rides_km_real"} */

      describe extended `hive_metastore`.`default`.`trusted_rides_km_real`
  
[0m19:48:01.782750 [debug] [Thread-1  ]: SQL status: OK in 0.8199999928474426 seconds
[0m19:48:01.840540 [debug] [Thread-1  ]: Writing runtime sql for node "model.default.trusted_rides_km_real"
[0m19:48:01.841879 [debug] [Thread-1  ]: Databricks adapter: conn: 4417679808: get_thread_connection: sess: 01eee642-b27f-1515-9637-b7406fb4f917, name: model.default.trusted_rides_km_real, idle: 0.9264037609100342s, acqrelcnt: 1, lang: sql, thrd: (68873, 6437613568), cmpt: ``, lut: 1710888480.915353
[0m19:48:01.842151 [debug] [Thread-1  ]: Databricks adapter: conn: 4417679808: idle check connection: sess: 01eee642-b27f-1515-9637-b7406fb4f917, name: model.default.trusted_rides_km_real, idle: 0.9266979694366455s, acqrelcnt: 1, lang: sql, thrd: (68873, 6437613568), cmpt: ``, lut: 1710888480.915353
[0m19:48:01.842345 [debug] [Thread-1  ]: Using databricks connection "model.default.trusted_rides_km_real"
[0m19:48:01.842604 [debug] [Thread-1  ]: On model.default.trusted_rides_km_real: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "node_id": "model.default.trusted_rides_km_real"} */

  
    
        create or replace table `hive_metastore`.`default`.`trusted_rides_km_real`
      
      
    using delta
      
      
      
      
      
      
      
      as
      

WITH trusted_transactions_per_cab_type AS (
    SELECT *
    FROM `hive_metastore`.`default`.`trusted_transactions_per_cab_type`
)
SELECT r.company_type AS company_type,
       r.cab_type AS cab_type,
       r.distance_in_miles AS distance_in_miles,
       0.0 AS distance_in_km,
       r.price_usd AS price_usd,
       '' AS price_real_brl
FROM trusted_transactions_per_cab_type AS r
  
[0m19:48:09.789659 [debug] [Thread-1  ]: SQL status: OK in 7.949999809265137 seconds
[0m19:48:10.097159 [debug] [Thread-1  ]: Timing info for model.default.trusted_rides_km_real (execute): 19:48:00.946215 => 19:48:10.096944
[0m19:48:10.097636 [debug] [Thread-1  ]: Databricks adapter: conn: 4417679808: _release sess: 01eee642-b27f-1515-9637-b7406fb4f917, name: model.default.trusted_rides_km_real, idle: 5.0067901611328125e-06s, acqrelcnt: 0, lang: sql, thrd: (68873, 6437613568), cmpt: ``, lut: 1710888490.097478
[0m19:48:10.098386 [debug] [Thread-1  ]: Databricks adapter: conn: 4417679808: _release sess: 01eee642-b27f-1515-9637-b7406fb4f917, name: model.default.trusted_rides_km_real, idle: 1.9073486328125e-06s, acqrelcnt: 0, lang: sql, thrd: (68873, 6437613568), cmpt: ``, lut: 1710888490.098283
[0m19:48:10.098751 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e5fcfc99-1c4e-42cf-81b9-363764cf5392', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x137e536a0>]}
[0m19:48:10.099233 [info ] [Thread-1  ]: 1 of 1 OK created sql table model default.trusted_rides_km_real ................ [[32mOK[0m in 9.16s]
[0m19:48:10.099606 [debug] [Thread-1  ]: Finished running node model.default.trusted_rides_km_real
[0m19:48:10.101120 [debug] [MainThread]: Databricks adapter: conn: 5218054000: idle check connection: sess: None, name: master, idle: 9.177963018417358s, acqrelcnt: 0, lang: None, thrd: (68873, 7965269056), cmpt: ``, lut: 1710888480.923031
[0m19:48:10.101506 [debug] [MainThread]: Databricks adapter: conn: 5218054000: reusing connection master sess: None, name: master, idle: 9.178408861160278s, acqrelcnt: 0, lang: None, thrd: (68873, 7965269056), cmpt: ``, lut: 1710888480.923031
[0m19:48:10.101708 [debug] [MainThread]: Databricks adapter: Thread (68873, 7965269056) using default compute resource.
[0m19:48:10.101912 [debug] [MainThread]: Databricks adapter: conn: 5218054000: _acquire sess: None, name: master, idle: 9.178821802139282s, acqrelcnt: 1, lang: None, thrd: (68873, 7965269056), cmpt: ``, lut: 1710888480.923031
[0m19:48:10.102136 [debug] [MainThread]: Databricks adapter: conn: 5218054000: get_thread_connection: sess: None, name: master, idle: 9.179044723510742s, acqrelcnt: 1, lang: None, thrd: (68873, 7965269056), cmpt: ``, lut: 1710888480.923031
[0m19:48:10.102335 [debug] [MainThread]: Databricks adapter: conn: 5218054000: idle check connection: sess: None, name: master, idle: 9.17924690246582s, acqrelcnt: 1, lang: None, thrd: (68873, 7965269056), cmpt: ``, lut: 1710888480.923031
[0m19:48:10.102563 [debug] [MainThread]: On master: ROLLBACK
[0m19:48:10.102806 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:48:10.709677 [debug] [MainThread]: Databricks adapter: conn: 5218054000: session opened sess: 01eee642-c2da-15c4-b54f-1d562016b4c2, name: master, idle: 1.6689300537109375e-05s, acqrelcnt: 1, lang: None, thrd: (68873, 7965269056), cmpt: ``, lut: 1710888490.7090392
[0m19:48:10.711101 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m19:48:10.712193 [debug] [MainThread]: Databricks adapter: conn: 5218054000: get_thread_connection: sess: 01eee642-c2da-15c4-b54f-1d562016b4c2, name: master, idle: 0.002869844436645508s, acqrelcnt: 1, lang: None, thrd: (68873, 7965269056), cmpt: ``, lut: 1710888490.7090392
[0m19:48:10.713660 [debug] [MainThread]: Databricks adapter: conn: 5218054000: idle check connection: sess: 01eee642-c2da-15c4-b54f-1d562016b4c2, name: master, idle: 0.004263877868652344s, acqrelcnt: 1, lang: None, thrd: (68873, 7965269056), cmpt: ``, lut: 1710888490.7090392
[0m19:48:10.714650 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m19:48:10.715483 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m19:48:10.716864 [debug] [MainThread]: Databricks adapter: conn: 5218054000: _release sess: 01eee642-c2da-15c4-b54f-1d562016b4c2, name: master, idle: 5.7220458984375e-06s, acqrelcnt: 0, lang: None, thrd: (68873, 7965269056), cmpt: ``, lut: 1710888490.7165902
[0m19:48:10.718770 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:48:10.719298 [debug] [MainThread]: On master: ROLLBACK
[0m19:48:10.719741 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m19:48:10.720291 [debug] [MainThread]: On master: Close
[0m19:48:10.885048 [debug] [MainThread]: Connection 'model.default.trusted_rides_km_real' was properly closed.
[0m19:48:10.885827 [debug] [MainThread]: On model.default.trusted_rides_km_real: ROLLBACK
[0m19:48:10.886636 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m19:48:10.887469 [debug] [MainThread]: On model.default.trusted_rides_km_real: Close
[0m19:48:11.068920 [info ] [MainThread]: 
[0m19:48:11.069752 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 28.54 seconds (28.54s).
[0m19:48:11.071319 [debug] [MainThread]: Command end result
[0m19:48:11.095829 [info ] [MainThread]: 
[0m19:48:11.096844 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:48:11.097405 [info ] [MainThread]: 
[0m19:48:11.098003 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m19:48:11.136782 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 36.687855, "process_user_time": 4.690575, "process_kernel_time": 4.388096, "process_mem_max_rss": "208338944", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m19:48:11.138492 [debug] [MainThread]: Command `dbt run` succeeded at 19:48:11.138150 after 36.69 seconds
[0m19:48:11.140349 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f18af0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1379f2550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x137052c70>]}
[0m19:48:11.141075 [debug] [MainThread]: Flushing usage events
[0m19:56:37.575683 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107848b20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1118a88e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1118ba340>]}


============================== 19:56:37.578776 | 0254f21d-ad32-4be4-aaa8-52326643c305 ==============================
[0m19:56:37.578776 [info ] [MainThread]: Running with dbt=1.7.7
[0m19:56:37.579028 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt show --select trusted_rides_km_real.sql', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m19:56:38.628152 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0254f21d-ad32-4be4-aaa8-52326643c305', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11027cac0>]}
[0m19:56:38.669250 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0254f21d-ad32-4be4-aaa8-52326643c305', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1118b50d0>]}
[0m19:56:38.669583 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m19:56:38.685416 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m19:56:38.743248 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:56:38.743617 [debug] [MainThread]: Partial parsing: updated file: default://models/trusted/trusted_rides_km_real.sql
[0m19:56:38.814033 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0254f21d-ad32-4be4-aaa8-52326643c305', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16f1690d0>]}
[0m19:56:38.823385 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0254f21d-ad32-4be4-aaa8-52326643c305', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16b423ac0>]}
[0m19:56:38.823604 [info ] [MainThread]: Found 7 models, 3 sources, 0 exposures, 0 metrics, 537 macros, 0 groups, 0 semantic models
[0m19:56:38.823769 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0254f21d-ad32-4be4-aaa8-52326643c305', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16b423d00>]}
[0m19:56:38.824548 [info ] [MainThread]: 
[0m19:56:38.824950 [debug] [MainThread]: Databricks adapter: conn: 6094469488: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (70560, 7965269056), cmpt: ``, lut: None
[0m19:56:38.825103 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:56:38.825236 [debug] [MainThread]: Databricks adapter: Thread (70560, 7965269056) using default compute resource.
[0m19:56:38.825372 [debug] [MainThread]: Databricks adapter: conn: 6094469488: _acquire sess: None, name: master, idle: 7.152557373046875e-07s, acqrelcnt: 1, lang: None, thrd: (70560, 7965269056), cmpt: ``, lut: 1710888998.825328
[0m19:56:38.826009 [debug] [ThreadPool]: Databricks adapter: conn: 6095980720: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore_default, idle: 0s, acqrelcnt: 0, lang: None, thrd: (70560, 6368751616), cmpt: ``, lut: None
[0m19:56:38.826208 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_default'
[0m19:56:38.826347 [debug] [ThreadPool]: Databricks adapter: Thread (70560, 6368751616) using default compute resource.
[0m19:56:38.826487 [debug] [ThreadPool]: Databricks adapter: conn: 6095980720: _acquire sess: None, name: list_hive_metastore_default, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (70560, 6368751616), cmpt: ``, lut: 1710888998.8264441
[0m19:56:38.828736 [debug] [ThreadPool]: Databricks adapter: conn: 6095980720: get_thread_connection: sess: None, name: list_hive_metastore_default, idle: 0.0022430419921875s, acqrelcnt: 1, lang: None, thrd: (70560, 6368751616), cmpt: ``, lut: 1710888998.8264441
[0m19:56:38.828888 [debug] [ThreadPool]: Databricks adapter: conn: 6095980720: idle check connection: sess: None, name: list_hive_metastore_default, idle: 0.0024030208587646484s, acqrelcnt: 1, lang: None, thrd: (70560, 6368751616), cmpt: ``, lut: 1710888998.8264441
[0m19:56:38.829069 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m19:56:38.829202 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m19:56:38.829336 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:56:39.453518 [debug] [ThreadPool]: Databricks adapter: conn: 6095980720: session opened sess: 01eee643-f213-127e-b244-788b4c59ff02, name: list_hive_metastore_default, idle: 1.5735626220703125e-05s, acqrelcnt: 1, lang: None, thrd: (70560, 6368751616), cmpt: ``, lut: 1710888999.453057
[0m19:56:39.894734 [debug] [ThreadPool]: SQL status: OK in 1.0700000524520874 seconds
[0m19:56:39.911129 [debug] [ThreadPool]: Databricks adapter: conn: 6095980720: get_thread_connection: sess: 01eee643-f213-127e-b244-788b4c59ff02, name: list_hive_metastore_default, idle: 0.45797085762023926s, acqrelcnt: 1, lang: None, thrd: (70560, 6368751616), cmpt: ``, lut: 1710888999.453057
[0m19:56:39.911437 [debug] [ThreadPool]: Databricks adapter: conn: 6095980720: idle check connection: sess: 01eee643-f213-127e-b244-788b4c59ff02, name: list_hive_metastore_default, idle: 0.45830702781677246s, acqrelcnt: 1, lang: None, thrd: (70560, 6368751616), cmpt: ``, lut: 1710888999.453057
[0m19:56:39.911672 [debug] [ThreadPool]: Databricks adapter: conn: 6095980720: get_thread_connection: sess: 01eee643-f213-127e-b244-788b4c59ff02, name: list_hive_metastore_default, idle: 0.45854997634887695s, acqrelcnt: 1, lang: None, thrd: (70560, 6368751616), cmpt: ``, lut: 1710888999.453057
[0m19:56:39.911894 [debug] [ThreadPool]: Databricks adapter: conn: 6095980720: idle check connection: sess: 01eee643-f213-127e-b244-788b4c59ff02, name: list_hive_metastore_default, idle: 0.4587719440460205s, acqrelcnt: 1, lang: None, thrd: (70560, 6368751616), cmpt: ``, lut: 1710888999.453057
[0m19:56:39.912099 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m19:56:39.912286 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m19:56:39.912492 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m19:56:40.193928 [debug] [ThreadPool]: SQL status: OK in 0.2800000011920929 seconds
[0m19:56:40.203563 [debug] [ThreadPool]: Databricks adapter: conn: 6095980720: get_thread_connection: sess: 01eee643-f213-127e-b244-788b4c59ff02, name: list_hive_metastore_default, idle: 0.7503650188446045s, acqrelcnt: 1, lang: None, thrd: (70560, 6368751616), cmpt: ``, lut: 1710888999.453057
[0m19:56:40.203976 [debug] [ThreadPool]: Databricks adapter: conn: 6095980720: idle check connection: sess: 01eee643-f213-127e-b244-788b4c59ff02, name: list_hive_metastore_default, idle: 0.7508201599121094s, acqrelcnt: 1, lang: None, thrd: (70560, 6368751616), cmpt: ``, lut: 1710888999.453057
[0m19:56:40.204267 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m19:56:40.204572 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m19:56:40.525669 [debug] [ThreadPool]: SQL status: OK in 0.3199999928474426 seconds
[0m19:56:40.532379 [debug] [ThreadPool]: Databricks adapter: conn: 6095980720: _release sess: 01eee643-f213-127e-b244-788b4c59ff02, name: list_hive_metastore_default, idle: 2.86102294921875e-06s, acqrelcnt: 0, lang: None, thrd: (70560, 6368751616), cmpt: ``, lut: 1710889000.532228
[0m19:56:40.537089 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0254f21d-ad32-4be4-aaa8-52326643c305', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16b423670>]}
[0m19:56:40.537551 [debug] [MainThread]: Databricks adapter: conn: 6094469488: _release sess: None, name: master, idle: 3.0994415283203125e-06s, acqrelcnt: 0, lang: None, thrd: (70560, 7965269056), cmpt: ``, lut: 1710889000.537447
[0m19:56:40.538264 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:56:40.538575 [info ] [MainThread]: 
[0m19:56:40.542141 [debug] [Thread-1  ]: Began running node model.default.trusted_rides_km_real
[0m19:56:40.542917 [debug] [Thread-1  ]: Databricks adapter: conn: 6095980720: idle check connection: sess: 01eee643-f213-127e-b244-788b4c59ff02, name: list_hive_metastore_default, idle: 0.010532855987548828s, acqrelcnt: 0, lang: None, thrd: (70560, 6368751616), cmpt: ``, lut: 1710889000.532228
[0m19:56:40.543227 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.default.trusted_rides_km_real)
[0m19:56:40.543579 [debug] [Thread-1  ]: Databricks adapter: conn: 6095980720: reusing connection list_hive_metastore_default sess: 01eee643-f213-127e-b244-788b4c59ff02, name: model.default.trusted_rides_km_real, idle: 0.011208057403564453s, acqrelcnt: 0, lang: None, thrd: (70560, 6368751616), cmpt: ``, lut: 1710889000.532228
[0m19:56:40.543890 [debug] [Thread-1  ]: Databricks adapter: On thread (70560, 6368751616): `hive_metastore`.`default`.`trusted_rides_km_real` using default compute resource.
[0m19:56:40.544186 [debug] [Thread-1  ]: Databricks adapter: conn: 6095980720: _acquire sess: 01eee643-f213-127e-b244-788b4c59ff02, name: model.default.trusted_rides_km_real, idle: 0.011836051940917969s, acqrelcnt: 1, lang: sql, thrd: (70560, 6368751616), cmpt: ``, lut: 1710889000.532228
[0m19:56:40.544470 [debug] [Thread-1  ]: Began compiling node model.default.trusted_rides_km_real
[0m19:56:40.551987 [debug] [Thread-1  ]: Writing injected SQL for node "model.default.trusted_rides_km_real"
[0m19:56:40.553256 [debug] [Thread-1  ]: Timing info for model.default.trusted_rides_km_real (compile): 19:56:40.544652 => 19:56:40.553084
[0m19:56:40.553523 [debug] [Thread-1  ]: Began executing node model.default.trusted_rides_km_real
[0m19:56:40.559398 [debug] [Thread-1  ]: Databricks adapter: conn: 6095980720: get_thread_connection: sess: 01eee643-f213-127e-b244-788b4c59ff02, name: model.default.trusted_rides_km_real, idle: 0.027038097381591797s, acqrelcnt: 1, lang: sql, thrd: (70560, 6368751616), cmpt: ``, lut: 1710889000.532228
[0m19:56:40.559726 [debug] [Thread-1  ]: Databricks adapter: conn: 6095980720: idle check connection: sess: 01eee643-f213-127e-b244-788b4c59ff02, name: model.default.trusted_rides_km_real, idle: 0.027379989624023438s, acqrelcnt: 1, lang: sql, thrd: (70560, 6368751616), cmpt: ``, lut: 1710889000.532228
[0m19:56:40.559933 [debug] [Thread-1  ]: Using databricks connection "model.default.trusted_rides_km_real"
[0m19:56:40.560211 [debug] [Thread-1  ]: On model.default.trusted_rides_km_real: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "node_id": "model.default.trusted_rides_km_real"} */

  
    select *
    from (
        

WITH trusted_transactions_per_cab_type AS (
    SELECT *
    FROM `hive_metastore`.`default`.`trusted_transactions_per_cab_type`
)
SELECT r.company_type AS company_type,
       r.cab_type AS cab_type,
       r.distance_in_miles AS distance_in_miles,
       (r.distance_in_miles * 1.60934) AS distance_in_km,
       r.price_usd AS price_usd,
       '' AS price_real_brl
FROM trusted_transactions_per_cab_type AS r
    ) as model_limit_subq
    limit 5


[0m19:56:41.239368 [debug] [Thread-1  ]: SQL status: OK in 0.6800000071525574 seconds
[0m19:56:41.245778 [debug] [Thread-1  ]: Timing info for model.default.trusted_rides_km_real (execute): 19:56:40.553682 => 19:56:41.245375
[0m19:56:41.246374 [debug] [Thread-1  ]: Databricks adapter: conn: 6095980720: _release sess: 01eee643-f213-127e-b244-788b4c59ff02, name: model.default.trusted_rides_km_real, idle: 7.152557373046875e-06s, acqrelcnt: 0, lang: sql, thrd: (70560, 6368751616), cmpt: ``, lut: 1710889001.246163
[0m19:56:41.247383 [debug] [Thread-1  ]: Databricks adapter: conn: 6095980720: _release sess: 01eee643-f213-127e-b244-788b4c59ff02, name: model.default.trusted_rides_km_real, idle: 3.0994415283203125e-06s, acqrelcnt: 0, lang: sql, thrd: (70560, 6368751616), cmpt: ``, lut: 1710889001.247191
[0m19:56:41.248029 [debug] [Thread-1  ]: Finished running node model.default.trusted_rides_km_real
[0m19:56:41.249014 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:56:41.249373 [debug] [MainThread]: Connection 'model.default.trusted_rides_km_real' was properly closed.
[0m19:56:41.249673 [debug] [MainThread]: On model.default.trusted_rides_km_real: ROLLBACK
[0m19:56:41.249970 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m19:56:41.250249 [debug] [MainThread]: On model.default.trusted_rides_km_real: Close
[0m19:56:41.409289 [debug] [MainThread]: Command end result
[0m19:56:41.445207 [info ] [MainThread]: Previewing node 'trusted_rides_km_real':
| company_type | cab_type | distance_in_miles | distance_in_km | price_usd | price_real_brl |
| ------------ | -------- | ----------------- | -------------- | --------- | -------------- |
| Lux          | Lyft     |              0.44 |         0.708… |      11.0 |                |
| Lyft         | Lyft     |              0.44 |         0.708… |       7.0 |                |
| Lux Black XL | Lyft     |              0.44 |         0.708… |      26.0 |                |
| Lyft XL      | Lyft     |              0.44 |         0.708… |       9.0 |                |
| Lux Black    | Lyft     |              0.44 |         0.708… |      16.5 |                |

[0m19:56:41.449341 [debug] [MainThread]: Resource report: {"command_name": "show", "command_success": true, "command_wall_clock_time": 3.91192, "process_user_time": 2.22706, "process_kernel_time": 3.425861, "process_mem_max_rss": "205520896", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m19:56:41.449939 [debug] [MainThread]: Command `dbt show` succeeded at 19:56:41.449802 after 3.91 seconds
[0m19:56:41.450374 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107848b20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16ce7ae50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16b58ed90>]}
[0m19:56:41.450810 [debug] [MainThread]: Flushing usage events
[0m19:57:10.276783 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1026c9160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1067754c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1067b82e0>]}


============================== 19:57:10.279922 | cb5833fd-435d-4e44-80d5-6b24ea945a05 ==============================
[0m19:57:10.279922 [info ] [MainThread]: Running with dbt=1.7.7
[0m19:57:10.280221 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select trusted_rides_km_real.sql', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m19:57:11.276415 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cb5833fd-435d-4e44-80d5-6b24ea945a05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1067b8b80>]}
[0m19:57:11.321447 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cb5833fd-435d-4e44-80d5-6b24ea945a05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16658e8b0>]}
[0m19:57:11.321798 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m19:57:11.337049 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m19:57:11.392461 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:57:11.392705 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:57:11.396451 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'cb5833fd-435d-4e44-80d5-6b24ea945a05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x169fae0d0>]}
[0m19:57:11.403149 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'cb5833fd-435d-4e44-80d5-6b24ea945a05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x169f75d30>]}
[0m19:57:11.403417 [info ] [MainThread]: Found 7 models, 3 sources, 0 exposures, 0 metrics, 537 macros, 0 groups, 0 semantic models
[0m19:57:11.403597 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cb5833fd-435d-4e44-80d5-6b24ea945a05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x169f75f10>]}
[0m19:57:11.404428 [info ] [MainThread]: 
[0m19:57:11.404929 [debug] [MainThread]: Databricks adapter: conn: 6072786080: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (70612, 7965269056), cmpt: ``, lut: None
[0m19:57:11.405102 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m19:57:11.405251 [debug] [MainThread]: Databricks adapter: Thread (70612, 7965269056) using default compute resource.
[0m19:57:11.405411 [debug] [MainThread]: Databricks adapter: conn: 6072786080: _acquire sess: None, name: master, idle: 7.152557373046875e-07s, acqrelcnt: 1, lang: None, thrd: (70612, 7965269056), cmpt: ``, lut: 1710889031.4053602
[0m19:57:11.405990 [debug] [ThreadPool]: Databricks adapter: conn: 6070805936: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore, idle: 0s, acqrelcnt: 0, lang: None, thrd: (70612, 6185332736), cmpt: ``, lut: None
[0m19:57:11.406186 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m19:57:11.406338 [debug] [ThreadPool]: Databricks adapter: Thread (70612, 6185332736) using default compute resource.
[0m19:57:11.406501 [debug] [ThreadPool]: Databricks adapter: conn: 6070805936: _acquire sess: None, name: list_hive_metastore, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (70612, 6185332736), cmpt: ``, lut: 1710889031.406451
[0m19:57:11.406671 [debug] [ThreadPool]: Databricks adapter: conn: 6070805936: get_thread_connection: sess: None, name: list_hive_metastore, idle: 0.00017309188842773438s, acqrelcnt: 1, lang: None, thrd: (70612, 6185332736), cmpt: ``, lut: 1710889031.406451
[0m19:57:11.406894 [debug] [ThreadPool]: Databricks adapter: conn: 6070805936: idle check connection: sess: None, name: list_hive_metastore, idle: 0.0003960132598876953s, acqrelcnt: 1, lang: None, thrd: (70612, 6185332736), cmpt: ``, lut: 1710889031.406451
[0m19:57:11.407033 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m19:57:11.407175 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m19:57:11.407306 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:57:12.013577 [debug] [ThreadPool]: Databricks adapter: conn: 6070805936: session opened sess: 01eee644-057b-1eaa-87d8-564fc43f1297, name: list_hive_metastore, idle: 1.6927719116210938e-05s, acqrelcnt: 1, lang: None, thrd: (70612, 6185332736), cmpt: ``, lut: 1710889032.013002
[0m19:57:12.298722 [debug] [ThreadPool]: SQL status: OK in 0.8899999856948853 seconds
[0m19:57:12.312470 [debug] [ThreadPool]: Databricks adapter: conn: 6070805936: _release sess: 01eee644-057b-1eaa-87d8-564fc43f1297, name: list_hive_metastore, idle: 5.9604644775390625e-06s, acqrelcnt: 0, lang: None, thrd: (70612, 6185332736), cmpt: ``, lut: 1710889032.312287
[0m19:57:12.314548 [debug] [ThreadPool]: Databricks adapter: conn: 6070805936: idle check connection: sess: 01eee644-057b-1eaa-87d8-564fc43f1297, name: list_hive_metastore, idle: 0.0021469593048095703s, acqrelcnt: 0, lang: None, thrd: (70612, 6185332736), cmpt: ``, lut: 1710889032.312287
[0m19:57:12.314927 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now list_hive_metastore_default)
[0m19:57:12.315245 [debug] [ThreadPool]: Databricks adapter: conn: 6070805936: reusing connection list_hive_metastore sess: 01eee644-057b-1eaa-87d8-564fc43f1297, name: list_hive_metastore_default, idle: 0.0028657913208007812s, acqrelcnt: 0, lang: None, thrd: (70612, 6185332736), cmpt: ``, lut: 1710889032.312287
[0m19:57:12.315536 [debug] [ThreadPool]: Databricks adapter: Thread (70612, 6185332736) using default compute resource.
[0m19:57:12.315820 [debug] [ThreadPool]: Databricks adapter: conn: 6070805936: _acquire sess: 01eee644-057b-1eaa-87d8-564fc43f1297, name: list_hive_metastore_default, idle: 0.003446817398071289s, acqrelcnt: 1, lang: None, thrd: (70612, 6185332736), cmpt: ``, lut: 1710889032.312287
[0m19:57:12.321242 [debug] [ThreadPool]: Databricks adapter: conn: 6070805936: get_thread_connection: sess: 01eee644-057b-1eaa-87d8-564fc43f1297, name: list_hive_metastore_default, idle: 0.008844852447509766s, acqrelcnt: 1, lang: None, thrd: (70612, 6185332736), cmpt: ``, lut: 1710889032.312287
[0m19:57:12.321598 [debug] [ThreadPool]: Databricks adapter: conn: 6070805936: idle check connection: sess: 01eee644-057b-1eaa-87d8-564fc43f1297, name: list_hive_metastore_default, idle: 0.009217023849487305s, acqrelcnt: 1, lang: None, thrd: (70612, 6185332736), cmpt: ``, lut: 1710889032.312287
[0m19:57:12.321891 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m19:57:12.322181 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m19:57:12.597778 [debug] [ThreadPool]: SQL status: OK in 0.2800000011920929 seconds
[0m19:57:12.622361 [debug] [ThreadPool]: Databricks adapter: conn: 6070805936: get_thread_connection: sess: 01eee644-057b-1eaa-87d8-564fc43f1297, name: list_hive_metastore_default, idle: 0.3099179267883301s, acqrelcnt: 1, lang: None, thrd: (70612, 6185332736), cmpt: ``, lut: 1710889032.312287
[0m19:57:12.622806 [debug] [ThreadPool]: Databricks adapter: conn: 6070805936: idle check connection: sess: 01eee644-057b-1eaa-87d8-564fc43f1297, name: list_hive_metastore_default, idle: 0.3104288578033447s, acqrelcnt: 1, lang: None, thrd: (70612, 6185332736), cmpt: ``, lut: 1710889032.312287
[0m19:57:12.623123 [debug] [ThreadPool]: Databricks adapter: conn: 6070805936: get_thread_connection: sess: 01eee644-057b-1eaa-87d8-564fc43f1297, name: list_hive_metastore_default, idle: 0.31073975563049316s, acqrelcnt: 1, lang: None, thrd: (70612, 6185332736), cmpt: ``, lut: 1710889032.312287
[0m19:57:12.623422 [debug] [ThreadPool]: Databricks adapter: conn: 6070805936: idle check connection: sess: 01eee644-057b-1eaa-87d8-564fc43f1297, name: list_hive_metastore_default, idle: 0.3110499382019043s, acqrelcnt: 1, lang: None, thrd: (70612, 6185332736), cmpt: ``, lut: 1710889032.312287
[0m19:57:12.623704 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m19:57:12.623957 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m19:57:12.624245 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m19:57:12.893065 [debug] [ThreadPool]: SQL status: OK in 0.27000001072883606 seconds
[0m19:57:12.910957 [debug] [ThreadPool]: Databricks adapter: conn: 6070805936: get_thread_connection: sess: 01eee644-057b-1eaa-87d8-564fc43f1297, name: list_hive_metastore_default, idle: 0.5983898639678955s, acqrelcnt: 1, lang: None, thrd: (70612, 6185332736), cmpt: ``, lut: 1710889032.312287
[0m19:57:12.911766 [debug] [ThreadPool]: Databricks adapter: conn: 6070805936: idle check connection: sess: 01eee644-057b-1eaa-87d8-564fc43f1297, name: list_hive_metastore_default, idle: 0.5993468761444092s, acqrelcnt: 1, lang: None, thrd: (70612, 6185332736), cmpt: ``, lut: 1710889032.312287
[0m19:57:12.912265 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m19:57:12.912718 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m19:57:13.225864 [debug] [ThreadPool]: SQL status: OK in 0.3100000023841858 seconds
[0m19:57:13.232927 [debug] [ThreadPool]: Databricks adapter: conn: 6070805936: _release sess: 01eee644-057b-1eaa-87d8-564fc43f1297, name: list_hive_metastore_default, idle: 9.059906005859375e-06s, acqrelcnt: 0, lang: None, thrd: (70612, 6185332736), cmpt: ``, lut: 1710889033.232704
[0m19:57:13.240111 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cb5833fd-435d-4e44-80d5-6b24ea945a05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x169d123a0>]}
[0m19:57:13.240954 [debug] [MainThread]: Databricks adapter: conn: 6072786080: get_thread_connection: sess: None, name: master, idle: 1.8354177474975586s, acqrelcnt: 1, lang: None, thrd: (70612, 7965269056), cmpt: ``, lut: 1710889031.4053602
[0m19:57:13.241504 [debug] [MainThread]: Databricks adapter: conn: 6072786080: idle check connection: sess: None, name: master, idle: 1.8360388278961182s, acqrelcnt: 1, lang: None, thrd: (70612, 7965269056), cmpt: ``, lut: 1710889031.4053602
[0m19:57:13.241833 [debug] [MainThread]: Databricks adapter: conn: 6072786080: get_thread_connection: sess: None, name: master, idle: 1.8363828659057617s, acqrelcnt: 1, lang: None, thrd: (70612, 7965269056), cmpt: ``, lut: 1710889031.4053602
[0m19:57:13.242176 [debug] [MainThread]: Databricks adapter: conn: 6072786080: idle check connection: sess: None, name: master, idle: 1.8367035388946533s, acqrelcnt: 1, lang: None, thrd: (70612, 7965269056), cmpt: ``, lut: 1710889031.4053602
[0m19:57:13.242600 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m19:57:13.242934 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m19:57:13.243307 [debug] [MainThread]: Databricks adapter: conn: 6072786080: _release sess: None, name: master, idle: 3.0994415283203125e-06s, acqrelcnt: 0, lang: None, thrd: (70612, 7965269056), cmpt: ``, lut: 1710889033.243197
[0m19:57:13.244468 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:57:13.245048 [info ] [MainThread]: 
[0m19:57:13.250042 [debug] [Thread-1  ]: Began running node model.default.trusted_rides_km_real
[0m19:57:13.251063 [info ] [Thread-1  ]: 1 of 1 START sql table model default.trusted_rides_km_real ..................... [RUN]
[0m19:57:13.252459 [debug] [Thread-1  ]: Databricks adapter: conn: 6070805936: idle check connection: sess: 01eee644-057b-1eaa-87d8-564fc43f1297, name: list_hive_metastore_default, idle: 0.019529104232788086s, acqrelcnt: 0, lang: None, thrd: (70612, 6185332736), cmpt: ``, lut: 1710889033.232704
[0m19:57:13.253670 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.default.trusted_rides_km_real)
[0m19:57:13.254208 [debug] [Thread-1  ]: Databricks adapter: conn: 6070805936: reusing connection list_hive_metastore_default sess: 01eee644-057b-1eaa-87d8-564fc43f1297, name: model.default.trusted_rides_km_real, idle: 0.021293163299560547s, acqrelcnt: 0, lang: None, thrd: (70612, 6185332736), cmpt: ``, lut: 1710889033.232704
[0m19:57:13.254654 [debug] [Thread-1  ]: Databricks adapter: On thread (70612, 6185332736): `hive_metastore`.`default`.`trusted_rides_km_real` using default compute resource.
[0m19:57:13.255104 [debug] [Thread-1  ]: Databricks adapter: conn: 6070805936: _acquire sess: 01eee644-057b-1eaa-87d8-564fc43f1297, name: model.default.trusted_rides_km_real, idle: 0.022233247756958008s, acqrelcnt: 1, lang: sql, thrd: (70612, 6185332736), cmpt: ``, lut: 1710889033.232704
[0m19:57:13.255519 [debug] [Thread-1  ]: Began compiling node model.default.trusted_rides_km_real
[0m19:57:13.264681 [debug] [Thread-1  ]: Writing injected SQL for node "model.default.trusted_rides_km_real"
[0m19:57:13.266029 [debug] [Thread-1  ]: Timing info for model.default.trusted_rides_km_real (compile): 19:57:13.255770 => 19:57:13.265721
[0m19:57:13.266450 [debug] [Thread-1  ]: Began executing node model.default.trusted_rides_km_real
[0m19:57:13.281418 [debug] [Thread-1  ]: Databricks adapter: conn: 6070805936: get_thread_connection: sess: 01eee644-057b-1eaa-87d8-564fc43f1297, name: model.default.trusted_rides_km_real, idle: 0.04857516288757324s, acqrelcnt: 1, lang: sql, thrd: (70612, 6185332736), cmpt: ``, lut: 1710889033.232704
[0m19:57:13.281752 [debug] [Thread-1  ]: Databricks adapter: conn: 6070805936: idle check connection: sess: 01eee644-057b-1eaa-87d8-564fc43f1297, name: model.default.trusted_rides_km_real, idle: 0.04893803596496582s, acqrelcnt: 1, lang: sql, thrd: (70612, 6185332736), cmpt: ``, lut: 1710889033.232704
[0m19:57:13.281974 [debug] [Thread-1  ]: Using databricks connection "model.default.trusted_rides_km_real"
[0m19:57:13.282242 [debug] [Thread-1  ]: On model.default.trusted_rides_km_real: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "node_id": "model.default.trusted_rides_km_real"} */

      describe extended `hive_metastore`.`default`.`trusted_rides_km_real`
  
[0m19:57:13.642355 [debug] [Thread-1  ]: SQL status: OK in 0.36000001430511475 seconds
[0m19:57:13.698405 [debug] [Thread-1  ]: Writing runtime sql for node "model.default.trusted_rides_km_real"
[0m19:57:13.699839 [debug] [Thread-1  ]: Databricks adapter: conn: 6070805936: get_thread_connection: sess: 01eee644-057b-1eaa-87d8-564fc43f1297, name: model.default.trusted_rides_km_real, idle: 0.46701693534851074s, acqrelcnt: 1, lang: sql, thrd: (70612, 6185332736), cmpt: ``, lut: 1710889033.232704
[0m19:57:13.700108 [debug] [Thread-1  ]: Databricks adapter: conn: 6070805936: idle check connection: sess: 01eee644-057b-1eaa-87d8-564fc43f1297, name: model.default.trusted_rides_km_real, idle: 0.46730494499206543s, acqrelcnt: 1, lang: sql, thrd: (70612, 6185332736), cmpt: ``, lut: 1710889033.232704
[0m19:57:13.700302 [debug] [Thread-1  ]: Using databricks connection "model.default.trusted_rides_km_real"
[0m19:57:13.700570 [debug] [Thread-1  ]: On model.default.trusted_rides_km_real: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "node_id": "model.default.trusted_rides_km_real"} */

  
    
        create or replace table `hive_metastore`.`default`.`trusted_rides_km_real`
      
      
    using delta
      
      
      
      
      
      
      
      as
      

WITH trusted_transactions_per_cab_type AS (
    SELECT *
    FROM `hive_metastore`.`default`.`trusted_transactions_per_cab_type`
)
SELECT r.company_type AS company_type,
       r.cab_type AS cab_type,
       r.distance_in_miles AS distance_in_miles,
       (r.distance_in_miles * 1.60934) AS distance_in_km,
       r.price_usd AS price_usd,
       '' AS price_real_brl
FROM trusted_transactions_per_cab_type AS r
  
[0m19:57:16.131364 [debug] [Thread-1  ]: SQL status: OK in 2.430000066757202 seconds
[0m19:57:16.171049 [debug] [Thread-1  ]: Timing info for model.default.trusted_rides_km_real (execute): 19:57:13.266677 => 19:57:16.170813
[0m19:57:16.171603 [debug] [Thread-1  ]: Databricks adapter: conn: 6070805936: _release sess: 01eee644-057b-1eaa-87d8-564fc43f1297, name: model.default.trusted_rides_km_real, idle: 5.9604644775390625e-06s, acqrelcnt: 0, lang: sql, thrd: (70612, 6185332736), cmpt: ``, lut: 1710889036.171411
[0m19:57:16.172332 [debug] [Thread-1  ]: Databricks adapter: conn: 6070805936: _release sess: 01eee644-057b-1eaa-87d8-564fc43f1297, name: model.default.trusted_rides_km_real, idle: 1.9073486328125e-06s, acqrelcnt: 0, lang: sql, thrd: (70612, 6185332736), cmpt: ``, lut: 1710889036.172213
[0m19:57:16.172756 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cb5833fd-435d-4e44-80d5-6b24ea945a05', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16a984be0>]}
[0m19:57:16.173368 [info ] [Thread-1  ]: 1 of 1 OK created sql table model default.trusted_rides_km_real ................ [[32mOK[0m in 2.92s]
[0m19:57:16.173874 [debug] [Thread-1  ]: Finished running node model.default.trusted_rides_km_real
[0m19:57:16.175579 [debug] [MainThread]: Databricks adapter: conn: 6072786080: idle check connection: sess: None, name: master, idle: 2.9322240352630615s, acqrelcnt: 0, lang: None, thrd: (70612, 7965269056), cmpt: ``, lut: 1710889033.243197
[0m19:57:16.176060 [debug] [MainThread]: Databricks adapter: conn: 6072786080: reusing connection master sess: None, name: master, idle: 2.932770013809204s, acqrelcnt: 0, lang: None, thrd: (70612, 7965269056), cmpt: ``, lut: 1710889033.243197
[0m19:57:16.176342 [debug] [MainThread]: Databricks adapter: Thread (70612, 7965269056) using default compute resource.
[0m19:57:16.176604 [debug] [MainThread]: Databricks adapter: conn: 6072786080: _acquire sess: None, name: master, idle: 2.9333319664001465s, acqrelcnt: 1, lang: None, thrd: (70612, 7965269056), cmpt: ``, lut: 1710889033.243197
[0m19:57:16.176886 [debug] [MainThread]: Databricks adapter: conn: 6072786080: get_thread_connection: sess: None, name: master, idle: 2.9336090087890625s, acqrelcnt: 1, lang: None, thrd: (70612, 7965269056), cmpt: ``, lut: 1710889033.243197
[0m19:57:16.177143 [debug] [MainThread]: Databricks adapter: conn: 6072786080: idle check connection: sess: None, name: master, idle: 2.933873176574707s, acqrelcnt: 1, lang: None, thrd: (70612, 7965269056), cmpt: ``, lut: 1710889033.243197
[0m19:57:16.177398 [debug] [MainThread]: On master: ROLLBACK
[0m19:57:16.177642 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:57:16.764014 [debug] [MainThread]: Databricks adapter: conn: 6072786080: session opened sess: 01eee644-0853-17a8-94e6-6f2b82b92eb6, name: master, idle: 1.9073486328125e-05s, acqrelcnt: 1, lang: None, thrd: (70612, 7965269056), cmpt: ``, lut: 1710889036.763356
[0m19:57:16.765909 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m19:57:16.767013 [debug] [MainThread]: Databricks adapter: conn: 6072786080: get_thread_connection: sess: 01eee644-0853-17a8-94e6-6f2b82b92eb6, name: master, idle: 0.0033729076385498047s, acqrelcnt: 1, lang: None, thrd: (70612, 7965269056), cmpt: ``, lut: 1710889036.763356
[0m19:57:16.768037 [debug] [MainThread]: Databricks adapter: conn: 6072786080: idle check connection: sess: 01eee644-0853-17a8-94e6-6f2b82b92eb6, name: master, idle: 0.004414796829223633s, acqrelcnt: 1, lang: None, thrd: (70612, 7965269056), cmpt: ``, lut: 1710889036.763356
[0m19:57:16.769658 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m19:57:16.770731 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m19:57:16.771626 [debug] [MainThread]: Databricks adapter: conn: 6072786080: _release sess: 01eee644-0853-17a8-94e6-6f2b82b92eb6, name: master, idle: 5.9604644775390625e-06s, acqrelcnt: 0, lang: None, thrd: (70612, 7965269056), cmpt: ``, lut: 1710889036.771395
[0m19:57:16.773795 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:57:16.774399 [debug] [MainThread]: On master: ROLLBACK
[0m19:57:16.775151 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m19:57:16.775804 [debug] [MainThread]: On master: Close
[0m19:57:16.946336 [debug] [MainThread]: Connection 'model.default.trusted_rides_km_real' was properly closed.
[0m19:57:16.947408 [debug] [MainThread]: On model.default.trusted_rides_km_real: ROLLBACK
[0m19:57:16.948379 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m19:57:16.949269 [debug] [MainThread]: On model.default.trusted_rides_km_real: Close
[0m19:57:17.113645 [info ] [MainThread]: 
[0m19:57:17.115068 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 5.71 seconds (5.71s).
[0m19:57:17.117149 [debug] [MainThread]: Command end result
[0m19:57:17.155414 [info ] [MainThread]: 
[0m19:57:17.156601 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:57:17.157572 [info ] [MainThread]: 
[0m19:57:17.158410 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m19:57:17.163293 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 6.923192, "process_user_time": 2.35049, "process_kernel_time": 3.415758, "process_mem_max_rss": "205078528", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m19:57:17.164145 [debug] [MainThread]: Command `dbt run` succeeded at 19:57:17.163987 after 6.92 seconds
[0m19:57:17.164779 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1026c9160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16a952910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x169f9c820>]}
[0m19:57:17.165771 [debug] [MainThread]: Flushing usage events
[0m20:50:23.599250 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1050c5160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121d74520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121db60a0>]}


============================== 20:50:23.602468 | 6293452c-cb45-4179-8426-f195c16593d9 ==============================
[0m20:50:23.602468 [info ] [MainThread]: Running with dbt=1.7.7
[0m20:50:23.602746 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks/logs', 'profiles_dir': '/Users/luanmorenomaciel/GitHub/astro-dbt-airflow-dbsql/dags/dbt/databricks', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt show --select trusted_rides_km_real.sql', 'send_anonymous_usage_stats': 'True'}
[0m20:50:24.710367 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6293452c-cb45-4179-8426-f195c16593d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121db6eb0>]}
[0m20:50:24.753661 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6293452c-cb45-4179-8426-f195c16593d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16a4f46a0>]}
[0m20:50:24.754018 [info ] [MainThread]: Registered adapter: databricks=1.7.7
[0m20:50:24.769888 [debug] [MainThread]: checksum: 54188551c516f4dd1c42b8d9c289f2bf49f18ae42632e2ba36a64ad29fd60da4, vars: {}, profile: , target: , version: 1.7.7
[0m20:50:24.827293 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:50:24.827559 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:50:24.831570 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6293452c-cb45-4179-8426-f195c16593d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16a8ef0d0>]}
[0m20:50:24.840441 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6293452c-cb45-4179-8426-f195c16593d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16a8b8dc0>]}
[0m20:50:24.840709 [info ] [MainThread]: Found 7 models, 3 sources, 0 exposures, 0 metrics, 537 macros, 0 groups, 0 semantic models
[0m20:50:24.840893 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6293452c-cb45-4179-8426-f195c16593d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16a8b8fd0>]}
[0m20:50:24.841696 [info ] [MainThread]: 
[0m20:50:24.842129 [debug] [MainThread]: Databricks adapter: conn: 6082497456: Creating DatabricksDBTConnection sess: None, name: master, idle: 0s, acqrelcnt: 0, lang: None, thrd: (72921, 7965269056), cmpt: ``, lut: None
[0m20:50:24.842278 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m20:50:24.842410 [debug] [MainThread]: Databricks adapter: Thread (72921, 7965269056) using default compute resource.
[0m20:50:24.842553 [debug] [MainThread]: Databricks adapter: conn: 6082497456: _acquire sess: None, name: master, idle: 2.1457672119140625e-06s, acqrelcnt: 1, lang: None, thrd: (72921, 7965269056), cmpt: ``, lut: 1710892224.842505
[0m20:50:24.843277 [debug] [ThreadPool]: Databricks adapter: conn: 6079178544: Creating DatabricksDBTConnection sess: None, name: list_hive_metastore_default, idle: 0s, acqrelcnt: 0, lang: None, thrd: (72921, 6140227584), cmpt: ``, lut: None
[0m20:50:24.843501 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore_default'
[0m20:50:24.843668 [debug] [ThreadPool]: Databricks adapter: Thread (72921, 6140227584) using default compute resource.
[0m20:50:24.843821 [debug] [ThreadPool]: Databricks adapter: conn: 6079178544: _acquire sess: None, name: list_hive_metastore_default, idle: 9.5367431640625e-07s, acqrelcnt: 1, lang: None, thrd: (72921, 6140227584), cmpt: ``, lut: 1710892224.843776
[0m20:50:24.846589 [debug] [ThreadPool]: Databricks adapter: conn: 6079178544: get_thread_connection: sess: None, name: list_hive_metastore_default, idle: 0.0027577877044677734s, acqrelcnt: 1, lang: None, thrd: (72921, 6140227584), cmpt: ``, lut: 1710892224.843776
[0m20:50:24.846760 [debug] [ThreadPool]: Databricks adapter: conn: 6079178544: idle check connection: sess: None, name: list_hive_metastore_default, idle: 0.002939939498901367s, acqrelcnt: 1, lang: None, thrd: (72921, 6140227584), cmpt: ``, lut: 1710892224.843776
[0m20:50:24.846903 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m20:50:24.847047 [debug] [ThreadPool]: On list_hive_metastore_default: GetTables(database=hive_metastore, schema=default, identifier=None)
[0m20:50:24.847188 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:50:25.704794 [debug] [ThreadPool]: Databricks adapter: conn: 6079178544: session opened sess: 01eee64b-7505-1102-92fe-6196f1c7593e, name: list_hive_metastore_default, idle: 1.9788742065429688e-05s, acqrelcnt: 1, lang: None, thrd: (72921, 6140227584), cmpt: ``, lut: 1710892225.7043462
[0m20:50:35.686396 [debug] [ThreadPool]: SQL status: OK in 10.84000015258789 seconds
[0m20:50:35.707257 [debug] [ThreadPool]: Databricks adapter: conn: 6079178544: get_thread_connection: sess: 01eee64b-7505-1102-92fe-6196f1c7593e, name: list_hive_metastore_default, idle: 10.002701759338379s, acqrelcnt: 1, lang: None, thrd: (72921, 6140227584), cmpt: ``, lut: 1710892225.7043462
[0m20:50:35.707832 [debug] [ThreadPool]: Databricks adapter: conn: 6079178544: idle check connection: sess: 01eee64b-7505-1102-92fe-6196f1c7593e, name: list_hive_metastore_default, idle: 10.003355741500854s, acqrelcnt: 1, lang: None, thrd: (72921, 6140227584), cmpt: ``, lut: 1710892225.7043462
[0m20:50:35.708145 [debug] [ThreadPool]: Databricks adapter: conn: 6079178544: get_thread_connection: sess: 01eee64b-7505-1102-92fe-6196f1c7593e, name: list_hive_metastore_default, idle: 10.003728866577148s, acqrelcnt: 1, lang: None, thrd: (72921, 6140227584), cmpt: ``, lut: 1710892225.7043462
[0m20:50:35.708386 [debug] [ThreadPool]: Databricks adapter: conn: 6079178544: idle check connection: sess: 01eee64b-7505-1102-92fe-6196f1c7593e, name: list_hive_metastore_default, idle: 10.003978967666626s, acqrelcnt: 1, lang: None, thrd: (72921, 6140227584), cmpt: ``, lut: 1710892225.7043462
[0m20:50:35.708590 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m20:50:35.708784 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m20:50:35.709004 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m20:50:36.831463 [debug] [ThreadPool]: SQL status: OK in 1.1200000047683716 seconds
[0m20:50:36.839542 [debug] [ThreadPool]: Databricks adapter: conn: 6079178544: get_thread_connection: sess: 01eee64b-7505-1102-92fe-6196f1c7593e, name: list_hive_metastore_default, idle: 11.135044813156128s, acqrelcnt: 1, lang: None, thrd: (72921, 6140227584), cmpt: ``, lut: 1710892225.7043462
[0m20:50:36.840102 [debug] [ThreadPool]: Databricks adapter: conn: 6079178544: idle check connection: sess: 01eee64b-7505-1102-92fe-6196f1c7593e, name: list_hive_metastore_default, idle: 11.135660648345947s, acqrelcnt: 1, lang: None, thrd: (72921, 6140227584), cmpt: ``, lut: 1710892225.7043462
[0m20:50:36.840371 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m20:50:36.840593 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show views in `hive_metastore`.`default`
  
[0m20:50:37.362448 [debug] [ThreadPool]: SQL status: OK in 0.5199999809265137 seconds
[0m20:50:37.369696 [debug] [ThreadPool]: Databricks adapter: conn: 6079178544: _release sess: 01eee64b-7505-1102-92fe-6196f1c7593e, name: list_hive_metastore_default, idle: 5.0067901611328125e-06s, acqrelcnt: 0, lang: None, thrd: (72921, 6140227584), cmpt: ``, lut: 1710892237.3695
[0m20:50:37.374516 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6293452c-cb45-4179-8426-f195c16593d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16a55b670>]}
[0m20:50:37.375012 [debug] [MainThread]: Databricks adapter: conn: 6082497456: _release sess: None, name: master, idle: 4.291534423828125e-06s, acqrelcnt: 0, lang: None, thrd: (72921, 7965269056), cmpt: ``, lut: 1710892237.3749068
[0m20:50:37.375714 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:50:37.376036 [info ] [MainThread]: 
[0m20:50:37.381550 [debug] [Thread-1  ]: Began running node model.default.trusted_rides_km_real
[0m20:50:37.383034 [debug] [Thread-1  ]: Databricks adapter: conn: 6079178544: idle check connection: sess: 01eee64b-7505-1102-92fe-6196f1c7593e, name: list_hive_metastore_default, idle: 0.013135194778442383s, acqrelcnt: 0, lang: None, thrd: (72921, 6140227584), cmpt: ``, lut: 1710892237.3695
[0m20:50:37.383372 [debug] [Thread-1  ]: Re-using an available connection from the pool (formerly list_hive_metastore_default, now model.default.trusted_rides_km_real)
[0m20:50:37.383720 [debug] [Thread-1  ]: Databricks adapter: conn: 6079178544: reusing connection list_hive_metastore_default sess: 01eee64b-7505-1102-92fe-6196f1c7593e, name: model.default.trusted_rides_km_real, idle: 0.014088153839111328s, acqrelcnt: 0, lang: None, thrd: (72921, 6140227584), cmpt: ``, lut: 1710892237.3695
[0m20:50:37.384047 [debug] [Thread-1  ]: Databricks adapter: On thread (72921, 6140227584): `hive_metastore`.`default`.`trusted_rides_km_real` using default compute resource.
[0m20:50:37.384373 [debug] [Thread-1  ]: Databricks adapter: conn: 6079178544: _acquire sess: 01eee64b-7505-1102-92fe-6196f1c7593e, name: model.default.trusted_rides_km_real, idle: 0.014744997024536133s, acqrelcnt: 1, lang: sql, thrd: (72921, 6140227584), cmpt: ``, lut: 1710892237.3695
[0m20:50:37.384698 [debug] [Thread-1  ]: Began compiling node model.default.trusted_rides_km_real
[0m20:50:37.393063 [debug] [Thread-1  ]: Writing injected SQL for node "model.default.trusted_rides_km_real"
[0m20:50:37.395914 [debug] [Thread-1  ]: Timing info for model.default.trusted_rides_km_real (compile): 20:50:37.384907 => 20:50:37.395661
[0m20:50:37.396224 [debug] [Thread-1  ]: Began executing node model.default.trusted_rides_km_real
[0m20:50:37.403983 [debug] [Thread-1  ]: Databricks adapter: conn: 6079178544: get_thread_connection: sess: 01eee64b-7505-1102-92fe-6196f1c7593e, name: model.default.trusted_rides_km_real, idle: 0.03432106971740723s, acqrelcnt: 1, lang: sql, thrd: (72921, 6140227584), cmpt: ``, lut: 1710892237.3695
[0m20:50:37.404348 [debug] [Thread-1  ]: Databricks adapter: conn: 6079178544: idle check connection: sess: 01eee64b-7505-1102-92fe-6196f1c7593e, name: model.default.trusted_rides_km_real, idle: 0.03474307060241699s, acqrelcnt: 1, lang: sql, thrd: (72921, 6140227584), cmpt: ``, lut: 1710892237.3695
[0m20:50:37.404593 [debug] [Thread-1  ]: Using databricks connection "model.default.trusted_rides_km_real"
[0m20:50:37.404902 [debug] [Thread-1  ]: On model.default.trusted_rides_km_real: /* {"app": "dbt", "dbt_version": "1.7.7", "dbt_databricks_version": "1.7.7", "databricks_sql_connector_version": "2.9.3", "profile_name": "default", "target_name": "dev", "node_id": "model.default.trusted_rides_km_real"} */

  
    select *
    from (
        

WITH trusted_transactions_per_cab_type AS (
    SELECT *
    FROM `hive_metastore`.`default`.`trusted_transactions_per_cab_type`
)
SELECT r.company_type AS company_type,
       r.cab_type AS cab_type,
       r.distance_in_miles AS distance_in_miles,
       (r.distance_in_miles * 1.60934) AS distance_in_km,
       r.price_usd AS price_usd,
       '' AS price_real_brl
FROM trusted_transactions_per_cab_type AS r
    ) as model_limit_subq
    limit 5


[0m20:50:38.863051 [debug] [Thread-1  ]: SQL status: OK in 1.4600000381469727 seconds
[0m20:50:38.869332 [debug] [Thread-1  ]: Timing info for model.default.trusted_rides_km_real (execute): 20:50:37.396390 => 20:50:38.868900
[0m20:50:38.870050 [debug] [Thread-1  ]: Databricks adapter: conn: 6079178544: _release sess: 01eee64b-7505-1102-92fe-6196f1c7593e, name: model.default.trusted_rides_km_real, idle: 6.198883056640625e-06s, acqrelcnt: 0, lang: sql, thrd: (72921, 6140227584), cmpt: ``, lut: 1710892238.8698199
[0m20:50:38.870945 [debug] [Thread-1  ]: Databricks adapter: conn: 6079178544: _release sess: 01eee64b-7505-1102-92fe-6196f1c7593e, name: model.default.trusted_rides_km_real, idle: 9.5367431640625e-07s, acqrelcnt: 0, lang: sql, thrd: (72921, 6140227584), cmpt: ``, lut: 1710892238.870825
[0m20:50:38.871575 [debug] [Thread-1  ]: Finished running node model.default.trusted_rides_km_real
[0m20:50:38.872999 [debug] [MainThread]: Connection 'master' was properly closed.
[0m20:50:38.873346 [debug] [MainThread]: Connection 'model.default.trusted_rides_km_real' was properly closed.
[0m20:50:38.873617 [debug] [MainThread]: On model.default.trusted_rides_km_real: ROLLBACK
[0m20:50:38.873854 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m20:50:38.874078 [debug] [MainThread]: On model.default.trusted_rides_km_real: Close
[0m20:50:39.066002 [debug] [MainThread]: Command end result
[0m20:50:39.097223 [info ] [MainThread]: Previewing node 'trusted_rides_km_real':
| company_type | cab_type | distance_in_miles | distance_in_km | price_usd | price_real_brl |
| ------------ | -------- | ----------------- | -------------- | --------- | -------------- |
| Lux          | Lyft     |              0.44 |         0.708… |      11.0 |                |
| Lyft         | Lyft     |              0.44 |         0.708… |       7.0 |                |
| Lux Black XL | Lyft     |              0.44 |         0.708… |      26.0 |                |
| Lyft XL      | Lyft     |              0.44 |         0.708… |       9.0 |                |
| Lux Black    | Lyft     |              0.44 |         0.708… |      16.5 |                |

[0m20:50:39.100639 [debug] [MainThread]: Resource report: {"command_name": "show", "command_success": true, "command_wall_clock_time": 15.545344, "process_user_time": 2.312591, "process_kernel_time": 3.141042, "process_mem_max_rss": "200818688", "process_in_blocks": "0", "process_out_blocks": "0"}
[0m20:50:39.101102 [debug] [MainThread]: Command `dbt show` succeeded at 20:50:39.101008 after 15.55 seconds
[0m20:50:39.101444 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1050c5160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16a96efd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121db6eb0>]}
[0m20:50:39.101752 [debug] [MainThread]: Flushing usage events
